{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMA-943Aev9e"
   },
   "source": [
    "# Spiking multicompartment PC network\n",
    "\n",
    "## Abstract\n",
    "Predictive coding is a promising theoretical framework for understanding the hierarchical sensory processing in the brain, yet how it is implemented with cortical spiking neurons is still unclear. While most existing works have taken a hand-wiring approach to creating microcircuits which match experimental results, recent work in applying the optimisation approach revealed that cortical connectivity might result from self-organisation given some fundamental computational principle, ie. energy efficiency. We thus investigated whether predictive coding properties in a multicompartment spiking neural network can result from energy optimisation. We found that only the model trained with an energy objective in addition to a task-relevant objective was able to reconstruct internal representations given top-down expectation signals alone. Neurons in the energy-optimised model also showed differential responses to expected vs unexpected stimuli, qualitatively similar to experimental evidence for predictive coding. These findings indicated that predictive-coding-like behaviour might be an emergent property of energy optimisation, providing a new perspective on how predictive coding could be achieved in the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e5fUxmZcxfc",
    "outputId": "87a4b8e1-ab15-4a24-dc7e-20a6796837c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set seed\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RlhsaGdOj55t"
   },
   "outputs": [],
   "source": [
    "## Utils\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, prefix, filename='_rec2_bias_checkpoint.pth.tar'):\n",
    "    print('saving at ', prefix + filename)\n",
    "    torch.save(state, prefix + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(prefix + filename, prefix + '_rec2_bias_model_best.pth.tar')\n",
    "\n",
    "\n",
    "def model_result_dict_load(fn):\n",
    "    \"\"\"load tar file with saved model\n",
    "\n",
    "    Args:\n",
    "        fn (str): tar file name\n",
    "\n",
    "    Returns:\n",
    "        dict: dictornary containing saved results\n",
    "    \"\"\"\n",
    "    with open(fn, 'rb') as f:\n",
    "        dict = torch.load(f)\n",
    "    return dict\n",
    "\n",
    "def save_model(model_name,model):\n",
    "    torch.save(model,\"{}_model.pth\".format(model_name))\n",
    "    torch.save(model.state_dict(),\"{}_state_dict.pth\".format(model_name))\n",
    "\n",
    "def load_model(model_name):\n",
    "    model=torch.load(\".\\\\{}_model.pth\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p7KkVxPfe8q"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNZglAYUfXQF",
    "outputId": "1e81f699-656e-425f-b289-71eeff15ab2a"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "traindata = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testdata = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# data loading\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfP3cI8BfnoK"
   },
   "source": [
    "## Surrogate gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1KJqRTDNgaqj"
   },
   "outputs": [],
   "source": [
    "\n",
    "b_j0 = 0.1  # neural threshold baseline\n",
    "\n",
    "R_m = 3  # membrane resistance\n",
    "gamma = .5  # gradient scale\n",
    "lens = 0.5\n",
    "\n",
    "\n",
    "def gaussian(x, mu=0., sigma=.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # temp = abs(input) < lens\n",
    "        scale = 6.0\n",
    "        hight = .15\n",
    "        # temp = torch.exp(-(input**2)/(2*lens**2))/torch.sqrt(2*torch.tensor(math.pi))/lens\n",
    "        temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "               - gaussian(input, mu=lens, sigma=scale * lens) * hight \\\n",
    "               - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        # temp =  gaussian(input, mu=0., sigma=lens)\n",
    "        return grad_input * temp.float() * gamma\n",
    "        # return grad_input\n",
    "\n",
    "\n",
    "act_fun_adp = ActFun_adp.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gAwZBV_5gKUS"
   },
   "outputs": [],
   "source": [
    "# layers\n",
    "def shifted_sigmoid(currents):\n",
    "    return (1 / (1 + torch.exp(-currents)) - 0.5)/2\n",
    "\n",
    "\n",
    "class SnnLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "\n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = ff \n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, b_t1\n",
    "\n",
    "\n",
    "class SnnLayerRiseTime(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_curr_decay_init=10.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayerRiseTime, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_curr_decay = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_curr_decay, tau_curr_decay_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, current_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        current_decay = torch.exp(-self.dt/self.tau_curr_decay)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "                \n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = current_decay * current_curr + ff\n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, current_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, current_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, current_curr_t1, b_t1\n",
    "        \n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            out_dim: int,\n",
    "            is_fc: bool,\n",
    "            tau_fixed=None,\n",
    "            bias = True,\n",
    "            dt=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        output layer class\n",
    "        :param is_fc: whether integrator is fc to r_out in rec or not\n",
    "        \"\"\"\n",
    "        super(OutputLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.is_fc = is_fc\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_fc:\n",
    "            self.fc = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "        # tau_m\n",
    "        if tau_fixed is None:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim))\n",
    "            nn.init.constant_(self.tau_m, 5)\n",
    "        else:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim), requires_grad=False)\n",
    "            nn.init.constant_(self.tau_m, tau_fixed)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_t, mem_t):\n",
    "        \"\"\"\n",
    "        integrator neuron without spikes\n",
    "        \"\"\"\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "\n",
    "        if self.is_fc:\n",
    "            x_t = self.fc(x_t)\n",
    "        else:\n",
    "            x_t = x_t.view(-1, 10, int(self.in_dim / 10)).mean(dim=2)  # sum up population spike\n",
    "\n",
    "        # d_mem = -soma_t + x_t\n",
    "        mem = (mem_t + x_t) * alpha\n",
    "        # mem = alpha * soma_t + (1 - alpha) * x_t\n",
    "        return mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hpf_RNkHfknR"
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers\n",
    "class Decorrelation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decorrelation, self).__init__()\n",
    "        self.decorr_matrix_next = None\n",
    "    \n",
    "    def forward(self, input, decorr_matrix_prev_batch):\n",
    "        n=1e-3\n",
    "        diag = torch.diag_embed(torch.square(input)) # (batch_size,hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1],1) # (batch_size,hidden_dim,1)\n",
    "        input = torch.matmul(decorr_matrix_prev_batch, input) # (batch_size,hidden_dim,1)\n",
    "\n",
    "        mult = torch.matmul(input, torch.transpose(input,1,2)) # (batch_size,hidden_dim,hidden_dim)\n",
    "        update = torch.mean(mult - diag, dim=0) # (hidden_dim,hidden_dim)\n",
    "        self.decorr_matrix_next = decorr_matrix_prev_batch - n * torch.matmul(update, decorr_matrix_prev_batch) # (hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1]) # (batch_size,hidden_dim)\n",
    "        return input\n",
    "        \n",
    "class SnnNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnNetwork, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dim = out_dim\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.is_rec = is_rec\n",
    "        self.rise_time = rise_time\n",
    "        self.dp = nn.Dropout(dp_rate)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer1 = SnnLayerRiseTime(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer1 = SnnLayer(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        # r in to r out\n",
    "        self.layer1to2 = nn.Linear(hidden_dims[0], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer1to2.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer2to1 = nn.Linear(hidden_dims[1], hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to1.weight)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer2 = SnnLayerRiseTime(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer2 = SnnLayer(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[1], out_dim, is_fc=True, bias=bias)\n",
    "\n",
    "        self.out2layer2 = nn.Linear(out_dim, hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer2.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer1to2.bias, 0)\n",
    "            nn.init.constant_(self.layer2to1.bias, 0)\n",
    "            nn.init.constant_(self.out2layer2.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "        self.fr_layer2 = 0\n",
    "        self.fr_layer1 = 0\n",
    "\n",
    "        self.error1 = 0\n",
    "        self.error2 = 0\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t*0.5)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.out2layer2(F.normalize(h[-1], dim=1)), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_2, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def inference(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def inference_rise_time(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward_rise_time(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward_rise_time(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate_rise_time(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward_rise_time(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # r\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # p\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "\n",
    "# 3 hidden layers\n",
    "\n",
    "class SnnNetwork3Layer(SnnNetwork):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super().__init__(in_dim, hidden_dims, out_dim, is_adapt, one_to_one, dp_rate, is_rec, rise_time)\n",
    "\n",
    "        # decorrelation\n",
    "        self.decorr_layer_0 = Decorrelation()\n",
    "        self.decorr_layer_1 = Decorrelation()\n",
    "        self.decorr_layer_2 = Decorrelation()\n",
    "        self.decorr_layer_3 = Decorrelation()\n",
    "        self.decorr_layer_4 = Decorrelation()\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer3 = SnnLayerRiseTime(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer3 = SnnLayer(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "            \n",
    "        self.layer2to3 = nn.Linear(hidden_dims[1], hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to3.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer3to2 = nn.Linear(hidden_dims[2], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer3to2.weight)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[2], out_dim, is_fc=True)\n",
    "\n",
    "        self.out2layer3 = nn.Linear(out_dim, hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer3.weight)\n",
    "\n",
    "        self.fr_layer3 = 0\n",
    "\n",
    "        self.error3 = 0\n",
    "\n",
    "        self.input_fc = nn.Linear(in_dim, hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.input_fc.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer2to3.bias, 0)\n",
    "            nn.init.constant_(self.layer3to2.bias, 0)\n",
    "            nn.init.constant_(self.out2layer3.bias, 0)\n",
    "            nn.init.constant_(self.input_fc.bias, 0)\n",
    "            print('bias set to 0')\n",
    "\n",
    "    def forward_rise_time(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, current_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[6]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], current_curr_t=h[3], b_t=h[4])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, current_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[11]), soma_t=h[5],\n",
    "                                                   spk_t=h[6], a_curr_t=h[7], current_curr_t=h[8], b_t=h[9])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, current_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[10],\n",
    "                                                   spk_t=h[11], a_curr_t=h[12], current_curr_t=h[13], b_t=h[14])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, current_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, current_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, current_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "        \n",
    "    def forward_decorrelation(self, x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        # decorrelate input\n",
    "        x_t = self.decorr_layer_0(x_t, decorr_matrix_0)\n",
    "        decorr_matrix_0 = self.decorr_layer_0.decorr_matrix_next.data.clone()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        # decorrelate input to L1\n",
    "        x_t = self.decorr_layer_1(x_t, decorr_matrix_1)\n",
    "        decorr_matrix_1 = self.decorr_layer_1.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # decorrelate input to L2\n",
    "        spk_1 = self.decorr_layer_2(spk_1, decorr_matrix_2)\n",
    "        decorr_matrix_2 = self.decorr_layer_2.decorr_matrix_next.data.clone()\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        # decorrelate input to L3\n",
    "        spk_2 = self.decorr_layer_3(spk_2, decorr_matrix_3)\n",
    "        decorr_matrix_3 = self.decorr_layer_3.decorr_matrix_next.data.clone()\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        # decorrelate input to output layer\n",
    "        spk_3 = self.decorr_layer_4(spk_3, decorr_matrix_4)\n",
    "        decorr_matrix_4 = self.decorr_layer_4.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "        return log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "    def inference_decorrelation(self, x_t, h, T, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            else:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t[t], h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def init_hidden_rise_time(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "        \n",
    "    def init_hidden_allzero(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def clamp_withnoise(self, test_class, zeros, h_clamped, T, noise, index, batch=False, clamp_value=0.5):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param noise: noise values\n",
    "        :param index: index in h where noise is added to\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                h_clamped[index][:, :] += noise * h_clamped[index][:, :]\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8p3rNn-kPx7"
   },
   "source": [
    "## FTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VOFtn6gOkPfk"
   },
   "outputs": [],
   "source": [
    "alpha = .2\n",
    "beta = .5\n",
    "rho = 0.\n",
    "\n",
    "\n",
    "# %%\n",
    "def get_stats_named_params(model):\n",
    "    named_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        sm, lm, dm = param.detach().clone(), 0.0 * param.detach().clone(), 0.0 * param.detach().clone()\n",
    "        named_params[name] = (param, sm, lm, dm)\n",
    "    return named_params\n",
    "\n",
    "\n",
    "def post_optimizer_updates(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        lm.data.add_(-alpha * (param - sm))\n",
    "        sm.data.mul_((1.0 - beta))\n",
    "        sm.data.add_(beta * param - (beta / alpha) * lm)\n",
    "\n",
    "\n",
    "def get_regularizer_named_params(named_params, _lambda=1.0):\n",
    "    regularization = torch.zeros([], device=device)\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        regularization += (rho - 1.) * torch.sum(param * lm)\n",
    "        r_p = _lambda * 0.5 * alpha * torch.sum(torch.square(param - sm))\n",
    "        regularization += r_p\n",
    "        # print(name,r_p)\n",
    "    return regularization\n",
    "\n",
    "\n",
    "def reset_named_params(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        param.data.copy_(sm.data)\n",
    "\n",
    "\n",
    "def train_fptt(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, rise_time):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    spk3,memout=[],[]\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "            \n",
    "            if p == 0:\n",
    "                if rise_time:\n",
    "                    h = model.init_hidden_rise_time(data.size(0))\n",
    "                else:\n",
    "                    h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            if rise_time:\n",
    "                o, h = model.forward_rise_time(data, h)\n",
    "            else:\n",
    "                o, h = model.forward(data, h)\n",
    "            #print(\"\\n\\nbatch\",batch_idx)\n",
    "            #print(\"\\np\",p)\n",
    "            #print(\"\\nh1\",h[1],\"\\nh6\",h[6],\"\\nh11\",h[11])\n",
    "            #print(\"\\nmemout\",h[-1])\n",
    "            #spk3.append(h[11])\n",
    "            #memout.append(h[-1])\n",
    "\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                #print(\"k updates\",k_updates,F.nll_loss(o, target),\"o\",o,o.shape,\"target\",target,target.shape)\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                #print(\"Loss\", clf_loss, regularizer, energy, spike_loss)\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "\n",
    "def train_fptt_decorr(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "\n",
    "            if p == 0:\n",
    "                h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            o, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = model.forward_decorrelation(data, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "    return decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCAgkJ3hkxPH"
   },
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WL8XFlKykz7K"
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_with_added_noise(model, test_loader, time_steps, noise_mean, noise_std):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data += torch.randn(data.size()) * noise_std + noise_mean\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "def test_rise_time(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden_rise_time(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_rise_time(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_decorrelation(model, test_loader, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_decorrelation(data, hidden, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_DORsdsg-TS"
   },
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oaqFPBCFg0vF"
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "adap_neuron = True  # whether use adaptive neuron or not\n",
    "clf_alpha = 1\n",
    "\n",
    "model_type = \"energy\"\n",
    "if model_type == \"control\":\n",
    "    energy_alpha = 0\n",
    "else:\n",
    "    energy_alpha = 0.05\n",
    "    \n",
    "spike_alpha = 0.  # energy loss on spikes\n",
    "num_readout = 10\n",
    "onetoone = True\n",
    "lr = 1e-3\n",
    "alg = 'fptt'\n",
    "dp = 0.4\n",
    "is_rec = False\n",
    "\n",
    "# training parameters\n",
    "T = 50\n",
    "K = 10  # k_updates is num updates per sequence\n",
    "omega = int(T / K)  # update frequency\n",
    "clip = 1.\n",
    "log_interval = 20\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias set to 0\n",
      "SnnNetwork3Layer(\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      "  (layer1): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer1to2): Linear(in_features=600, out_features=500, bias=True)\n",
      "  (layer2to1): Linear(in_features=500, out_features=600, bias=True)\n",
      "  (layer2): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (fc): Linear(in_features=500, out_features=10, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (out2layer2): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (decorr_layer_0): Decorrelation()\n",
      "  (decorr_layer_1): Decorrelation()\n",
      "  (decorr_layer_2): Decorrelation()\n",
      "  (decorr_layer_3): Decorrelation()\n",
      "  (decorr_layer_4): Decorrelation()\n",
      "  (layer3): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer2to3): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (layer3to2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (out2layer3): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (input_fc): Linear(in_features=784, out_features=600, bias=True)\n",
      ")\n",
      "total param count 2455520\n"
     ]
    }
   ],
   "source": [
    "# set input and t param\n",
    "IN_dim = 784\n",
    "hidden_dim = [600, 500, 500]\n",
    "n_classes = 10\n",
    "rise_time=False\n",
    "\n",
    "# define network\n",
    "model = SnnNetwork3Layer(IN_dim, hidden_dim, n_classes, is_adapt=adap_neuron,\n",
    "                         one_to_one=onetoone, dp_rate=dp, is_rec=is_rec, rise_time=rise_time)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# define new loss and optimiser\n",
    "total_params = count_parameters(model)\n",
    "print('total param count %i' % total_params)\n",
    "\n",
    "# define optimiser\n",
    "optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "# reduce the learning after 20 epochs by a factor of 10\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-seG48koNP"
   },
   "source": [
    "## Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 4.4367, Accuracy: 715/10000 (7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# untrained network\n",
    "test_loss, acc1, test_energy = test(model, test_loader, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2b with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq_ifdLDkDY4",
    "outputId": "988e9a43-d50f-4f14-b933-e952d1a650c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7753, Accuracy: 7707/10000 (77%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3108, Accuracy: 8998/10000 (90%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6421, Accuracy: 8149/10000 (81%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.3822, Accuracy: 8883/10000 (89%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2453, Accuracy: 9292/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.2180, Accuracy: 9326/10000 (93%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1380, Accuracy: 9590/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1212, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1163, Accuracy: 9669/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1272, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1073, Accuracy: 9681/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0902, Accuracy: 9745/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0982, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0930, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9716/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0981, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9743/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0975, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0971, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0975, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0974, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0978, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0978, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0970, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0973, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0974, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0971, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0980, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0971, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0929, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9727/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "named_params = get_stats_named_params(model)\n",
    "all_test_losses = []\n",
    "all_test_acc = []\n",
    "all_test_energy = []\n",
    "seeds = [3,7,11,55,79,101,123,304,709,999]\n",
    "energy_all_test_acc, control_all_test_acc = [], []\n",
    "add_noise = False\n",
    "noise_mean, noise_std = 0, 0.1\n",
    "\n",
    "for seed in seeds:\n",
    "    for model_type in [\"energy\",\"control\"]:\n",
    "        for epoch in range(epochs):\n",
    "            model_name = \"results/{}_seed{}_\".format(alg,seed) + model_type + \"/{}_model.pth\".format(epoch+1)\n",
    "            model = torch.load(model_name, weights_only=False)\n",
    "\n",
    "            if add_noise:\n",
    "                # add Gaussian noise to the input data\n",
    "                test_loss, test_acc, test_energy = test_with_added_noise(model, test_loader, T, noise_mean, noise_std)\n",
    "            else:\n",
    "                test_loss, test_acc, test_energy = test(model, test_loader, T)\n",
    "                \n",
    "            if model_type == \"energy\":\n",
    "                energy_all_test_acc.append(test_acc)\n",
    "            elif model_type == \"control\":\n",
    "                control_all_test_acc.append(test_acc)\n",
    "\n",
    "energy_all_test_acc = np.array(energy_all_test_acc).reshape(len(seeds),epochs)\n",
    "control_all_test_acc = np.array(control_all_test_acc).reshape(len(seeds),epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.gca()\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "errors_control = 100*(10000-100*np.array(control_all_test_acc))/10000\n",
    "errors_energy = 100*(10000-100*np.array(energy_all_test_acc))/10000\n",
    "\n",
    "means_control = errors_control.mean(axis=0)\n",
    "std_control = errors_control.std(axis=0)\n",
    "\n",
    "means_energy = errors_energy.mean(axis=0)\n",
    "std_energy = errors_energy.std(axis=0)\n",
    "\n",
    "plt.plot(means_control,label=\"Control\")\n",
    "plt.fill_between(range(15), means_control - std_control, means_control + std_control, color='b', alpha=0.2)\n",
    "\n",
    "plt.plot(means_energy,label=\"Energy\")\n",
    "plt.fill_between(range(15), means_energy - std_energy, means_energy + std_energy, color='orange', alpha=0.2)\n",
    "\n",
    "plt.legend(fontsize=17,frameon=False)\n",
    "\n",
    "plt.xticks([4,9,14],[5,10,15],fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.title('Test error rate during training',fontsize=20)\n",
    "plt.xlabel('Epoch',fontsize=20)\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.savefig('2b_test_error_rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2d with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each seed, extract the model at the epoch that first reaches 97% accuracy\n",
    "\n",
    "def get_epochs_97_acc(model_type):\n",
    "    models = []\n",
    "    all_test_accs = []\n",
    "    seeds = [3,7,11,55,79,101,123,304,709,999]\n",
    "    epochs = []\n",
    "    for seed in seeds:\n",
    "        idx = 0\n",
    "        path = \"results/{}_seed{}_{}/\".format(alg,seed,model_type)\n",
    "        test_accs = np.load(path+\"test_acc.npy\")\n",
    "        all_test_accs.append(test_accs)\n",
    "        if model_type=='control' and seed==7:\n",
    "            idx = 9\n",
    "        elif model_type=='control' and seed==999:\n",
    "            idx = 9\n",
    "        else:\n",
    "            for idx,acc in enumerate(test_accs[:15]):\n",
    "                if round(acc) == 97:\n",
    "                    break\n",
    "\n",
    "        epochs.append(idx+1)\n",
    "        model = torch.load(path+\"{}_model.pth\".format(idx+1))\n",
    "        models.append(model)\n",
    "\n",
    "    return epochs, np.array(all_test_accs), models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2750915/3220343608.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(path+\"{}_model.pth\".format(idx+1))\n"
     ]
    }
   ],
   "source": [
    "epochs_control, control_all_test_acc, control_models = get_epochs_97_acc('control')\n",
    "epochs_energy, energy_all_test_acc, energy_models = get_epochs_97_acc('energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get energy & spikes of the 2 models\n",
    "def get_energy(model, test_loader, time_steps, add_noise, noise_std, noise_mean):\n",
    "    model.eval()\n",
    "\n",
    "    spikes1, spikes2, spikes3 = [],[],[]\n",
    "    errors1,errors2,errors3=[],[],[]\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        if add_noise:\n",
    "            data += torch.randn(data.size()) * noise_std + noise_mean\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            errors_batch1=[]\n",
    "            errors_batch2=[]\n",
    "            errors_batch3=[]\n",
    "            spikes_batch1=[]\n",
    "            spikes_batch2=[]\n",
    "            spikes_batch3=[]\n",
    "            \n",
    "            #log_softmax_outputs, hidden, errors = model.inference(data, hidden, time_steps)\n",
    "            for t in range(time_steps):\n",
    "                log_softmax, hidden = model.forward(data, hidden)\n",
    "\n",
    "                errors_batch1.append(torch.abs(hidden[0]-hidden[2]).sum())\n",
    "                errors_batch2.append(torch.abs(hidden[4]-hidden[6]).sum())\n",
    "                errors_batch3.append(torch.abs(hidden[8]-hidden[10]).sum())\n",
    "                \n",
    "                spikes_batch1.append(torch.abs(hidden[1]).sum())\n",
    "                spikes_batch2.append(torch.abs(hidden[5]).sum())\n",
    "                spikes_batch3.append(torch.abs(hidden[9]).sum())\n",
    "                \n",
    "        errors1.append(torch.stack(errors_batch1))\n",
    "        errors2.append(torch.stack(errors_batch2))\n",
    "        errors3.append(torch.stack(errors_batch3))\n",
    "\n",
    "        spikes1.append(torch.stack(spikes_batch1))\n",
    "        spikes2.append(torch.stack(spikes_batch1))\n",
    "        spikes3.append(torch.stack(spikes_batch1))\n",
    "        \n",
    "    errors1 = torch.stack(errors1)\n",
    "    errors2 = torch.stack(errors2)\n",
    "    errors3 = torch.stack(errors3)\n",
    "    spikes1 = torch.stack(spikes1)\n",
    "    spikes2 = torch.stack(spikes2)\n",
    "    spikes3 = torch.stack(spikes3)\n",
    "\n",
    "    #return  spikes1/len(test_loader.dataset)/hidden_dim[0], spikes2/len(test_loader.dataset)/hidden_dim[1], spikes3/len(test_loader.dataset)/hidden_dim[2], torch.sum(energy1/len(test_loader.dataset))/energy1.shape[1], torch.sum(energy2/len(test_loader.dataset))/energy2.shape[1], torch.sum(energy3/len(test_loader.dataset))/energy2.shape[1]\n",
    "    #return  errors, spikes1/len(test_loader.dataset)/hidden_dim[0], spikes2/len(test_loader.dataset)/hidden_dim[1], spikes3/len(test_loader.dataset)/hidden_dim[2], torch.sum(energy1/len(test_loader.dataset))/energy1.shape[1], torch.sum(energy2/len(test_loader.dataset))/energy2.shape[1], torch.sum(energy3/len(test_loader.dataset))/energy2.shape[1]\n",
    "    return (torch.sum(errors1/len(test_loader.dataset)/hidden_dim[0],axis=0).cpu().detach().numpy(),\n",
    "           torch.sum(errors2/len(test_loader.dataset)/hidden_dim[1],axis=0).cpu().detach().numpy(),\n",
    "            torch.sum(errors3/len(test_loader.dataset)/hidden_dim[2],axis=0).cpu().detach().numpy(),\n",
    "            torch.sum(spikes1).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[0]/time_steps,\n",
    "            torch.sum(spikes2).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[1]/time_steps,\n",
    "            torch.sum(spikes3).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[2]/time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise, noise_mean, noise_std = False, 0, 0.1\n",
    "control_energy1_all,control_energy2_all,control_energy3_all = [],[],[]\n",
    "control_spikes1_all,control_spikes2_all,control_spikes3_all = [],[],[]\n",
    "for control_model in control_models:\n",
    "    control_energy1, control_energy2, control_energy3, control_spikes1, control_spikes2, control_spikes3 = get_energy(control_model, test_loader, T, add_noise, noise_std, noise_mean)\n",
    "    control_energy1_all.append(control_energy1)\n",
    "    control_energy2_all.append(control_energy2)\n",
    "    control_energy3_all.append(control_energy3)\n",
    "    control_spikes1_all.append(control_spikes1)\n",
    "    control_spikes2_all.append(control_spikes2)\n",
    "    control_spikes3_all.append(control_spikes3)\n",
    "energy_energy1_all,energy_energy2_all,energy_energy3_all = [],[],[]\n",
    "energy_spikes1_all,energy_spikes2_all,energy_spikes3_all = [],[],[]\n",
    "for energy_model in energy_models:\n",
    "    energy_energy1, energy_energy2, energy_energy3, energy_spikes1, energy_spikes2, energy_spikes3 = get_energy(energy_model, test_loader, T, add_noise, noise_std, noise_mean)\n",
    "    energy_energy1_all.append(energy_energy1)\n",
    "    energy_energy2_all.append(energy_energy2)\n",
    "    energy_energy3_all.append(energy_energy3)\n",
    "    energy_spikes1_all.append(energy_spikes1)\n",
    "    energy_spikes2_all.append(energy_spikes2)\n",
    "    energy_spikes3_all.append(energy_spikes3)\n",
    "\n",
    "control_energy1_all = np.array(control_energy1_all)\n",
    "control_energy2_all = np.array(control_energy2_all)\n",
    "control_energy3_all = np.array(control_energy3_all)\n",
    "control_spikes1_all = np.array(control_spikes1_all)\n",
    "control_spikes2_all = np.array(control_spikes2_all)\n",
    "control_spikes3_all = np.array(control_spikes3_all)\n",
    "\n",
    "energy_energy1_all = np.array(energy_energy1_all)\n",
    "energy_energy2_all = np.array(energy_energy2_all)\n",
    "energy_energy3_all = np.array(energy_energy3_all)\n",
    "energy_spikes1_all = np.array(energy_spikes1_all)\n",
    "energy_spikes2_all = np.array(energy_spikes2_all)\n",
    "energy_spikes3_all = np.array(energy_spikes3_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Barplots of mean spike rates \n",
    "\n",
    "ax = plt.gca()\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "pastel_blue = palette[0]\n",
    "pastel_orange = palette[1]\n",
    "\n",
    "X_axis = np.arange(3)\n",
    "plt.bar(X_axis-0.2,[control_spikes1_all.mean(), control_spikes2_all.mean(), control_spikes3_all.mean()],0.4,label=\"Control\",color=pastel_blue)\n",
    "plt.bar(X_axis+0.2,[energy_spikes1_all.mean(), energy_spikes2_all.mean(), energy_spikes3_all.mean()],0.4,label=\"Energy\",color=pastel_orange)\n",
    "\n",
    "labels=[\"1\",\"2\",\"3\"]\n",
    "plt.xticks(X_axis, labels, fontsize=20) \n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"L\",fontsize=20) \n",
    "plt.ylabel(\"Mean R\",fontsize=20) \n",
    "#plt.title(\"Average spike rate over all samples\",fontsize=15) \n",
    "plt.legend(frameon=False,fontsize=17,loc='upper left',bbox_to_anchor=(0, 0., 0, 1.05)) \n",
    "plt.savefig('10_models_graphs\\\\2d_average_spike_rate.png')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SNN_PC_Multicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
