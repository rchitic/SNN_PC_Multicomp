{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMA-943Aev9e"
   },
   "source": [
    "# Spiking multicompartment PC network\n",
    "\n",
    "## Abstract\n",
    "Predictive coding is a promising theoretical framework for understanding the hierarchical sensory processing in the brain, yet how it is implemented with cortical spiking neurons is still unclear. While most existing works have taken a hand-wiring approach to creating microcircuits which match experimental results, recent work in applying the optimisation approach revealed that cortical connectivity might result from self-organisation given some fundamental computational principle, ie. energy efficiency. We thus investigated whether predictive coding properties in a multicompartment spiking neural network can result from energy optimisation. We found that only the model trained with an energy objective in addition to a task-relevant objective was able to reconstruct internal representations given top-down expectation signals alone. Neurons in the energy-optimised model also showed differential responses to expected vs unexpected stimuli, qualitatively similar to experimental evidence for predictive coding. These findings indicated that predictive-coding-like behaviour might be an emergent property of energy optimisation, providing a new perspective on how predictive coding could be achieved in the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e5fUxmZcxfc",
    "outputId": "87a4b8e1-ab15-4a24-dc7e-20a6796837c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set seed\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RlhsaGdOj55t"
   },
   "outputs": [],
   "source": [
    "## Utils\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, prefix, filename='_rec2_bias_checkpoint.pth.tar'):\n",
    "    print('saving at ', prefix + filename)\n",
    "    torch.save(state, prefix + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(prefix + filename, prefix + '_rec2_bias_model_best.pth.tar')\n",
    "\n",
    "\n",
    "def model_result_dict_load(fn):\n",
    "    \"\"\"load tar file with saved model\n",
    "\n",
    "    Args:\n",
    "        fn (str): tar file name\n",
    "\n",
    "    Returns:\n",
    "        dict: dictornary containing saved results\n",
    "    \"\"\"\n",
    "    with open(fn, 'rb') as f:\n",
    "        dict = torch.load(f)\n",
    "    return dict\n",
    "\n",
    "def save_model(model_name,model):\n",
    "    torch.save(model,\"{}_model.pth\".format(model_name))\n",
    "    torch.save(model.state_dict(),\"{}_state_dict.pth\".format(model_name))\n",
    "\n",
    "def load_model(model_name):\n",
    "    model=torch.load(\".\\\\{}_model.pth\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p7KkVxPfe8q"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNZglAYUfXQF",
    "outputId": "1e81f699-656e-425f-b289-71eeff15ab2a"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "traindata = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testdata = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# data loading\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfP3cI8BfnoK"
   },
   "source": [
    "## Surrogate gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1KJqRTDNgaqj"
   },
   "outputs": [],
   "source": [
    "\n",
    "b_j0 = 0.1  # neural threshold baseline\n",
    "\n",
    "R_m = 3  # membrane resistance\n",
    "gamma = .5  # gradient scale\n",
    "lens = 0.5\n",
    "\n",
    "\n",
    "def gaussian(x, mu=0., sigma=.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # temp = abs(input) < lens\n",
    "        scale = 6.0\n",
    "        hight = .15\n",
    "        # temp = torch.exp(-(input**2)/(2*lens**2))/torch.sqrt(2*torch.tensor(math.pi))/lens\n",
    "        temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "               - gaussian(input, mu=lens, sigma=scale * lens) * hight \\\n",
    "               - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        # temp =  gaussian(input, mu=0., sigma=lens)\n",
    "        return grad_input * temp.float() * gamma\n",
    "        # return grad_input\n",
    "\n",
    "\n",
    "act_fun_adp = ActFun_adp.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gAwZBV_5gKUS"
   },
   "outputs": [],
   "source": [
    "# layers\n",
    "def shifted_sigmoid(currents):\n",
    "    return (1 / (1 + torch.exp(-currents)) - 0.5)/2\n",
    "\n",
    "\n",
    "class SnnLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "\n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = ff \n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, b_t1\n",
    "\n",
    "\n",
    "class SnnLayerRiseTime(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_curr_decay_init=10.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayerRiseTime, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_curr_decay = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_curr_decay, tau_curr_decay_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, current_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        current_decay = torch.exp(-self.dt/self.tau_curr_decay)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "                \n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = current_decay * current_curr + ff\n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, current_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, current_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, current_curr_t1, b_t1\n",
    "        \n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            out_dim: int,\n",
    "            is_fc: bool,\n",
    "            tau_fixed=None,\n",
    "            bias = True,\n",
    "            dt=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        output layer class\n",
    "        :param is_fc: whether integrator is fc to r_out in rec or not\n",
    "        \"\"\"\n",
    "        super(OutputLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.is_fc = is_fc\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_fc:\n",
    "            self.fc = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "        # tau_m\n",
    "        if tau_fixed is None:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim))\n",
    "            nn.init.constant_(self.tau_m, 5)\n",
    "        else:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim), requires_grad=False)\n",
    "            nn.init.constant_(self.tau_m, tau_fixed)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_t, mem_t):\n",
    "        \"\"\"\n",
    "        integrator neuron without spikes\n",
    "        \"\"\"\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "\n",
    "        if self.is_fc:\n",
    "            x_t = self.fc(x_t)\n",
    "        else:\n",
    "            x_t = x_t.view(-1, 10, int(self.in_dim / 10)).mean(dim=2)  # sum up population spike\n",
    "\n",
    "        # d_mem = -soma_t + x_t\n",
    "        mem = (mem_t + x_t) * alpha\n",
    "        # mem = alpha * soma_t + (1 - alpha) * x_t\n",
    "        return mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hpf_RNkHfknR"
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers\n",
    "class Decorrelation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decorrelation, self).__init__()\n",
    "        self.decorr_matrix_next = None\n",
    "    \n",
    "    def forward(self, input, decorr_matrix_prev_batch):\n",
    "        n=1e-3\n",
    "        diag = torch.diag_embed(torch.square(input)) # (batch_size,hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1],1) # (batch_size,hidden_dim,1)\n",
    "        input = torch.matmul(decorr_matrix_prev_batch, input) # (batch_size,hidden_dim,1)\n",
    "\n",
    "        mult = torch.matmul(input, torch.transpose(input,1,2)) # (batch_size,hidden_dim,hidden_dim)\n",
    "        update = torch.mean(mult - diag, dim=0) # (hidden_dim,hidden_dim)\n",
    "        self.decorr_matrix_next = decorr_matrix_prev_batch - n * torch.matmul(update, decorr_matrix_prev_batch) # (hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1]) # (batch_size,hidden_dim)\n",
    "        return input\n",
    "        \n",
    "class SnnNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnNetwork, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dim = out_dim\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.is_rec = is_rec\n",
    "        self.rise_time = rise_time\n",
    "        self.dp = nn.Dropout(dp_rate)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer1 = SnnLayerRiseTime(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer1 = SnnLayer(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        # r in to r out\n",
    "        self.layer1to2 = nn.Linear(hidden_dims[0], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer1to2.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer2to1 = nn.Linear(hidden_dims[1], hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to1.weight)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer2 = SnnLayerRiseTime(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer2 = SnnLayer(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[1], out_dim, is_fc=True, bias=bias)\n",
    "\n",
    "        self.out2layer2 = nn.Linear(out_dim, hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer2.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer1to2.bias, 0)\n",
    "            nn.init.constant_(self.layer2to1.bias, 0)\n",
    "            nn.init.constant_(self.out2layer2.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "        self.fr_layer2 = 0\n",
    "        self.fr_layer1 = 0\n",
    "\n",
    "        self.error1 = 0\n",
    "        self.error2 = 0\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t*0.5)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.out2layer2(F.normalize(h[-1], dim=1)), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_2, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def inference(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def inference_rise_time(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward_rise_time(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward_rise_time(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate_rise_time(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward_rise_time(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # r\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # p\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "\n",
    "# 3 hidden layers\n",
    "\n",
    "class SnnNetwork3Layer(SnnNetwork):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super().__init__(in_dim, hidden_dims, out_dim, is_adapt, one_to_one, dp_rate, is_rec, rise_time)\n",
    "\n",
    "        # decorrelation\n",
    "        self.decorr_layer_0 = Decorrelation()\n",
    "        self.decorr_layer_1 = Decorrelation()\n",
    "        self.decorr_layer_2 = Decorrelation()\n",
    "        self.decorr_layer_3 = Decorrelation()\n",
    "        self.decorr_layer_4 = Decorrelation()\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer3 = SnnLayerRiseTime(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer3 = SnnLayer(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "            \n",
    "        self.layer2to3 = nn.Linear(hidden_dims[1], hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to3.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer3to2 = nn.Linear(hidden_dims[2], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer3to2.weight)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[2], out_dim, is_fc=True)\n",
    "\n",
    "        self.out2layer3 = nn.Linear(out_dim, hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer3.weight)\n",
    "\n",
    "        self.fr_layer3 = 0\n",
    "\n",
    "        self.error3 = 0\n",
    "\n",
    "        self.input_fc = nn.Linear(in_dim, hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.input_fc.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer2to3.bias, 0)\n",
    "            nn.init.constant_(self.layer3to2.bias, 0)\n",
    "            nn.init.constant_(self.out2layer3.bias, 0)\n",
    "            nn.init.constant_(self.input_fc.bias, 0)\n",
    "            print('bias set to 0')\n",
    "\n",
    "    def forward_rise_time(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, current_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[6]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], current_curr_t=h[3], b_t=h[4])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, current_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[11]), soma_t=h[5],\n",
    "                                                   spk_t=h[6], a_curr_t=h[7], current_curr_t=h[8], b_t=h[9])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, current_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[10],\n",
    "                                                   spk_t=h[11], a_curr_t=h[12], current_curr_t=h[13], b_t=h[14])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, current_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, current_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, current_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "        \n",
    "    def forward_decorrelation(self, x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        # decorrelate input\n",
    "        x_t = self.decorr_layer_0(x_t, decorr_matrix_0)\n",
    "        decorr_matrix_0 = self.decorr_layer_0.decorr_matrix_next.data.clone()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        # decorrelate input to L1\n",
    "        x_t = self.decorr_layer_1(x_t, decorr_matrix_1)\n",
    "        decorr_matrix_1 = self.decorr_layer_1.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # decorrelate input to L2\n",
    "        spk_1 = self.decorr_layer_2(spk_1, decorr_matrix_2)\n",
    "        decorr_matrix_2 = self.decorr_layer_2.decorr_matrix_next.data.clone()\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        # decorrelate input to L3\n",
    "        spk_2 = self.decorr_layer_3(spk_2, decorr_matrix_3)\n",
    "        decorr_matrix_3 = self.decorr_layer_3.decorr_matrix_next.data.clone()\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        # decorrelate input to output layer\n",
    "        spk_3 = self.decorr_layer_4(spk_3, decorr_matrix_4)\n",
    "        decorr_matrix_4 = self.decorr_layer_4.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "        return log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "    def inference_decorrelation(self, x_t, h, T, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            else:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t[t], h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def init_hidden_rise_time(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "        \n",
    "    def init_hidden_allzero(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def clamp_withnoise(self, test_class, zeros, h_clamped, T, noise, index, batch=False, clamp_value=0.5):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param noise: noise values\n",
    "        :param index: index in h where noise is added to\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                h_clamped[index][:, :] += noise * h_clamped[index][:, :]\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8p3rNn-kPx7"
   },
   "source": [
    "## FTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VOFtn6gOkPfk"
   },
   "outputs": [],
   "source": [
    "alpha = .2\n",
    "beta = .5\n",
    "rho = 0.\n",
    "\n",
    "\n",
    "# %%\n",
    "def get_stats_named_params(model):\n",
    "    named_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        sm, lm, dm = param.detach().clone(), 0.0 * param.detach().clone(), 0.0 * param.detach().clone()\n",
    "        named_params[name] = (param, sm, lm, dm)\n",
    "    return named_params\n",
    "\n",
    "\n",
    "def post_optimizer_updates(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        lm.data.add_(-alpha * (param - sm))\n",
    "        sm.data.mul_((1.0 - beta))\n",
    "        sm.data.add_(beta * param - (beta / alpha) * lm)\n",
    "\n",
    "\n",
    "def get_regularizer_named_params(named_params, _lambda=1.0):\n",
    "    regularization = torch.zeros([], device=device)\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        regularization += (rho - 1.) * torch.sum(param * lm)\n",
    "        r_p = _lambda * 0.5 * alpha * torch.sum(torch.square(param - sm))\n",
    "        regularization += r_p\n",
    "        # print(name,r_p)\n",
    "    return regularization\n",
    "\n",
    "\n",
    "def reset_named_params(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        param.data.copy_(sm.data)\n",
    "\n",
    "\n",
    "def train_fptt(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, rise_time):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    spk3,memout=[],[]\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "            \n",
    "            if p == 0:\n",
    "                if rise_time:\n",
    "                    h = model.init_hidden_rise_time(data.size(0))\n",
    "                else:\n",
    "                    h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            if rise_time:\n",
    "                o, h = model.forward_rise_time(data, h)\n",
    "            else:\n",
    "                o, h = model.forward(data, h)\n",
    "            #print(\"\\n\\nbatch\",batch_idx)\n",
    "            #print(\"\\np\",p)\n",
    "            #print(\"\\nh1\",h[1],\"\\nh6\",h[6],\"\\nh11\",h[11])\n",
    "            #print(\"\\nmemout\",h[-1])\n",
    "            #spk3.append(h[11])\n",
    "            #memout.append(h[-1])\n",
    "\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                #print(\"k updates\",k_updates,F.nll_loss(o, target),\"o\",o,o.shape,\"target\",target,target.shape)\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                #print(\"Loss\", clf_loss, regularizer, energy, spike_loss)\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "\n",
    "def train_fptt_decorr(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "\n",
    "            if p == 0:\n",
    "                h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            o, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = model.forward_decorrelation(data, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "    return decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCAgkJ3hkxPH"
   },
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WL8XFlKykz7K"
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_with_added_noise(model, test_loader, time_steps, noise_mean, noise_std):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data + torch.randn(data.size()) * noise_std + noise_mean\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "def test_rise_time(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden_rise_time(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_rise_time(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_decorrelation(model, test_loader, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_decorrelation(data, hidden, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_DORsdsg-TS"
   },
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oaqFPBCFg0vF"
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "adap_neuron = True  # whether use adaptive neuron or not\n",
    "clf_alpha = 1\n",
    "\n",
    "model_type = \"energy\"\n",
    "if model_type == \"control\":\n",
    "    energy_alpha = 0\n",
    "else:\n",
    "    energy_alpha = 0.05\n",
    "    \n",
    "spike_alpha = 0.  # energy loss on spikes\n",
    "num_readout = 10\n",
    "onetoone = True\n",
    "lr = 1e-3\n",
    "alg = 'fptt'\n",
    "dp = 0.4\n",
    "is_rec = False\n",
    "\n",
    "# training parameters\n",
    "T = 50\n",
    "K = 10  # k_updates is num updates per sequence\n",
    "omega = int(T / K)  # update frequency\n",
    "clip = 1.\n",
    "log_interval = 20\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias set to 0\n",
      "SnnNetwork3Layer(\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      "  (layer1): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer1to2): Linear(in_features=600, out_features=500, bias=True)\n",
      "  (layer2to1): Linear(in_features=500, out_features=600, bias=True)\n",
      "  (layer2): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (fc): Linear(in_features=500, out_features=10, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (out2layer2): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (decorr_layer_0): Decorrelation()\n",
      "  (decorr_layer_1): Decorrelation()\n",
      "  (decorr_layer_2): Decorrelation()\n",
      "  (decorr_layer_3): Decorrelation()\n",
      "  (decorr_layer_4): Decorrelation()\n",
      "  (layer3): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer2to3): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (layer3to2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (out2layer3): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (input_fc): Linear(in_features=784, out_features=600, bias=True)\n",
      ")\n",
      "total param count 2455520\n"
     ]
    }
   ],
   "source": [
    "# set input and t param\n",
    "IN_dim = 784\n",
    "hidden_dim = [600, 500, 500]\n",
    "n_classes = 10\n",
    "rise_time=False\n",
    "\n",
    "# define network\n",
    "model = SnnNetwork3Layer(IN_dim, hidden_dim, n_classes, is_adapt=adap_neuron,\n",
    "                         one_to_one=onetoone, dp_rate=dp, is_rec=is_rec, rise_time=rise_time)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# define new loss and optimiser\n",
    "total_params = count_parameters(model)\n",
    "print('total param count %i' % total_params)\n",
    "\n",
    "# define optimiser\n",
    "optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "# reduce the learning after 20 epochs by a factor of 10\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-seG48koNP"
   },
   "source": [
    "## Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 4.4367, Accuracy: 715/10000 (7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# untrained network\n",
    "test_loss, acc1, test_energy = test(model, test_loader, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2b with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mq_ifdLDkDY4",
    "outputId": "988e9a43-d50f-4f14-b933-e952d1a650c0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.782908\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.754872                \tClf: 2.891433\tReg: -0.225706\tFr_p: 0.362995\tFr_r: 0.387857\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.787736\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.632127                \tClf: 2.770145\tReg: -0.227405\tFr_p: 0.105229\tFr_r: 0.115431\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.798879\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.614676                \tClf: 2.752177\tReg: -0.227444\tFr_p: 0.105428\tFr_r: 0.115436\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.805074\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.900297                \tClf: 3.037437\tReg: -0.227394\tFr_p: 0.105281\tFr_r: 0.115300\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.785173\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.552729                \tClf: 2.690876\tReg: -0.227406\tFr_p: 0.105173\tFr_r: 0.114569\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.809519\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.601287                \tClf: 2.738216\tReg: -0.227405\tFr_p: 0.105586\tFr_r: 0.115721\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.790207\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.621218                \tClf: 2.759086\tReg: -0.227379\tFr_p: 0.105341\tFr_r: 0.114939\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.783113\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.821077                \tClf: 2.959255\tReg: -0.227333\tFr_p: 0.105387\tFr_r: 0.114801\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.789524\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.622329                \tClf: 2.760213\tReg: -0.227360\tFr_p: 0.105275\tFr_r: 0.114126\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.807676\tlr: 0.001000\ttrain acc:96.9500\tLoss: 2.935252                \tClf: 3.072217\tReg: -0.227349\tFr_p: 0.105525\tFr_r: 0.114677\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.795588\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.683193                \tClf: 2.820765\tReg: -0.227351\tFr_p: 0.105617\tFr_r: 0.115022\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.792661\tlr: 0.001000\ttrain acc:96.9750\tLoss: 2.988541                \tClf: 3.126227\tReg: -0.227320\tFr_p: 0.105794\tFr_r: 0.115306\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.812407\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.840713                \tClf: 2.977406\tReg: -0.227313\tFr_p: 0.106441\tFr_r: 0.116961\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.792432\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.559608                \tClf: 2.697287\tReg: -0.227300\tFr_p: 0.106311\tFr_r: 0.116165\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.786378\tlr: 0.001000\ttrain acc:98.4500\tLoss: 1.966172                \tClf: 2.104133\tReg: -0.227280\tFr_p: 0.105390\tFr_r: 0.113239\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.798518\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.785191                \tClf: 2.861521\tReg: -0.166255\tFr_p: 0.367859\tFr_r: 0.394566\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.804878\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.733972                \tClf: 2.807177\tReg: -0.163449\tFr_p: 0.106528\tFr_r: 0.117174\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.815445\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.703111                \tClf: 2.775830\tReg: -0.163492\tFr_p: 0.106654\tFr_r: 0.116874\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.822014\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.023996                \tClf: 3.096354\tReg: -0.163458\tFr_p: 0.106477\tFr_r: 0.116612\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.803318\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.638111                \tClf: 2.711403\tReg: -0.163458\tFr_p: 0.106586\tFr_r: 0.116438\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.825991\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.761995                \tClf: 2.834178\tReg: -0.163482\tFr_p: 0.106800\tFr_r: 0.117021\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.808858\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.843765                \tClf: 2.916789\tReg: -0.163467\tFr_p: 0.106729\tFr_r: 0.116556\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.799224\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.854600                \tClf: 2.928060\tReg: -0.163421\tFr_p: 0.106637\tFr_r: 0.116044\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.804988\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.608949                \tClf: 2.682140\tReg: -0.163440\tFr_p: 0.106397\tFr_r: 0.115061\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.825811\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.047946                \tClf: 3.120081\tReg: -0.163426\tFr_p: 0.106873\tFr_r: 0.116158\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.813169\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.728425                \tClf: 2.801202\tReg: -0.163435\tFr_p: 0.106996\tFr_r: 0.116545\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.808781\tlr: 0.001000\ttrain acc:96.7750\tLoss: 2.982872                \tClf: 3.055846\tReg: -0.163414\tFr_p: 0.107119\tFr_r: 0.116763\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.827911\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.903385                \tClf: 2.975403\tReg: -0.163414\tFr_p: 0.107719\tFr_r: 0.118244\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.808565\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.739157                \tClf: 2.812150\tReg: -0.163421\tFr_p: 0.107675\tFr_r: 0.117683\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.801986\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.052449                \tClf: 2.125741\tReg: -0.163392\tFr_p: 0.106730\tFr_r: 0.114713\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.813312\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.915031                \tClf: 2.914425\tReg: -0.090059\tFr_p: 0.371853\tFr_r: 0.397451\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.817712\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.778808                \tClf: 2.776285\tReg: -0.088363\tFr_p: 0.107563\tFr_r: 0.117656\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.833891\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.840577                \tClf: 2.837280\tReg: -0.088398\tFr_p: 0.108184\tFr_r: 0.118834\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.838020\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.107372                \tClf: 3.103852\tReg: -0.088380\tFr_p: 0.107821\tFr_r: 0.118071\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.815454\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.750738                \tClf: 2.748344\tReg: -0.088379\tFr_p: 0.107622\tFr_r: 0.117282\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.839128\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.895547                \tClf: 2.891973\tReg: -0.088383\tFr_p: 0.107912\tFr_r: 0.118059\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.821036\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.863326                \tClf: 2.860661\tReg: -0.088387\tFr_p: 0.107779\tFr_r: 0.117603\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.809642\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.891658                \tClf: 2.889542\tReg: -0.088366\tFr_p: 0.107540\tFr_r: 0.116827\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.816921\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.811594                \tClf: 2.809128\tReg: -0.088380\tFr_p: 0.107518\tFr_r: 0.116611\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.835556\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.015758                \tClf: 3.012340\tReg: -0.088360\tFr_p: 0.107666\tFr_r: 0.116892\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.824189\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.851227                \tClf: 2.848393\tReg: -0.088375\tFr_p: 0.107939\tFr_r: 0.117666\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.816801\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.091044                \tClf: 3.088564\tReg: -0.088360\tFr_p: 0.107810\tFr_r: 0.117343\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.837920\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.065109                \tClf: 3.061586\tReg: -0.088373\tFr_p: 0.108514\tFr_r: 0.119286\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.816690\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.709663                \tClf: 2.707189\tReg: -0.088361\tFr_p: 0.108355\tFr_r: 0.118465\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.812701\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.148938                \tClf: 2.146658\tReg: -0.088355\tFr_p: 0.107574\tFr_r: 0.115860\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.821685\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.899407                \tClf: 2.921562\tReg: -0.113240\tFr_p: 0.374819\tFr_r: 0.401644\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.825139\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.786801                \tClf: 2.808466\tReg: -0.112922\tFr_p: 0.108138\tFr_r: 0.118130\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.841515\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.728510                \tClf: 2.749379\tReg: -0.112945\tFr_p: 0.108740\tFr_r: 0.119289\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.846748\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.142436                \tClf: 3.163031\tReg: -0.112932\tFr_p: 0.108445\tFr_r: 0.118685\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.824399\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.746687                \tClf: 2.768396\tReg: -0.112929\tFr_p: 0.108259\tFr_r: 0.117894\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.845865\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.778896                \tClf: 2.799529\tReg: -0.112927\tFr_p: 0.108397\tFr_r: 0.118288\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.828989\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.795729                \tClf: 2.817209\tReg: -0.112929\tFr_p: 0.108416\tFr_r: 0.118238\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.818227\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.934980                \tClf: 2.956972\tReg: -0.112903\tFr_p: 0.108233\tFr_r: 0.117583\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.825236\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.718619                \tClf: 2.740287\tReg: -0.112930\tFr_p: 0.108150\tFr_r: 0.117166\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.841465\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.090778                \tClf: 3.111612\tReg: -0.112907\tFr_p: 0.108193\tFr_r: 0.117183\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.831182\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.798832                \tClf: 2.820192\tReg: -0.112919\tFr_p: 0.108450\tFr_r: 0.117966\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.824681\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.067408                \tClf: 3.089080\tReg: -0.112907\tFr_p: 0.108396\tFr_r: 0.117721\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.845549\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.001650                \tClf: 3.022290\tReg: -0.112918\tFr_p: 0.109107\tFr_r: 0.119656\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.821887\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.665470                \tClf: 2.687267\tReg: -0.112891\tFr_p: 0.108813\tFr_r: 0.118503\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.818653\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.076351                \tClf: 2.098308\tReg: -0.112890\tFr_p: 0.108139\tFr_r: 0.116224\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.827972\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.942750                \tClf: 2.927862\tReg: -0.076511\tFr_p: 0.376935\tFr_r: 0.403115\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.829885\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.858188                \tClf: 2.842485\tReg: -0.075791\tFr_p: 0.108551\tFr_r: 0.117920\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.848712\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.878065                \tClf: 2.861424\tReg: -0.075794\tFr_p: 0.109267\tFr_r: 0.119558\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.852781\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.099342                \tClf: 3.082483\tReg: -0.075779\tFr_p: 0.108961\tFr_r: 0.118966\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.830874\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.807282                \tClf: 2.791545\tReg: -0.075807\tFr_p: 0.108789\tFr_r: 0.118403\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.851177\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.870173                \tClf: 2.853428\tReg: -0.075814\tFr_p: 0.108913\tFr_r: 0.118701\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.836092\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.907751                \tClf: 2.891756\tReg: -0.075809\tFr_p: 0.108965\tFr_r: 0.118791\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.822439\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.945885                \tClf: 2.930552\tReg: -0.075789\tFr_p: 0.108613\tFr_r: 0.117698\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.829840\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.738832                \tClf: 2.723148\tReg: -0.075809\tFr_p: 0.108620\tFr_r: 0.117660\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.843754\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.062961                \tClf: 3.046552\tReg: -0.075779\tFr_p: 0.108478\tFr_r: 0.117157\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.837080\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.829715                \tClf: 2.813649\tReg: -0.075788\tFr_p: 0.108934\tFr_r: 0.118556\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.828899\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.163763                \tClf: 3.148109\tReg: -0.075791\tFr_p: 0.108698\tFr_r: 0.118089\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.846297\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.067081                \tClf: 3.050514\tReg: -0.075747\tFr_p: 0.109267\tFr_r: 0.119594\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.826409\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.717574                \tClf: 2.702043\tReg: -0.075790\tFr_p: 0.109203\tFr_r: 0.119094\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.824078\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.122939                \tClf: 2.107555\tReg: -0.075819\tFr_p: 0.108613\tFr_r: 0.117109\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.831211\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.908941                \tClf: 2.893789\tReg: -0.076409\tFr_p: 0.378829\tFr_r: 0.406672\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.833479\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.900094                \tClf: 2.884199\tReg: -0.075778\tFr_p: 0.108852\tFr_r: 0.118311\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.852686\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.874911                \tClf: 2.858076\tReg: -0.075799\tFr_p: 0.109615\tFr_r: 0.120001\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.855920\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.136388                \tClf: 3.119359\tReg: -0.075767\tFr_p: 0.109268\tFr_r: 0.119265\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.834237\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.745475                \tClf: 2.729548\tReg: -0.075785\tFr_p: 0.109096\tFr_r: 0.118705\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.855357\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.872840                \tClf: 2.855863\tReg: -0.075791\tFr_p: 0.109195\tFr_r: 0.119034\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.839207\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.894087                \tClf: 2.877915\tReg: -0.075788\tFr_p: 0.109193\tFr_r: 0.119001\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.827212\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.962924                \tClf: 2.947341\tReg: -0.075777\tFr_p: 0.108889\tFr_r: 0.117997\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.834660\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.725327                \tClf: 2.709381\tReg: -0.075786\tFr_p: 0.108953\tFr_r: 0.118103\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.849281\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.042519                \tClf: 3.025816\tReg: -0.075761\tFr_p: 0.108807\tFr_r: 0.117592\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.841217\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.769081                \tClf: 2.752788\tReg: -0.075768\tFr_p: 0.109273\tFr_r: 0.118984\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.831717\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.051952                \tClf: 3.036115\tReg: -0.075748\tFr_p: 0.108972\tFr_r: 0.118372\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.851386\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.019142                \tClf: 3.002315\tReg: -0.075742\tFr_p: 0.109589\tFr_r: 0.119961\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.830118\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.767508                \tClf: 2.751750\tReg: -0.075749\tFr_p: 0.109449\tFr_r: 0.119299\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.827582\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.128412                \tClf: 2.112815\tReg: -0.075782\tFr_p: 0.108828\tFr_r: 0.117207\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.834003\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.981216                \tClf: 2.913750\tReg: -0.024235\tFr_p: 0.379573\tFr_r: 0.407118\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.834552\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.968159                \tClf: 2.899771\tReg: -0.023339\tFr_p: 0.108886\tFr_r: 0.117856\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.851150\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.899072                \tClf: 2.829890\tReg: -0.023376\tFr_p: 0.109513\tFr_r: 0.119265\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.858523\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.128615                \tClf: 3.059069\tReg: -0.023381\tFr_p: 0.109458\tFr_r: 0.119353\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.838118\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.768373                \tClf: 2.699891\tReg: -0.023424\tFr_p: 0.109358\tFr_r: 0.118996\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.859704\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.960883                \tClf: 2.891297\tReg: -0.023400\tFr_p: 0.109458\tFr_r: 0.119303\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.842819\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.007894                \tClf: 2.939166\tReg: -0.023413\tFr_p: 0.109466\tFr_r: 0.119238\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.828795\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.985436                \tClf: 2.917325\tReg: -0.023329\tFr_p: 0.109060\tFr_r: 0.118051\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.836891\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.785871                \tClf: 2.717451\tReg: -0.023424\tFr_p: 0.109152\tFr_r: 0.118306\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.850892\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.124502                \tClf: 3.055263\tReg: -0.023306\tFr_p: 0.109019\tFr_r: 0.117813\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.844365\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.920478                \tClf: 2.851615\tReg: -0.023356\tFr_p: 0.109422\tFr_r: 0.119097\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.835073\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.126991                \tClf: 3.058532\tReg: -0.023294\tFr_p: 0.109193\tFr_r: 0.118690\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.852446\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.152864                \tClf: 3.083519\tReg: -0.023278\tFr_p: 0.109599\tFr_r: 0.119854\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.831989\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.809279                \tClf: 2.741068\tReg: -0.023389\tFr_p: 0.109586\tFr_r: 0.119479\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.832750\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.208539                \tClf: 2.140446\tReg: -0.023545\tFr_p: 0.109219\tFr_r: 0.118210\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.838406\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.933903                \tClf: 2.865567\tReg: -0.023584\tFr_p: 0.381144\tFr_r: 0.410924\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.836674\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.901527                \tClf: 2.833033\tReg: -0.023339\tFr_p: 0.109115\tFr_r: 0.118298\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.853396\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.882929                \tClf: 2.813632\tReg: -0.023372\tFr_p: 0.109674\tFr_r: 0.119466\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.861083\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.164821                \tClf: 3.095094\tReg: -0.023327\tFr_p: 0.109594\tFr_r: 0.119518\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.840588\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.799312                \tClf: 2.730661\tReg: -0.023379\tFr_p: 0.109508\tFr_r: 0.119188\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.861279\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.878304                \tClf: 2.808644\tReg: -0.023405\tFr_p: 0.109565\tFr_r: 0.119395\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.845507\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.949394                \tClf: 2.880531\tReg: -0.023413\tFr_p: 0.109694\tFr_r: 0.119659\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.832860\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.981486                \tClf: 2.913170\tReg: -0.023327\tFr_p: 0.109288\tFr_r: 0.118469\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.837468\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.684637                \tClf: 2.616180\tReg: -0.023416\tFr_p: 0.109256\tFr_r: 0.118324\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.852155\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.081769                \tClf: 3.012429\tReg: -0.023268\tFr_p: 0.109115\tFr_r: 0.117751\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.844100\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.915262                \tClf: 2.846367\tReg: -0.023310\tFr_p: 0.109482\tFr_r: 0.118964\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.837177\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.139484                \tClf: 3.070918\tReg: -0.023293\tFr_p: 0.109325\tFr_r: 0.118771\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.854233\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.043573                \tClf: 2.974133\tReg: -0.023271\tFr_p: 0.109764\tFr_r: 0.119946\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.833703\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.803403                \tClf: 2.735093\tReg: -0.023375\tFr_p: 0.109731\tFr_r: 0.119617\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.833941\tlr: 0.001000\ttrain acc:98.4750\tLoss: 2.187541                \tClf: 2.119427\tReg: -0.023583\tFr_p: 0.109318\tFr_r: 0.118155\n",
      "\n",
      "Test set: Average loss: 0.0926, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.838923\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.986200                \tClf: 2.894508\tReg: -0.000254\tFr_p: 0.381354\tFr_r: 0.410480\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.837645\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.989853                \tClf: 2.897899\tReg: 0.000072\tFr_p: 0.109111\tFr_r: 0.118008\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.853225\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.911629                \tClf: 2.818900\tReg: 0.000068\tFr_p: 0.109585\tFr_r: 0.118945\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.860953\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.173074                \tClf: 3.079958\tReg: 0.000068\tFr_p: 0.109608\tFr_r: 0.119262\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.840709\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.831964                \tClf: 2.739862\tReg: 0.000067\tFr_p: 0.109536\tFr_r: 0.119016\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.861878\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.935867                \tClf: 2.842707\tReg: 0.000066\tFr_p: 0.109718\tFr_r: 0.119431\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.846316\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.905863                \tClf: 2.813482\tReg: 0.000066\tFr_p: 0.109786\tFr_r: 0.119671\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.833269\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.017775                \tClf: 2.926043\tReg: 0.000069\tFr_p: 0.109430\tFr_r: 0.118597\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.840143\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.878033                \tClf: 2.785960\tReg: 0.000065\tFr_p: 0.109490\tFr_r: 0.118748\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.854804\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.148243                \tClf: 3.055434\tReg: 0.000069\tFr_p: 0.109298\tFr_r: 0.118141\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.846045\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.921191                \tClf: 2.828823\tReg: 0.000066\tFr_p: 0.109585\tFr_r: 0.119017\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.837991\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.194016                \tClf: 3.102049\tReg: 0.000067\tFr_p: 0.109370\tFr_r: 0.118767\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.852282\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.115583                \tClf: 3.022902\tReg: 0.000068\tFr_p: 0.109642\tFr_r: 0.119482\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.833103\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.843564                \tClf: 2.751841\tReg: 0.000068\tFr_p: 0.109653\tFr_r: 0.119278\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.835756\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.253049                \tClf: 2.161195\tReg: 0.000066\tFr_p: 0.109469\tFr_r: 0.118480\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.841447\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.020240                \tClf: 2.928102\tReg: 0.000066\tFr_p: 0.382255\tFr_r: 0.412627\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.839332\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.012219                \tClf: 2.920182\tReg: 0.000071\tFr_p: 0.109292\tFr_r: 0.118338\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.853522\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.950725                \tClf: 2.857981\tReg: 0.000068\tFr_p: 0.109677\tFr_r: 0.119098\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863257\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.133404                \tClf: 3.040172\tReg: 0.000069\tFr_p: 0.109741\tFr_r: 0.119513\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842491\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.797659                \tClf: 2.705467\tReg: 0.000068\tFr_p: 0.109680\tFr_r: 0.119275\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.863902\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.989512                \tClf: 2.896251\tReg: 0.000066\tFr_p: 0.109799\tFr_r: 0.119630\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.848119\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.901261                \tClf: 2.808789\tReg: 0.000066\tFr_p: 0.109894\tFr_r: 0.119903\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.834985\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.967606                \tClf: 2.875788\tReg: 0.000069\tFr_p: 0.109500\tFr_r: 0.118691\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.841640\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.766441                \tClf: 2.674294\tReg: 0.000066\tFr_p: 0.109549\tFr_r: 0.118786\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.856000\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.158991                \tClf: 3.066123\tReg: 0.000069\tFr_p: 0.109367\tFr_r: 0.118193\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.846477\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.001366                \tClf: 2.908975\tReg: 0.000067\tFr_p: 0.109606\tFr_r: 0.118979\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.839732\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.172796                \tClf: 3.080741\tReg: 0.000068\tFr_p: 0.109461\tFr_r: 0.118901\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.854968\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.093269                \tClf: 3.000452\tReg: 0.000068\tFr_p: 0.109774\tFr_r: 0.119757\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.834481\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.823995                \tClf: 2.732203\tReg: 0.000068\tFr_p: 0.109738\tFr_r: 0.119421\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.836698\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.232227                \tClf: 2.140327\tReg: 0.000066\tFr_p: 0.109566\tFr_r: 0.118619\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.842478\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.002001                \tClf: 2.909814\tReg: 0.000064\tFr_p: 0.382589\tFr_r: 0.413147\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.840884\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.977408                \tClf: 2.885298\tReg: 0.000067\tFr_p: 0.109353\tFr_r: 0.118379\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854077\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.908418                \tClf: 2.815650\tReg: 0.000065\tFr_p: 0.109626\tFr_r: 0.118787\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862016\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.201276                \tClf: 3.108111\tReg: 0.000065\tFr_p: 0.109673\tFr_r: 0.119149\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.841979\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.774273                \tClf: 2.682110\tReg: 0.000064\tFr_p: 0.109647\tFr_r: 0.119033\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.863877\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.915889                \tClf: 2.822632\tReg: 0.000064\tFr_p: 0.109792\tFr_r: 0.119404\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.848928\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.945647                \tClf: 2.853137\tReg: 0.000063\tFr_p: 0.109959\tFr_r: 0.119855\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.835614\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.037609                \tClf: 2.945764\tReg: 0.000065\tFr_p: 0.109578\tFr_r: 0.118798\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.843024\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.812494                \tClf: 2.720280\tReg: 0.000063\tFr_p: 0.109653\tFr_r: 0.118984\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.857211\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.102079                \tClf: 3.009154\tReg: 0.000065\tFr_p: 0.109512\tFr_r: 0.118484\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.847980\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.971774                \tClf: 2.879311\tReg: 0.000064\tFr_p: 0.109701\tFr_r: 0.119106\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.839470\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.201342                \tClf: 3.109304\tReg: 0.000064\tFr_p: 0.109503\tFr_r: 0.118963\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.854310\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.125137                \tClf: 3.032357\tReg: 0.000064\tFr_p: 0.109735\tFr_r: 0.119502\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834781\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.861818                \tClf: 2.770014\tReg: 0.000064\tFr_p: 0.109726\tFr_r: 0.119277\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.837960\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.290379                \tClf: 2.198417\tReg: 0.000063\tFr_p: 0.109632\tFr_r: 0.118765\n",
      "\n",
      "Test set: Average loss: 0.0930, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.843572\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.978438                \tClf: 2.886197\tReg: 0.000063\tFr_p: 0.382948\tFr_r: 0.413890\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.841390\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.933875                \tClf: 2.841740\tReg: 0.000066\tFr_p: 0.109403\tFr_r: 0.118497\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854900\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.904558                \tClf: 2.811748\tReg: 0.000065\tFr_p: 0.109678\tFr_r: 0.118837\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862543\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.132265                \tClf: 3.039073\tReg: 0.000065\tFr_p: 0.109701\tFr_r: 0.119171\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842251\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.815611                \tClf: 2.723434\tReg: 0.000064\tFr_p: 0.109642\tFr_r: 0.119010\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864901\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.956498                \tClf: 2.863189\tReg: 0.000064\tFr_p: 0.109834\tFr_r: 0.119514\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849220\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.920657                \tClf: 2.828133\tReg: 0.000063\tFr_p: 0.109958\tFr_r: 0.119873\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.836468\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.978473                \tClf: 2.886585\tReg: 0.000065\tFr_p: 0.109639\tFr_r: 0.118900\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.843856\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.862794                \tClf: 2.770538\tReg: 0.000063\tFr_p: 0.109700\tFr_r: 0.119044\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.858302\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.075534                \tClf: 2.982554\tReg: 0.000065\tFr_p: 0.109540\tFr_r: 0.118499\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.847972\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.942284                \tClf: 2.849822\tReg: 0.000064\tFr_p: 0.109750\tFr_r: 0.119156\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.840916\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.172216                \tClf: 3.080106\tReg: 0.000064\tFr_p: 0.109585\tFr_r: 0.119087\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.854483\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.123159                \tClf: 3.030370\tReg: 0.000064\tFr_p: 0.109770\tFr_r: 0.119549\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.834417\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.899867                \tClf: 2.808082\tReg: 0.000064\tFr_p: 0.109750\tFr_r: 0.119294\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.838366\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.215695                \tClf: 2.123713\tReg: 0.000064\tFr_p: 0.109677\tFr_r: 0.118763\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.843842\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.989206                \tClf: 2.896951\tReg: 0.000062\tFr_p: 0.383009\tFr_r: 0.413871\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.840859\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.030205                \tClf: 2.938098\tReg: 0.000064\tFr_p: 0.109471\tFr_r: 0.118564\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.853369\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.936801                \tClf: 2.844070\tReg: 0.000063\tFr_p: 0.109650\tFr_r: 0.118701\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862378\tlr: 0.001000\ttrain acc:96.7750\tLoss: 3.231770                \tClf: 3.138588\tReg: 0.000063\tFr_p: 0.109674\tFr_r: 0.119062\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843097\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.854484                \tClf: 2.762266\tReg: 0.000063\tFr_p: 0.109679\tFr_r: 0.119008\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864625\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.885117                \tClf: 2.791823\tReg: 0.000063\tFr_p: 0.109853\tFr_r: 0.119459\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.849286\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.959765                \tClf: 2.867238\tReg: 0.000062\tFr_p: 0.109984\tFr_r: 0.119856\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837619\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.015179                \tClf: 2.923235\tReg: 0.000063\tFr_p: 0.109686\tFr_r: 0.118971\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844215\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.747165                \tClf: 2.654892\tReg: 0.000062\tFr_p: 0.109760\tFr_r: 0.119141\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859462\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.087671                \tClf: 2.994635\tReg: 0.000063\tFr_p: 0.109634\tFr_r: 0.118673\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.848811\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.905963                \tClf: 2.813460\tReg: 0.000063\tFr_p: 0.109795\tFr_r: 0.119247\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.841117\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.217497                \tClf: 3.125379\tReg: 0.000063\tFr_p: 0.109624\tFr_r: 0.119139\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855123\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.160471                \tClf: 3.067652\tReg: 0.000063\tFr_p: 0.109771\tFr_r: 0.119527\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835021\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.824189                \tClf: 2.732375\tReg: 0.000063\tFr_p: 0.109757\tFr_r: 0.119266\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839140\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.169410                \tClf: 2.077391\tReg: 0.000062\tFr_p: 0.109712\tFr_r: 0.118891\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.844990\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.998849                \tClf: 2.906537\tReg: 0.000062\tFr_p: 0.383261\tFr_r: 0.414531\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842054\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.897600                \tClf: 2.805434\tReg: 0.000064\tFr_p: 0.109530\tFr_r: 0.118697\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854904\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.946859                \tClf: 2.854051\tReg: 0.000063\tFr_p: 0.109704\tFr_r: 0.118802\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863003\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.131805                \tClf: 3.038592\tReg: 0.000063\tFr_p: 0.109722\tFr_r: 0.119117\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842393\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.887092                \tClf: 2.794909\tReg: 0.000063\tFr_p: 0.109698\tFr_r: 0.119075\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865129\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.991286                \tClf: 2.897968\tReg: 0.000062\tFr_p: 0.109862\tFr_r: 0.119490\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850480\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.974530                \tClf: 2.881944\tReg: 0.000062\tFr_p: 0.110035\tFr_r: 0.119968\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837301\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.007547                \tClf: 2.915619\tReg: 0.000063\tFr_p: 0.109729\tFr_r: 0.119002\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844624\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.839430                \tClf: 2.747137\tReg: 0.000062\tFr_p: 0.109785\tFr_r: 0.119185\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859212\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.065119                \tClf: 2.972096\tReg: 0.000063\tFr_p: 0.109644\tFr_r: 0.118697\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849481\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.901791                \tClf: 2.809254\tReg: 0.000062\tFr_p: 0.109812\tFr_r: 0.119270\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.841773\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.125449                \tClf: 3.033298\tReg: 0.000063\tFr_p: 0.109666\tFr_r: 0.119183\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.854859\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.174579                \tClf: 3.081773\tReg: 0.000063\tFr_p: 0.109766\tFr_r: 0.119502\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835139\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.865992                \tClf: 2.774172\tReg: 0.000063\tFr_p: 0.109759\tFr_r: 0.119262\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.838823\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.220834                \tClf: 2.128831\tReg: 0.000062\tFr_p: 0.109716\tFr_r: 0.118839\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845495\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.003962                \tClf: 2.911625\tReg: 0.000062\tFr_p: 0.383231\tFr_r: 0.414365\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842134\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.935478                \tClf: 2.843309\tReg: 0.000062\tFr_p: 0.109499\tFr_r: 0.118650\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.853950\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.898612                \tClf: 2.805853\tReg: 0.000062\tFr_p: 0.109675\tFr_r: 0.118695\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863185\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.212469                \tClf: 3.119248\tReg: 0.000062\tFr_p: 0.109697\tFr_r: 0.118998\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842922\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.866572                \tClf: 2.774364\tReg: 0.000062\tFr_p: 0.109668\tFr_r: 0.118915\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864555\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.933238                \tClf: 2.839949\tReg: 0.000062\tFr_p: 0.109829\tFr_r: 0.119376\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.849695\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.033595                \tClf: 2.941049\tReg: 0.000062\tFr_p: 0.110003\tFr_r: 0.119793\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837795\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.014340                \tClf: 2.922389\tReg: 0.000062\tFr_p: 0.109717\tFr_r: 0.118965\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844360\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.789448                \tClf: 2.697169\tReg: 0.000062\tFr_p: 0.109792\tFr_r: 0.119145\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859496\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.084429                \tClf: 2.991392\tReg: 0.000062\tFr_p: 0.109660\tFr_r: 0.118656\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849549\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.898184                \tClf: 2.805645\tReg: 0.000062\tFr_p: 0.109810\tFr_r: 0.119252\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.841860\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.126596                \tClf: 3.034441\tReg: 0.000062\tFr_p: 0.109669\tFr_r: 0.119184\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855402\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.136768                \tClf: 3.043936\tReg: 0.000062\tFr_p: 0.109784\tFr_r: 0.119461\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834479\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.781938                \tClf: 2.690153\tReg: 0.000062\tFr_p: 0.109738\tFr_r: 0.119144\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.838692\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.234668                \tClf: 2.142672\tReg: 0.000062\tFr_p: 0.109701\tFr_r: 0.118827\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845957\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.903687                \tClf: 2.811327\tReg: 0.000061\tFr_p: 0.383369\tFr_r: 0.414596\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842702\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.927544                \tClf: 2.835347\tReg: 0.000062\tFr_p: 0.109546\tFr_r: 0.118687\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854291\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.954018                \tClf: 2.861242\tReg: 0.000062\tFr_p: 0.109704\tFr_r: 0.118722\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862611\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.210910                \tClf: 3.117718\tReg: 0.000062\tFr_p: 0.109699\tFr_r: 0.119027\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842652\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.805243                \tClf: 2.713048\tReg: 0.000062\tFr_p: 0.109671\tFr_r: 0.118896\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865821\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.961486                \tClf: 2.868133\tReg: 0.000062\tFr_p: 0.109851\tFr_r: 0.119409\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850242\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.933763                \tClf: 2.841190\tReg: 0.000062\tFr_p: 0.110023\tFr_r: 0.119848\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838068\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.064308                \tClf: 2.972343\tReg: 0.000062\tFr_p: 0.109720\tFr_r: 0.118951\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.844374\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.818440                \tClf: 2.726160\tReg: 0.000062\tFr_p: 0.109807\tFr_r: 0.119178\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859274\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.110770                \tClf: 3.017744\tReg: 0.000062\tFr_p: 0.109696\tFr_r: 0.118754\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850284\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.877195                \tClf: 2.784619\tReg: 0.000062\tFr_p: 0.109845\tFr_r: 0.119299\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842498\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.228893                \tClf: 3.136706\tReg: 0.000062\tFr_p: 0.109683\tFr_r: 0.119209\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856174\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.164653                \tClf: 3.071782\tReg: 0.000062\tFr_p: 0.109805\tFr_r: 0.119531\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835217\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.816899                \tClf: 2.725077\tReg: 0.000062\tFr_p: 0.109771\tFr_r: 0.119197\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839119\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.198607                \tClf: 2.106589\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845556\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.933959                \tClf: 2.841620\tReg: 0.000061\tFr_p: 0.383405\tFr_r: 0.414692\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842504\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.963862                \tClf: 2.871675\tReg: 0.000062\tFr_p: 0.109544\tFr_r: 0.118701\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854965\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.877664                \tClf: 2.784855\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118678\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862616\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.243244                \tClf: 3.150052\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118987\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842768\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.882219                \tClf: 2.790019\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118892\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864402\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.891884                \tClf: 2.798602\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119385\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.849831\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.959784                \tClf: 2.867232\tReg: 0.000061\tFr_p: 0.110008\tFr_r: 0.119830\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837547\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.963738                \tClf: 2.871800\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118975\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844820\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.841982                \tClf: 2.749680\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119189\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859141\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.093075                \tClf: 3.000057\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118767\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.848703\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.934226                \tClf: 2.841730\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119296\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842106\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.148008                \tClf: 3.055841\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.119206\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855200\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.106252                \tClf: 3.013430\tReg: 0.000061\tFr_p: 0.109795\tFr_r: 0.119488\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835149\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.860671                \tClf: 2.768852\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119171\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839401\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.219504                \tClf: 2.127472\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845578\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.956031                \tClf: 2.863691\tReg: 0.000061\tFr_p: 0.383409\tFr_r: 0.414696\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843097\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.957832                \tClf: 2.865616\tReg: 0.000062\tFr_p: 0.109550\tFr_r: 0.118689\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854864\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.984785                \tClf: 2.891980\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118701\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.861824\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.143943                \tClf: 3.050790\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118993\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843297\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.866164                \tClf: 2.773938\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118928\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865564\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.952462                \tClf: 2.859122\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119405\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849635\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.942270                \tClf: 2.849727\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119817\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837885\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.051837                \tClf: 2.959881\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118964\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845673\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.823354                \tClf: 2.731009\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119205\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859545\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.052835                \tClf: 2.959797\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118772\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849782\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.921463                \tClf: 2.828913\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119340\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842609\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.156175                \tClf: 3.063983\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.119240\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.856100\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.109808                \tClf: 3.016941\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119538\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835103\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.840510                \tClf: 2.748693\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119200\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839609\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.197376                \tClf: 2.105334\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845474\tlr: 0.001000\ttrain acc:97.8000\tLoss: 3.012178                \tClf: 2.919843\tReg: 0.000061\tFr_p: 0.383496\tFr_r: 0.414828\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842937\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.991972                \tClf: 2.899764\tReg: 0.000061\tFr_p: 0.109550\tFr_r: 0.118728\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854265\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.865955                \tClf: 2.773180\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118690\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862874\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.143633                \tClf: 3.050428\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119021\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842917\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.877932                \tClf: 2.785725\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118876\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864508\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.973860                \tClf: 2.880573\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119332\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.849802\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.932060                \tClf: 2.839508\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119802\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.838055\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.073013                \tClf: 2.981049\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118955\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845223\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.836360                \tClf: 2.744037\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119191\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859680\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.107587                \tClf: 3.014541\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118794\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849884\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.876569                \tClf: 2.784014\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119332\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842356\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.197682                \tClf: 3.105503\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119279\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855441\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.177220                \tClf: 3.084387\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119488\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834646\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.857736                \tClf: 2.765943\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119166\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839765\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.258856                \tClf: 2.166807\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118884\n",
      "\n",
      "Test set: Average loss: 0.0923, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.845767\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.954932                \tClf: 2.862583\tReg: 0.000061\tFr_p: 0.383469\tFr_r: 0.414821\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842468\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.997178                \tClf: 2.904993\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118755\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854616\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.994530                \tClf: 2.901738\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118691\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862487\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.186184                \tClf: 3.092998\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118964\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842744\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.885354                \tClf: 2.793156\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118865\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864805\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.942063                \tClf: 2.848761\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119353\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849791\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.944912                \tClf: 2.852361\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119804\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837819\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.044309                \tClf: 2.952357\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118975\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844518\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.839665                \tClf: 2.747378\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119181\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859849\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.159346                \tClf: 3.066293\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118833\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849689\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.978770                \tClf: 2.886224\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119317\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.843213\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.166039                \tClf: 3.073817\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119249\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855786\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.110499                \tClf: 3.017648\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119530\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.833753\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.872770                \tClf: 2.781021\tReg: 0.000061\tFr_p: 0.109782\tFr_r: 0.119191\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839250\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.248715                \tClf: 2.156691\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.845742\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.982906                \tClf: 2.890558\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414918\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.842839\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.940028                \tClf: 2.847825\tReg: 0.000061\tFr_p: 0.109563\tFr_r: 0.118735\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854004\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.907124                \tClf: 2.814363\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118704\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862767\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.277788                \tClf: 3.184588\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118987\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843488\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.842552                \tClf: 2.750316\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118924\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865550\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.967146                \tClf: 2.873807\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119380\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850133\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.980916                \tClf: 2.888349\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119811\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837240\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.043834                \tClf: 2.951911\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118971\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845466\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.743276                \tClf: 2.650941\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119209\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860186\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.148406                \tClf: 3.055336\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118835\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849922\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.958462                \tClf: 2.865905\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119331\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842592\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.145234                \tClf: 3.053043\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119284\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.856108\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.103609                \tClf: 3.010743\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119508\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835922\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.824733                \tClf: 2.732876\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119194\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839562\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.238884                \tClf: 2.146845\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845660\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.975510                \tClf: 2.883166\tReg: 0.000061\tFr_p: 0.383514\tFr_r: 0.414911\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843320\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.942425                \tClf: 2.850198\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118766\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855308\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.995309                \tClf: 2.902482\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118703\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862954\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.218638                \tClf: 3.125429\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118987\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842874\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.815896                \tClf: 2.723691\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118929\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865399\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.888645                \tClf: 2.795314\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119391\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850146\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.016140                \tClf: 2.923572\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119831\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837150\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.026523                \tClf: 2.934605\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118989\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845143\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.806132                \tClf: 2.713814\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119178\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860316\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.116451                \tClf: 3.023374\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118829\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850872\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.918180                \tClf: 2.825576\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119384\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.843040\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.116426                \tClf: 3.024213\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.119282\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855875\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.104432                \tClf: 3.011577\tReg: 0.000061\tFr_p: 0.109793\tFr_r: 0.119501\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835196\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.873158                \tClf: 2.781337\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119139\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839046\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.216205                \tClf: 2.124192\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118894\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.845559\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.959834                \tClf: 2.867495\tReg: 0.000061\tFr_p: 0.383528\tFr_r: 0.414922\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843449\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.000796                \tClf: 2.908562\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118784\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854698\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.937015                \tClf: 2.844219\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118694\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863076\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.172649                \tClf: 3.079434\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119001\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843161\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.793468                \tClf: 2.701249\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118893\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864347\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.951222                \tClf: 2.857943\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119360\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849935\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.963165                \tClf: 2.870607\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119830\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838064\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.045515                \tClf: 2.953551\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118984\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844908\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.749387                \tClf: 2.657080\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119174\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860036\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.059169                \tClf: 2.966106\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118810\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850004\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.920339                \tClf: 2.827777\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119344\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842583\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.166627                \tClf: 3.074437\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.119284\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855746\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.103948                \tClf: 3.011099\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119522\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835140\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.777748                \tClf: 2.685930\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119156\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839358\tlr: 0.001000\ttrain acc:98.5250\tLoss: 2.134473                \tClf: 2.042444\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118882\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846434\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.988464                \tClf: 2.896081\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414945\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843029\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.989002                \tClf: 2.896790\tReg: 0.000061\tFr_p: 0.109601\tFr_r: 0.118784\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854388\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.907661                \tClf: 2.814881\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118702\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863035\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.230404                \tClf: 3.137191\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118994\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843383\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.788535                \tClf: 2.696305\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118892\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865335\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.988782                \tClf: 2.895454\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119337\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849989\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.900291                \tClf: 2.807731\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119840\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837495\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.044789                \tClf: 2.952853\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118941\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845903\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.857646                \tClf: 2.765290\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119200\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860204\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.130707                \tClf: 3.037636\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118822\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850179\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.896016                \tClf: 2.803446\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119360\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843011\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.220764                \tClf: 3.128552\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.119269\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855595\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.105520                \tClf: 3.012679\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.119497\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834973\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.875576                \tClf: 2.783767\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.119167\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839974\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.194874                \tClf: 2.102815\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846034\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.979387                \tClf: 2.887024\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414934\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843151\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.954848                \tClf: 2.862629\tReg: 0.000061\tFr_p: 0.109568\tFr_r: 0.118764\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854194\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.943483                \tClf: 2.850712\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118691\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863657\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.196763                \tClf: 3.103519\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119016\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842597\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.833041                \tClf: 2.740850\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118881\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864706\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.958922                \tClf: 2.865626\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119370\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850648\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.931651                \tClf: 2.839058\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119824\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837618\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.072329                \tClf: 2.980387\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118974\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844775\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.841689                \tClf: 2.749389\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119200\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860123\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.076929                \tClf: 2.983862\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118831\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850176\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.916439                \tClf: 2.823869\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119341\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842763\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.211261                \tClf: 3.119062\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119280\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855368\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.156156                \tClf: 3.063327\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119511\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.836325\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.916221                \tClf: 2.824344\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119188\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840031\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.208274                \tClf: 2.116212\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.845999\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.950300                \tClf: 2.857939\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414945\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842628\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.962640                \tClf: 2.870448\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118765\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854993\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.892387                \tClf: 2.799576\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118723\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862944\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.125862                \tClf: 3.032654\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118996\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842764\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.850811                \tClf: 2.758612\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118875\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864479\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.938109                \tClf: 2.844824\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119340\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849705\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.976608                \tClf: 2.884061\tReg: 0.000061\tFr_p: 0.110009\tFr_r: 0.119790\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837475\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.093203                \tClf: 3.001268\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118961\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845382\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.819709                \tClf: 2.727379\tReg: 0.000061\tFr_p: 0.109841\tFr_r: 0.119200\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859499\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.086285                \tClf: 2.993249\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118810\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850653\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.913993                \tClf: 2.821399\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119373\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.843027\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.221459                \tClf: 3.129247\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119266\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855956\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.047378                \tClf: 2.954519\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119533\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834733\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.771948                \tClf: 2.680150\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.119135\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839714\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.200293                \tClf: 2.108246\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118926\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846306\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.961010                \tClf: 2.868634\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414946\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842806\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.926774                \tClf: 2.834572\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118757\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855037\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.968560                \tClf: 2.875747\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118682\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862516\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.262390                \tClf: 3.169203\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119003\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842755\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.748808                \tClf: 2.656609\tReg: 0.000061\tFr_p: 0.109656\tFr_r: 0.118857\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864976\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.880909                \tClf: 2.787599\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119349\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850786\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.936424                \tClf: 2.843823\tReg: 0.000061\tFr_p: 0.110009\tFr_r: 0.119811\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837655\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.041644                \tClf: 2.949700\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118972\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845149\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.819766                \tClf: 2.727448\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119181\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860051\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.077333                \tClf: 2.984269\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118829\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849655\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.902698                \tClf: 2.810154\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119361\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843776\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.172306                \tClf: 3.080056\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.119248\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855555\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.181974                \tClf: 3.089136\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119511\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.834979\tlr: 0.001000\ttrain acc:96.9750\tLoss: 2.898038                \tClf: 2.806228\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119148\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.840438\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.193120                \tClf: 2.101037\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846098\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.038868                \tClf: 2.946502\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414931\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843134\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.940098                \tClf: 2.847880\tReg: 0.000061\tFr_p: 0.109597\tFr_r: 0.118784\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855323\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.919711                \tClf: 2.826883\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118716\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862720\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.157915                \tClf: 3.064718\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118967\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842286\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.857321                \tClf: 2.765146\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118883\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865126\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.953792                \tClf: 2.860474\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119372\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850814\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.975329                \tClf: 2.882727\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119812\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837899\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.938467                \tClf: 2.846511\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118959\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845256\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.827449                \tClf: 2.735125\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119223\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859656\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.109773                \tClf: 3.016729\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118823\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849975\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.966336                \tClf: 2.873776\tReg: 0.000061\tFr_p: 0.109900\tFr_r: 0.119370\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843203\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.165962                \tClf: 3.073741\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119256\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855166\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.108470                \tClf: 3.015651\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119542\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835568\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.864080                \tClf: 2.772241\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119155\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839458\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.174376                \tClf: 2.082342\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846740\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.956706                \tClf: 2.864308\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414917\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842371\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.943499                \tClf: 2.851319\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118778\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855144\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.899219                \tClf: 2.806400\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118727\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862967\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.217877                \tClf: 3.124667\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118988\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842489\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.810062                \tClf: 2.717876\tReg: 0.000061\tFr_p: 0.109671\tFr_r: 0.118862\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865307\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.955658                \tClf: 2.862332\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119385\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850262\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.950718                \tClf: 2.858144\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119837\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837527\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.988869                \tClf: 2.896931\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118972\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845116\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.835773                \tClf: 2.743456\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119179\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859030\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.105424                \tClf: 3.012411\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118815\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849806\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.902052                \tClf: 2.809500\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119372\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842537\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.217122                \tClf: 3.124934\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119259\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855514\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.122654                \tClf: 3.029817\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119473\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835288\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.906256                \tClf: 2.814430\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119169\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.838837\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.223380                \tClf: 2.131377\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845684\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.962114                \tClf: 2.869769\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414907\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843127\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.954379                \tClf: 2.862162\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118763\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854409\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.946089                \tClf: 2.853308\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118709\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862710\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.172849                \tClf: 3.079652\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118984\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843281\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.863495                \tClf: 2.771269\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118888\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864958\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.966210                \tClf: 2.872901\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119368\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850452\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.937551                \tClf: 2.844967\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119800\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837474\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.010628                \tClf: 2.918693\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118987\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844564\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.771796                \tClf: 2.679506\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119218\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860569\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.108309                \tClf: 3.015219\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118842\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849788\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.913689                \tClf: 2.821138\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119349\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842793\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.201012                \tClf: 3.108812\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.119280\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855736\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.136946                \tClf: 3.044098\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119523\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835205\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.840613                \tClf: 2.748791\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119166\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.840067\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.178174                \tClf: 2.086109\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118905\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845556\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.939186                \tClf: 2.846848\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414953\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843702\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.905037                \tClf: 2.812791\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118775\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855145\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.888350                \tClf: 2.795532\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118712\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862529\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.279201                \tClf: 3.186013\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119004\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842876\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.874928                \tClf: 2.782723\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118895\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864839\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.955330                \tClf: 2.862027\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119356\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850748\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.999165                \tClf: 2.906567\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119810\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837061\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.026695                \tClf: 2.934781\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118963\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845217\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.829753                \tClf: 2.737431\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119198\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859735\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.102852                \tClf: 3.009804\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118815\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849835\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.918470                \tClf: 2.825917\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119342\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.843053\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.153284                \tClf: 3.061070\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119295\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855585\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.135863                \tClf: 3.043023\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119530\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835596\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.870968                \tClf: 2.779127\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119165\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.838720\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.172208                \tClf: 2.080211\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846259\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.995956                \tClf: 2.903582\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414930\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842771\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.952072                \tClf: 2.859872\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118769\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854902\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.002360                \tClf: 2.909554\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118688\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862823\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.205856                \tClf: 3.112654\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118983\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842804\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.874733                \tClf: 2.782532\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118888\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865245\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.011042                \tClf: 2.917719\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119360\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850291\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.940594                \tClf: 2.848019\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119824\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.836946\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.037126                \tClf: 2.945218\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.119002\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845290\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.858161                \tClf: 2.765836\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119197\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859669\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.152442                \tClf: 3.059398\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118807\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850134\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.929259                \tClf: 2.836691\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119346\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842296\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.139348                \tClf: 3.047172\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119280\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855460\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.157596                \tClf: 3.064762\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119494\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835455\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.856613                \tClf: 2.764779\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119161\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839717\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.204661                \tClf: 2.112614\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845571\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.948969                \tClf: 2.856630\tReg: 0.000061\tFr_p: 0.383513\tFr_r: 0.414934\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.842951\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.944599                \tClf: 2.852390\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118787\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854705\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.914113                \tClf: 2.821316\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118680\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863191\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.215478                \tClf: 3.122258\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118997\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842929\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.843750                \tClf: 2.751542\tReg: 0.000061\tFr_p: 0.109665\tFr_r: 0.118878\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865596\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.952776                \tClf: 2.859435\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119351\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849987\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.016513                \tClf: 2.923953\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119801\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837796\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.991480                \tClf: 2.899530\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118974\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844630\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.837022                \tClf: 2.744729\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119179\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860425\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.127213                \tClf: 3.034131\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118826\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849588\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.964001                \tClf: 2.871461\tReg: 0.000061\tFr_p: 0.109902\tFr_r: 0.119384\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842623\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.178066                \tClf: 3.085873\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119298\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855594\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.081845                \tClf: 2.989004\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119514\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835300\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.832218                \tClf: 2.740392\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119129\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.840402\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.175389                \tClf: 2.083308\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118928\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846322\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.022106                \tClf: 2.929729\tReg: 0.000061\tFr_p: 0.383557\tFr_r: 0.414931\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843519\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.035556                \tClf: 2.943319\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118779\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854736\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.957020                \tClf: 2.864222\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118726\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863545\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.201170                \tClf: 3.107932\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119004\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843176\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.884901                \tClf: 2.792681\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118883\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865276\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.020400                \tClf: 2.927076\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119341\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850274\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.995244                \tClf: 2.902670\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119831\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837382\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.013093                \tClf: 2.921163\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118972\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845205\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.835700                \tClf: 2.743379\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119206\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860153\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.138532                \tClf: 3.045463\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118805\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850424\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.918226                \tClf: 2.825643\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119359\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842780\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.152192                \tClf: 3.059992\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119273\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855628\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.172189                \tClf: 3.079347\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119515\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834799\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.831850                \tClf: 2.740049\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.119149\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839673\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.257267                \tClf: 2.165223\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.845704\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.051724                \tClf: 2.959378\tReg: 0.000061\tFr_p: 0.383547\tFr_r: 0.414936\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842966\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.969871                \tClf: 2.877662\tReg: 0.000061\tFr_p: 0.109573\tFr_r: 0.118740\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854906\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.920642                \tClf: 2.827835\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118709\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862911\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.190195                \tClf: 3.096988\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.119004\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843993\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.836232                \tClf: 2.743971\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118902\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865108\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.967871                \tClf: 2.874554\tReg: 0.000061\tFr_p: 0.109852\tFr_r: 0.119354\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850264\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.950534                \tClf: 2.857959\tReg: 0.000061\tFr_p: 0.110012\tFr_r: 0.119794\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837215\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.969048                \tClf: 2.877126\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118954\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844838\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.792742                \tClf: 2.700439\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119209\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859723\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.121909                \tClf: 3.028862\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118782\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849365\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.942363                \tClf: 2.849834\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119350\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842538\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.178667                \tClf: 3.086479\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119260\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.854716\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.086510                \tClf: 2.993713\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119490\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835637\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.859782                \tClf: 2.767939\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.119191\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.838758\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.234979                \tClf: 2.142980\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118888\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846373\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.935543                \tClf: 2.843164\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414959\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843005\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.924506                \tClf: 2.832295\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118793\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854939\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.888140                \tClf: 2.795332\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118714\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863606\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.212373                \tClf: 3.119131\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118994\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843288\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.821368                \tClf: 2.729143\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118901\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.864962\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.977190                \tClf: 2.883881\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119342\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850573\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.968885                \tClf: 2.876295\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119810\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837787\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.048149                \tClf: 2.956199\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118990\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845006\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.828902                \tClf: 2.736591\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119199\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860201\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.072912                \tClf: 2.979841\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118816\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849355\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.900645                \tClf: 2.808116\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119348\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843151\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.166113                \tClf: 3.073894\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.119281\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855446\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.083525                \tClf: 2.990691\tReg: 0.000061\tFr_p: 0.109790\tFr_r: 0.119503\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835369\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.819499                \tClf: 2.727670\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119149\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.838925\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.187723                \tClf: 2.095716\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845331\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.924848                \tClf: 2.832520\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414947\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843223\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.924367                \tClf: 2.832145\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118776\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855030\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.950265                \tClf: 2.857452\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118705\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862981\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.217154                \tClf: 3.123944\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118969\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.844119\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.899610                \tClf: 2.807343\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118877\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864932\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.987387                \tClf: 2.894079\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119348\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849579\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.028327                \tClf: 2.935787\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119798\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837893\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.032938                \tClf: 2.940983\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118965\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844909\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.820222                \tClf: 2.727915\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119171\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.859726\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.124507                \tClf: 3.031459\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118821\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849806\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.934734                \tClf: 2.842183\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119346\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.843395\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.212295                \tClf: 3.120065\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.119302\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855508\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.106922                \tClf: 3.014086\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119512\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834555\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.902394                \tClf: 2.810606\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.119147\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839233\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.162385                \tClf: 2.070362\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.845283\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.951834                \tClf: 2.859509\tReg: 0.000061\tFr_p: 0.383557\tFr_r: 0.414915\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843330\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.951774                \tClf: 2.859547\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118764\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854446\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.938639                \tClf: 2.845856\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118716\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862916\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.206048                \tClf: 3.112841\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118975\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842565\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.908513                \tClf: 2.816323\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118904\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864795\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.947089                \tClf: 2.853788\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119345\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850269\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.970105                \tClf: 2.877531\tReg: 0.000061\tFr_p: 0.110024\tFr_r: 0.119803\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838166\tlr: 0.001000\ttrain acc:97.7250\tLoss: 3.042336                \tClf: 2.950366\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119016\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844997\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.861104                \tClf: 2.768793\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119213\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859662\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.158820                \tClf: 3.065776\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118786\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850001\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.911903                \tClf: 2.819342\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119362\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843679\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.194693                \tClf: 3.102447\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119297\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855738\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.076279                \tClf: 2.983431\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119514\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.834730\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.809954                \tClf: 2.718157\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119159\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839219\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.183295                \tClf: 2.091273\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118882\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845353\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.974700                \tClf: 2.882371\tReg: 0.000061\tFr_p: 0.383547\tFr_r: 0.414952\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.842659\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.972936                \tClf: 2.880742\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118768\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855258\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.958377                \tClf: 2.865553\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118704\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862812\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.223537                \tClf: 3.130335\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118987\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843019\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.817192                \tClf: 2.724980\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118865\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864855\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.957695                \tClf: 2.864391\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119352\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850448\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.964269                \tClf: 2.871686\tReg: 0.000061\tFr_p: 0.110023\tFr_r: 0.119815\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.838054\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.096427                \tClf: 3.004463\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118978\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845645\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.787704                \tClf: 2.695361\tReg: 0.000061\tFr_p: 0.109837\tFr_r: 0.119196\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860522\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.080736                \tClf: 2.987648\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118834\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849961\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.975635                \tClf: 2.883076\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119351\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842072\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.207478                \tClf: 3.115313\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.119255\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855783\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.073625                \tClf: 2.980775\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119498\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835152\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.866059                \tClf: 2.774240\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119161\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839465\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.203344                \tClf: 2.111309\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118909\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846615\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.947919                \tClf: 2.855527\tReg: 0.000061\tFr_p: 0.383532\tFr_r: 0.414965\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843449\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.008231                \tClf: 2.915997\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118761\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854299\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.902130                \tClf: 2.809354\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118714\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863299\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.206173                \tClf: 3.112947\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118995\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843162\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.825942                \tClf: 2.733722\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118867\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865521\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.933902                \tClf: 2.840565\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119374\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849857\tlr: 0.001000\ttrain acc:97.0750\tLoss: 2.939103                \tClf: 2.846549\tReg: 0.000061\tFr_p: 0.110013\tFr_r: 0.119789\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837883\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.015758                \tClf: 2.923803\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118972\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844533\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.826167                \tClf: 2.733880\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119193\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860772\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.086328                \tClf: 2.993228\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118835\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849554\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.924765                \tClf: 2.832226\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119362\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842549\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.218783                \tClf: 3.126595\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119250\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855116\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.111233                \tClf: 3.018416\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119510\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835572\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.772188                \tClf: 2.680348\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119155\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839811\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.243628                \tClf: 2.151576\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.845828\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.938913                \tClf: 2.846560\tReg: 0.000061\tFr_p: 0.383551\tFr_r: 0.414924\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843312\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.954402                \tClf: 2.862176\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118793\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854453\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.877810                \tClf: 2.785026\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118678\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863281\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.232443                \tClf: 3.139218\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119006\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842750\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.852260                \tClf: 2.760061\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118879\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864690\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.981810                \tClf: 2.888514\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119330\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850446\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.986405                \tClf: 2.893822\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119807\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837848\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.027199                \tClf: 2.935246\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118976\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845134\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.824443                \tClf: 2.732125\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119178\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859907\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.083491                \tClf: 2.990434\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118821\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849984\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.947393                \tClf: 2.854832\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119376\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842741\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.119887                \tClf: 3.027689\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119249\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855677\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.115139                \tClf: 3.022294\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119541\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835242\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.827972                \tClf: 2.736149\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119145\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.840046\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.220665                \tClf: 2.128602\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118907\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845614\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.034028                \tClf: 2.941686\tReg: 0.000061\tFr_p: 0.383537\tFr_r: 0.414940\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843294\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.027470                \tClf: 2.935244\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118796\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855011\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.951739                \tClf: 2.858927\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118702\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863090\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.245729                \tClf: 3.152514\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118992\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842741\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.838082                \tClf: 2.745884\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118874\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865134\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.963135                \tClf: 2.869817\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119352\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850457\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.944975                \tClf: 2.852391\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119796\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837983\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.013978                \tClf: 2.922017\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118986\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845061\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.799829                \tClf: 2.707515\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119194\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859710\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.023281                \tClf: 2.930234\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118834\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849657\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.899182                \tClf: 2.806638\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119345\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842056\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.184668                \tClf: 3.092504\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.119259\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855638\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.141290                \tClf: 3.048447\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119501\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835254\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.801189                \tClf: 2.709366\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119162\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839750\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.202174                \tClf: 2.110126\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846886\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.006688                \tClf: 2.914283\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414915\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843723\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.996048                \tClf: 2.903801\tReg: 0.000061\tFr_p: 0.109573\tFr_r: 0.118763\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854850\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.931928                \tClf: 2.839124\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118737\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862878\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.156367                \tClf: 3.063162\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118989\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843051\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.871094                \tClf: 2.778880\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118871\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864794\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.931067                \tClf: 2.837766\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119371\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850019\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.964129                \tClf: 2.871567\tReg: 0.000061\tFr_p: 0.110037\tFr_r: 0.119819\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837923\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.008566                \tClf: 2.916609\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118987\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844761\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.855232                \tClf: 2.762933\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119181\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859843\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.132406                \tClf: 3.039352\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118831\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850246\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.947866                \tClf: 2.855292\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119366\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843247\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.183559                \tClf: 3.091335\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.119274\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855593\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.087467                \tClf: 2.994626\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119544\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835102\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.844686                \tClf: 2.752870\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119158\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839653\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.206875                \tClf: 2.114831\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846999\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.012236                \tClf: 2.919825\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414948\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842421\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.943852                \tClf: 2.851670\tReg: 0.000061\tFr_p: 0.109604\tFr_r: 0.118797\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855207\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.951477                \tClf: 2.858655\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118694\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862919\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.177280                \tClf: 3.084073\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118978\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842971\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.854443                \tClf: 2.762233\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118905\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.864895\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.971648                \tClf: 2.878342\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119362\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850389\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.066953                \tClf: 2.974372\tReg: 0.000061\tFr_p: 0.110052\tFr_r: 0.119843\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837972\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.007828                \tClf: 2.915868\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118970\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845428\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.827654                \tClf: 2.735321\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119184\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860216\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.146290                \tClf: 3.053219\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118823\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849816\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.960346                \tClf: 2.867794\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119380\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.841836\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.177773                \tClf: 3.085621\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.119260\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855609\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.084229                \tClf: 2.991387\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119523\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835101\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.836484                \tClf: 2.744668\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119163\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839538\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.238668                \tClf: 2.146630\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845897\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.946771                \tClf: 2.854415\tReg: 0.000061\tFr_p: 0.383544\tFr_r: 0.414963\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843177\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.915629                \tClf: 2.823409\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118784\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854334\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.888230                \tClf: 2.795452\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118719\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863228\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.220956                \tClf: 3.127734\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119000\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842972\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.885964                \tClf: 2.793754\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118913\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865070\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.011277                \tClf: 2.917963\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119383\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850314\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.936423                \tClf: 2.843846\tReg: 0.000061\tFr_p: 0.110023\tFr_r: 0.119816\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837333\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.972650                \tClf: 2.880723\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118996\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845100\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.902292                \tClf: 2.809976\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119225\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860089\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.106518                \tClf: 3.013452\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118841\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850720\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.893829                \tClf: 2.801232\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119362\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842425\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.193586                \tClf: 3.101404\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119261\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855328\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.090352                \tClf: 2.997525\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119509\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835263\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.867379                \tClf: 2.775555\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119194\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839830\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.161876                \tClf: 2.069823\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846998\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.966630                \tClf: 2.874219\tReg: 0.000061\tFr_p: 0.383539\tFr_r: 0.414953\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843172\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.913766                \tClf: 2.821546\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118774\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854767\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.975319                \tClf: 2.882520\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118704\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862657\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.210046                \tClf: 3.116852\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119009\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843231\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.789595                \tClf: 2.697372\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118881\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865108\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.997927                \tClf: 2.904610\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119344\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850308\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.968484                \tClf: 2.875908\tReg: 0.000061\tFr_p: 0.110021\tFr_r: 0.119821\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837552\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.026560                \tClf: 2.934621\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118987\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845234\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.830944                \tClf: 2.738621\tReg: 0.000061\tFr_p: 0.109848\tFr_r: 0.119215\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859427\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.142733                \tClf: 3.049701\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118802\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849543\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.905195                \tClf: 2.812656\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119349\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.841576\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.173664                \tClf: 3.081524\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.119278\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855379\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.113492                \tClf: 3.020662\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119526\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835043\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.852727                \tClf: 2.760914\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119180\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839554\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.211887                \tClf: 2.119848\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118905\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846158\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.987634                \tClf: 2.895265\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414964\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843304\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.955040                \tClf: 2.862813\tReg: 0.000061\tFr_p: 0.109564\tFr_r: 0.118780\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855184\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.962820                \tClf: 2.870000\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118718\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863594\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.154170                \tClf: 3.060929\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118983\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843436\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.843408                \tClf: 2.751175\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118915\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865705\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.955342                \tClf: 2.861996\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119343\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850601\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.963698                \tClf: 2.871107\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119825\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838157\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.015620                \tClf: 2.923651\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118983\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844864\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.779416                \tClf: 2.687112\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119189\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860243\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.123440                \tClf: 3.030367\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118824\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849918\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.949186                \tClf: 2.856629\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119323\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843155\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.218212                \tClf: 3.125993\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119291\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855538\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.085800                \tClf: 2.992962\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119507\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835064\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.867676                \tClf: 2.775861\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119172\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839577\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.158253                \tClf: 2.066213\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845937\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.989811                \tClf: 2.897453\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414942\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843238\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.945563                \tClf: 2.853340\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118776\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854100\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.951868                \tClf: 2.859102\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118706\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862975\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.241069                \tClf: 3.147859\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118980\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843108\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.825754                \tClf: 2.733537\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118914\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865415\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.918238                \tClf: 2.824906\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119343\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849715\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.977207                \tClf: 2.884660\tReg: 0.000061\tFr_p: 0.110037\tFr_r: 0.119799\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837738\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.063800                \tClf: 2.971852\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118957\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844986\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.806148                \tClf: 2.713838\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119212\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860291\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.047595                \tClf: 2.954519\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118800\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849640\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.988886                \tClf: 2.896343\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119355\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843139\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.269488                \tClf: 3.177269\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119279\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855412\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.074780                \tClf: 2.981948\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119512\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835609\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.873988                \tClf: 2.782146\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119182\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839717\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.202184                \tClf: 2.110137\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118906\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845875\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.941811                \tClf: 2.849457\tReg: 0.000061\tFr_p: 0.383560\tFr_r: 0.414944\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843529\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.939836                \tClf: 2.847599\tReg: 0.000061\tFr_p: 0.109608\tFr_r: 0.118790\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855540\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.991228                \tClf: 2.898390\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118727\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862643\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.237451                \tClf: 3.144258\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118969\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842991\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.841472                \tClf: 2.749261\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118897\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865093\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.998514                \tClf: 2.905198\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119375\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850095\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.990576                \tClf: 2.898010\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119829\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837704\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.997241                \tClf: 2.905295\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118943\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845039\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.817684                \tClf: 2.725371\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119187\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860140\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.099378                \tClf: 3.006310\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118818\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850157\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.882087                \tClf: 2.789518\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119340\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842762\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.247776                \tClf: 3.155577\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.119284\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.854968\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.095943                \tClf: 3.003133\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119517\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834753\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.861674                \tClf: 2.769875\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119169\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839811\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.212145                \tClf: 2.120094\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846033\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.955525                \tClf: 2.863163\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414908\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842713\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.001418                \tClf: 2.909222\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118784\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854206\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.933870                \tClf: 2.841099\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118686\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863279\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.285813                \tClf: 3.192588\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118986\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843425\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.891027                \tClf: 2.798795\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118899\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864879\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.926891                \tClf: 2.833585\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119362\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850225\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.896898                \tClf: 2.804325\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119824\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838239\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.984828                \tClf: 2.892855\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118956\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844714\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.841401                \tClf: 2.749104\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119207\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860229\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.117432                \tClf: 3.024360\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118820\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850504\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.859655                \tClf: 2.767069\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119399\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.841770\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.158830                \tClf: 3.066681\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.119265\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.854926\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.103073                \tClf: 3.010266\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119507\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835445\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.810199                \tClf: 2.718365\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119181\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839271\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.245071                \tClf: 2.153046\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118864\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846226\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.971733                \tClf: 2.879361\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414941\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843572\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.972779                \tClf: 2.880539\tReg: 0.000061\tFr_p: 0.109593\tFr_r: 0.118786\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855461\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.922562                \tClf: 2.829728\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118687\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863084\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.187735                \tClf: 3.094520\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118987\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842909\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.837352                \tClf: 2.745146\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118874\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865501\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.971054                \tClf: 2.877717\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119351\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850411\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.965673                \tClf: 2.873092\tReg: 0.000061\tFr_p: 0.110015\tFr_r: 0.119802\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837791\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.029526                \tClf: 2.937576\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118970\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845106\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.854188                \tClf: 2.761871\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119215\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859673\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.080907                \tClf: 2.987862\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118817\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850080\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.829494                \tClf: 2.736928\tReg: 0.000061\tFr_p: 0.109877\tFr_r: 0.119364\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842494\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.175622                \tClf: 3.083436\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119258\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855623\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.089914                \tClf: 2.997072\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119493\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835138\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.852112                \tClf: 2.760294\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.119169\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839705\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.161679                \tClf: 2.069633\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118891\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845967\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.026071                \tClf: 2.933712\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414954\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843359\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.884407                \tClf: 2.792177\tReg: 0.000061\tFr_p: 0.109581\tFr_r: 0.118786\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855426\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.909677                \tClf: 2.816844\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118703\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862682\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.239853                \tClf: 3.146658\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118979\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842851\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.723175                \tClf: 2.630971\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118915\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864400\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.957015                \tClf: 2.863734\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119359\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850153\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.952446                \tClf: 2.859878\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119833\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838188\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.049146                \tClf: 2.957175\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118982\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845342\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.823362                \tClf: 2.731033\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119234\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.859802\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.118673                \tClf: 3.025621\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118820\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849817\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.951961                \tClf: 2.859409\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119334\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842771\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.237138                \tClf: 3.144938\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119262\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855770\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.049186                \tClf: 2.956336\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119531\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835307\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.884463                \tClf: 2.792637\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119155\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839968\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.175529                \tClf: 2.083469\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.845748\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.004218                \tClf: 2.911870\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414933\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843291\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.978333                \tClf: 2.886107\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118787\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854886\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.940546                \tClf: 2.847741\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118688\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863726\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.208787                \tClf: 3.115539\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119013\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843644\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.836852                \tClf: 2.744609\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118930\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865249\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.944152                \tClf: 2.850829\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119345\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849552\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.944862                \tClf: 2.852323\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119790\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837598\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.023261                \tClf: 2.931320\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118970\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844609\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.841951                \tClf: 2.749659\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119206\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860409\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.156852                \tClf: 3.063771\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118837\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850873\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.903372                \tClf: 2.810767\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119364\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842887\tlr: 0.001000\ttrain acc:96.7000\tLoss: 3.217676                \tClf: 3.125470\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119287\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855560\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.133412                \tClf: 3.040573\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119488\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.834769\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.848258                \tClf: 2.756458\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119142\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839589\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.264827                \tClf: 2.172787\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846078\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.966942                \tClf: 2.874577\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414931\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843909\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.924738                \tClf: 2.832482\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118778\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855117\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.909599                \tClf: 2.816782\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118690\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862882\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.223369                \tClf: 3.130164\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119000\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842855\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.836345                \tClf: 2.744141\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118899\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864783\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.909711                \tClf: 2.816411\tReg: 0.000061\tFr_p: 0.109855\tFr_r: 0.119342\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850195\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.933476                \tClf: 2.840905\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119840\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.838918\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.021464                \tClf: 2.929457\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118990\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844701\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.825821                \tClf: 2.733525\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119186\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859720\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.083475                \tClf: 2.990428\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118803\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849627\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.934542                \tClf: 2.842000\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119338\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843042\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.224209                \tClf: 3.131995\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119274\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855444\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.072759                \tClf: 2.979926\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119498\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835697\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.858445                \tClf: 2.766599\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.119144\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839923\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.190556                \tClf: 2.098499\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846215\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.933176                \tClf: 2.840805\tReg: 0.000061\tFr_p: 0.383537\tFr_r: 0.414915\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843335\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.946458                \tClf: 2.854230\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118761\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854896\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.914405                \tClf: 2.821599\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118708\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862975\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.210036                \tClf: 3.116826\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119005\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842930\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.929719                \tClf: 2.837511\tReg: 0.000061\tFr_p: 0.109665\tFr_r: 0.118899\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864923\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.921245                \tClf: 2.827938\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119350\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849823\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.986180                \tClf: 2.893627\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119799\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837095\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.039010                \tClf: 2.947094\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118953\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844883\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.777045                \tClf: 2.684740\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119179\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859356\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.113857                \tClf: 3.020829\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118807\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850109\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.958780                \tClf: 2.866214\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119374\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842178\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.165279                \tClf: 3.073109\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.119253\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855741\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.058569                \tClf: 2.965721\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119531\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.834711\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.825235                \tClf: 2.733439\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.119156\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839724\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.198236                \tClf: 2.106189\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846152\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.004579                \tClf: 2.912211\tReg: 0.000061\tFr_p: 0.383522\tFr_r: 0.414921\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843108\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.986918                \tClf: 2.894702\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118796\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855858\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.912191                \tClf: 2.819337\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118734\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863201\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.213351                \tClf: 3.120130\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119000\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842742\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.870538                \tClf: 2.778340\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118900\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864840\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.966294                \tClf: 2.872991\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119361\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850484\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.886352                \tClf: 2.793767\tReg: 0.000061\tFr_p: 0.110049\tFr_r: 0.119837\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837691\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.984054                \tClf: 2.892109\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118947\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845310\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.729540                \tClf: 2.637213\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119192\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859819\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.092941                \tClf: 2.999889\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118831\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850026\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.902867                \tClf: 2.810305\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119340\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842416\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.151988                \tClf: 3.059806\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.119242\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855883\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.087506                \tClf: 2.994651\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119515\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835006\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.846540                \tClf: 2.754728\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119180\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839846\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.209004                \tClf: 2.116951\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118903\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845893\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.982285                \tClf: 2.889929\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414947\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842489\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.998559                \tClf: 2.906373\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118767\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854591\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.904099                \tClf: 2.811308\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118696\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863083\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.236987                \tClf: 3.143772\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.119019\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842899\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.875816                \tClf: 2.783610\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118883\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864568\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.013161                \tClf: 2.919872\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119369\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850234\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.990791                \tClf: 2.898218\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119803\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.838066\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.996203                \tClf: 2.904239\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118992\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845101\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.827963                \tClf: 2.735647\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119177\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860107\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.142726                \tClf: 3.049660\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118845\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850529\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.940866                \tClf: 2.848279\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119342\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843018\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.169140                \tClf: 3.076928\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119272\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855367\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.163025                \tClf: 3.070196\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119513\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835882\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.868041                \tClf: 2.776186\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119183\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.840118\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.180929                \tClf: 2.088862\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118906\n",
      "\n",
      "Test set: Average loss: 0.0931, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846663\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.011065                \tClf: 2.918671\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414950\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843044\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.927473                \tClf: 2.835260\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118778\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854796\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.918562                \tClf: 2.825761\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118739\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862897\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.238726                \tClf: 3.145520\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118955\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843043\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.857578                \tClf: 2.765365\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118897\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865359\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.963272                \tClf: 2.869943\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119341\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850013\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.952698                \tClf: 2.860136\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119820\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837764\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.049858                \tClf: 2.957908\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118975\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845474\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.798328                \tClf: 2.705993\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119219\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859995\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.133452                \tClf: 3.040391\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118843\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850019\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.933724                \tClf: 2.841162\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119374\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842681\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.235525                \tClf: 3.143329\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119284\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855684\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.114127                \tClf: 3.021282\tReg: 0.000061\tFr_p: 0.109795\tFr_r: 0.119516\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.834588\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.860836                \tClf: 2.769046\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119155\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.840124\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.213951                \tClf: 2.121883\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845990\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.926306                \tClf: 2.833945\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414935\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842869\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.935916                \tClf: 2.843711\tReg: 0.000061\tFr_p: 0.109570\tFr_r: 0.118784\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854605\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.960629                \tClf: 2.867838\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118721\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862556\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.219386                \tClf: 3.126197\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119001\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842912\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.802413                \tClf: 2.710206\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118887\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865740\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.958720                \tClf: 2.865372\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119340\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850472\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.934954                \tClf: 2.842369\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119816\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837702\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.003369                \tClf: 2.911423\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118992\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845811\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.805334                \tClf: 2.712982\tReg: 0.000061\tFr_p: 0.109841\tFr_r: 0.119212\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860365\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.099010                \tClf: 3.005930\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118802\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850458\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.922578                \tClf: 2.829994\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119316\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842519\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.211222                \tClf: 3.119035\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119267\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855803\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.119655                \tClf: 3.026804\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119505\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834826\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.859732                \tClf: 2.767930\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119175\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839412\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.228530                \tClf: 2.136499\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846105\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.975980                \tClf: 2.883613\tReg: 0.000061\tFr_p: 0.383571\tFr_r: 0.414954\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843563\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.932513                \tClf: 2.840274\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118779\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854722\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.857163                \tClf: 2.764365\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118709\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862714\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.273451                \tClf: 3.180254\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118995\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843227\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.773444                \tClf: 2.681222\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118882\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865512\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.956112                \tClf: 2.862776\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119353\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850112\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.008922                \tClf: 2.916355\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119807\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837655\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.022039                \tClf: 2.930095\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118924\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845056\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.816997                \tClf: 2.724683\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119176\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859746\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.113059                \tClf: 3.020011\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118797\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850634\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.907935                \tClf: 2.815343\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119368\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842443\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.217406                \tClf: 3.125223\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.119281\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855784\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.131630                \tClf: 3.038780\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119514\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834993\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.873011                \tClf: 2.781201\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119194\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839559\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.180909                \tClf: 2.088870\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118921\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846381\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.008793                \tClf: 2.916413\tReg: 0.000061\tFr_p: 0.383567\tFr_r: 0.414957\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843252\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.939041                \tClf: 2.846817\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118778\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854374\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.876464                \tClf: 2.783685\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118680\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863218\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.169794                \tClf: 3.076572\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118970\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843260\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.816345                \tClf: 2.724121\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118890\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864586\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.004712                \tClf: 2.911421\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119327\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.851046\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.964630                \tClf: 2.872017\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119823\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837861\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.048616                \tClf: 2.956662\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118982\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.844793\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.880067                \tClf: 2.787766\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119174\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860199\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.078109                \tClf: 2.985038\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118826\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850395\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.008401                \tClf: 2.915820\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119367\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842954\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.134739                \tClf: 3.042530\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119281\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856079\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.087818                \tClf: 2.994952\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119487\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835744\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.803318                \tClf: 2.711470\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119188\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839578\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.199436                \tClf: 2.107396\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845897\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.963979                \tClf: 2.871623\tReg: 0.000061\tFr_p: 0.383530\tFr_r: 0.414941\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842532\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.030112                \tClf: 2.937924\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118800\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854485\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.942969                \tClf: 2.850184\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118694\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862917\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.189345                \tClf: 3.096138\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118994\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842088\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.861335                \tClf: 2.769170\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118884\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865399\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.922072                \tClf: 2.828741\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119371\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850212\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.922106                \tClf: 2.829534\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119825\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838351\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.038281                \tClf: 2.946303\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118977\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845022\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.849451                \tClf: 2.757138\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119191\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859421\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.131969                \tClf: 3.038937\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118816\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849884\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.918873                \tClf: 2.826317\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119341\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842588\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.209000                \tClf: 3.116810\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119300\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855555\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.092051                \tClf: 2.999212\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119515\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835330\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.819642                \tClf: 2.727815\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.119139\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839210\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.214295                \tClf: 2.122273\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846693\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.953398                \tClf: 2.861003\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414987\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.842864\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.976609                \tClf: 2.884405\tReg: 0.000061\tFr_p: 0.109571\tFr_r: 0.118791\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854487\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.040556                \tClf: 2.947770\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118727\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862786\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.214288                \tClf: 3.121087\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118999\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843259\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.889866                \tClf: 2.797642\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118921\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865114\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.866299                \tClf: 2.772982\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119377\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849643\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.995191                \tClf: 2.902648\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119817\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837412\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.021484                \tClf: 2.929552\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118962\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845710\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.808899                \tClf: 2.716552\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119203\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859756\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.112598                \tClf: 3.019549\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118819\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849344\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.880188                \tClf: 2.787659\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119356\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842881\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.160441                \tClf: 3.068236\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.119257\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855509\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.114625                \tClf: 3.021789\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119506\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835350\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.884597                \tClf: 2.792768\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119188\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.838911\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.187944                \tClf: 2.095937\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845674\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.961541                \tClf: 2.869196\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414914\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843237\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.996978                \tClf: 2.904755\tReg: 0.000061\tFr_p: 0.109569\tFr_r: 0.118772\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855616\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.935085                \tClf: 2.842243\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118705\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863158\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.217681                \tClf: 3.124462\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118995\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843027\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.787743                \tClf: 2.695530\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118917\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865282\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.945170                \tClf: 2.851845\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119375\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.849883\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.923689                \tClf: 2.831133\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119795\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837885\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.969039                \tClf: 2.877083\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118962\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845344\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.816795                \tClf: 2.724467\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119213\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860049\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.136916                \tClf: 3.043852\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118810\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850034\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.921383                \tClf: 2.828820\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119363\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842860\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.286372                \tClf: 3.194168\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119276\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855567\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.093249                \tClf: 3.000409\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119518\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835012\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.872406                \tClf: 2.780594\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119158\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839880\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.275890                \tClf: 2.183835\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846069\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.911005                \tClf: 2.818640\tReg: 0.000061\tFr_p: 0.383512\tFr_r: 0.414924\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843004\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.978723                \tClf: 2.886512\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118774\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855320\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.913382                \tClf: 2.820555\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118751\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863926\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.251840                \tClf: 3.158582\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119025\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842578\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.855223                \tClf: 2.763033\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118892\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865090\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.935892                \tClf: 2.842576\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119344\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850620\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.945803                \tClf: 2.853211\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119815\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838129\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.999546                \tClf: 2.907578\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118963\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844547\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.864368                \tClf: 2.772080\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119199\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859707\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.114338                \tClf: 3.021292\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118800\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850081\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.876710                \tClf: 2.784145\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119358\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842890\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.207433                \tClf: 3.115228\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119246\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855639\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.136791                \tClf: 3.043948\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119519\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834784\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.806042                \tClf: 2.714242\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119164\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839251\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.164228                \tClf: 2.072205\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118930\n",
      "\n",
      "Test set: Average loss: 0.0973, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846088\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.974018                \tClf: 2.881653\tReg: 0.000061\tFr_p: 0.383522\tFr_r: 0.414913\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.842937\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.996057                \tClf: 2.903849\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118781\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854324\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.887504                \tClf: 2.794727\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118706\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862378\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.222939                \tClf: 3.129759\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118987\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842636\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.776013                \tClf: 2.683820\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118887\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865253\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.996350                \tClf: 2.903026\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119354\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849682\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.905785                \tClf: 2.813240\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119800\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837440\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.964960                \tClf: 2.873027\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118938\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844883\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.766080                \tClf: 2.673775\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119191\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860071\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.107531                \tClf: 3.014467\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118836\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849710\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.025329                \tClf: 2.932783\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119354\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842700\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.116848                \tClf: 3.024652\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119262\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855565\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.137415                \tClf: 3.044575\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119514\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834299\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.822412                \tClf: 2.730636\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119177\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839340\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.200478                \tClf: 2.108450\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118874\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846428\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.938130                \tClf: 2.845748\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414948\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843010\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.927954                \tClf: 2.835742\tReg: 0.000061\tFr_p: 0.109611\tFr_r: 0.118801\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855223\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.939126                \tClf: 2.846304\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118723\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862726\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.275779                \tClf: 3.182582\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118982\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842935\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.808971                \tClf: 2.716764\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118913\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864719\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.933404                \tClf: 2.840107\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119374\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.851072\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.937768                \tClf: 2.845153\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119786\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838059\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.044039                \tClf: 2.952075\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118976\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844521\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.825571                \tClf: 2.733284\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119189\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860158\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.144606                \tClf: 3.051537\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118826\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850038\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.945307                \tClf: 2.852744\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119356\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.843102\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.190012                \tClf: 3.097795\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119292\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855023\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.092718                \tClf: 2.999906\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119523\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835184\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.880632                \tClf: 2.788811\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119151\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839079\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.229229                \tClf: 2.137214\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846080\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.978985                \tClf: 2.886620\tReg: 0.000061\tFr_p: 0.383567\tFr_r: 0.414947\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843132\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.957069                \tClf: 2.864851\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118795\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854600\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.973933                \tClf: 2.881142\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118694\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862698\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.160592                \tClf: 3.067396\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119010\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842760\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.807125                \tClf: 2.714926\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118902\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865466\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.911356                \tClf: 2.818021\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119359\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850424\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.978519                \tClf: 2.885937\tReg: 0.000061\tFr_p: 0.110013\tFr_r: 0.119815\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838210\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.065988                \tClf: 2.974017\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119015\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844942\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.822806                \tClf: 2.730498\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119195\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859739\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.127246                \tClf: 3.034198\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118816\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850011\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.941036                \tClf: 2.848474\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119361\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842711\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.211298                \tClf: 3.119102\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119258\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855074\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.079309                \tClf: 2.986494\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119525\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835139\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.844802                \tClf: 2.752984\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119199\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839066\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.195894                \tClf: 2.103880\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845819\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.000240                \tClf: 2.907888\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414917\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843541\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.944171                \tClf: 2.851932\tReg: 0.000061\tFr_p: 0.109571\tFr_r: 0.118749\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855417\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.935710                \tClf: 2.842878\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118703\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862558\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.151933                \tClf: 3.058744\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118983\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842791\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.801241                \tClf: 2.709040\tReg: 0.000061\tFr_p: 0.109659\tFr_r: 0.118871\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865575\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.934637                \tClf: 2.841297\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119367\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.851227\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.975935                \tClf: 2.883313\tReg: 0.000061\tFr_p: 0.110045\tFr_r: 0.119826\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837848\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.043091                \tClf: 2.951138\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118975\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844376\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.860825                \tClf: 2.768545\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119197\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859897\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.123008                \tClf: 3.029952\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118816\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850049\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.897722                \tClf: 2.805158\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119358\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842796\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.177772                \tClf: 3.085572\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119279\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855824\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.102152                \tClf: 3.009300\tReg: 0.000061\tFr_p: 0.109795\tFr_r: 0.119504\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835514\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.895498                \tClf: 2.803661\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119183\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.840435\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.232269                \tClf: 2.140186\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118909\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.845641\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.019871                \tClf: 2.927527\tReg: 0.000061\tFr_p: 0.383548\tFr_r: 0.414927\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843250\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.932346                \tClf: 2.840122\tReg: 0.000061\tFr_p: 0.109610\tFr_r: 0.118789\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855059\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.911328                \tClf: 2.818513\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118712\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862844\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.179675                \tClf: 3.086472\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118977\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842901\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.839478                \tClf: 2.747272\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118888\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865729\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.920012                \tClf: 2.826665\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119364\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849584\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.009484                \tClf: 2.916944\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119806\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837624\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.981063                \tClf: 2.889121\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118971\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845038\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.796196                \tClf: 2.703883\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119196\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859848\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.104239                \tClf: 3.011186\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118823\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849787\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.896779                \tClf: 2.804229\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119342\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842406\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.151538                \tClf: 3.059357\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119288\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.856078\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.133524                \tClf: 3.040659\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119502\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835518\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.891816                \tClf: 2.799979\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119154\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839844\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.201740                \tClf: 2.109687\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118874\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846094\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.946762                \tClf: 2.854396\tReg: 0.000061\tFr_p: 0.383497\tFr_r: 0.414948\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843334\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.955672                \tClf: 2.863444\tReg: 0.000061\tFr_p: 0.109562\tFr_r: 0.118752\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.856057\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.965729                \tClf: 2.872865\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118743\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863331\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.261908                \tClf: 3.168680\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118990\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843251\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.870643                \tClf: 2.778420\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118880\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.866036\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.959198                \tClf: 2.865836\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119370\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849962\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.912986                \tClf: 2.820426\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119802\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837785\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.014349                \tClf: 2.922398\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118970\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.844893\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.809086                \tClf: 2.716781\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119202\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860427\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.028479                \tClf: 2.935397\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118833\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849806\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.980423                \tClf: 2.887872\tReg: 0.000061\tFr_p: 0.109898\tFr_r: 0.119365\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842622\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.192468                \tClf: 3.100275\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119273\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855642\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.132882                \tClf: 3.040038\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119544\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835660\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.839587                \tClf: 2.747743\tReg: 0.000061\tFr_p: 0.109785\tFr_r: 0.119182\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839509\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.210904                \tClf: 2.118868\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845954\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.972547                \tClf: 2.880188\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414918\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843106\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.946816                \tClf: 2.854599\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118782\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855281\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.976918                \tClf: 2.884093\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118723\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862788\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.227969                \tClf: 3.134768\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118963\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842956\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.856146                \tClf: 2.763937\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118880\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864669\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.850537                \tClf: 2.757243\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119354\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849823\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.981176                \tClf: 2.888624\tReg: 0.000061\tFr_p: 0.110008\tFr_r: 0.119798\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.838143\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.028968                \tClf: 2.937000\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118965\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844771\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.824148                \tClf: 2.731848\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119203\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860499\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.131060                \tClf: 3.037974\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118839\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850111\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.901890                \tClf: 2.809324\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119357\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842963\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.174392                \tClf: 3.082182\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119270\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855155\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.060408                \tClf: 2.967589\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119524\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835327\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.830154                \tClf: 2.738326\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.119157\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839956\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.205978                \tClf: 2.113919\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845975\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.959905                \tClf: 2.867545\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414919\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843186\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.030856                \tClf: 2.938635\tReg: 0.000061\tFr_p: 0.109604\tFr_r: 0.118811\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854384\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.899840                \tClf: 2.807060\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118713\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862992\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.239719                \tClf: 3.146508\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119005\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842826\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.824594                \tClf: 2.732391\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118869\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865670\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.947638                \tClf: 2.854293\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119364\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850362\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.996080                \tClf: 2.903501\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119808\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837863\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.046218                \tClf: 2.954264\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118980\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845363\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.847099                \tClf: 2.754770\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119214\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860478\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.091273                \tClf: 2.998188\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118821\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850434\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.868380                \tClf: 2.775797\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119360\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842914\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.245948                \tClf: 3.153741\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119270\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855064\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.129574                \tClf: 3.036760\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119521\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835345\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.831675                \tClf: 2.739847\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119145\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839680\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.186453                \tClf: 2.094408\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845579\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.072387                \tClf: 2.980047\tReg: 0.000061\tFr_p: 0.383526\tFr_r: 0.414915\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843198\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.936116                \tClf: 2.843895\tReg: 0.000061\tFr_p: 0.109595\tFr_r: 0.118768\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854850\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.883173                \tClf: 2.790370\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118709\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863446\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.256447                \tClf: 3.163214\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119000\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843673\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.858235                \tClf: 2.765990\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118915\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.864696\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.913276                \tClf: 2.819981\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119340\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850402\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.962841                \tClf: 2.870260\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119818\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837589\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.979645                \tClf: 2.887705\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.119005\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845156\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.809245                \tClf: 2.716927\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119212\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859742\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.097298                \tClf: 3.004250\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118806\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850060\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.925143                \tClf: 2.832579\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119353\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842727\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.242307                \tClf: 3.150109\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119272\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.856310\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.122693                \tClf: 3.029816\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119509\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835612\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.884356                \tClf: 2.792514\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.119159\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839770\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.214795                \tClf: 2.122745\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846231\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.983573                \tClf: 2.891201\tReg: 0.000061\tFr_p: 0.383569\tFr_r: 0.414963\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842774\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.978875                \tClf: 2.886675\tReg: 0.000061\tFr_p: 0.109564\tFr_r: 0.118768\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854524\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.953012                \tClf: 2.860224\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118689\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862812\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.244200                \tClf: 3.150998\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119001\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842939\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.856692                \tClf: 2.764484\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118863\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864790\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.944898                \tClf: 2.851597\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119366\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850407\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.951529                \tClf: 2.858947\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119822\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837321\tlr: 0.001000\ttrain acc:97.7250\tLoss: 3.005367                \tClf: 2.913440\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118991\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845106\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.852458                \tClf: 2.760142\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119195\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859866\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.076377                \tClf: 2.983322\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118824\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850567\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.944027                \tClf: 2.851438\tReg: 0.000061\tFr_p: 0.109898\tFr_r: 0.119372\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842106\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.177345                \tClf: 3.085179\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.119261\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855303\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.138695                \tClf: 3.045869\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119503\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834944\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.864556                \tClf: 2.772748\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119190\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839905\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.226726                \tClf: 2.134670\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118920\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845691\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.950859                \tClf: 2.858514\tReg: 0.000061\tFr_p: 0.383557\tFr_r: 0.414923\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.844182\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.950532                \tClf: 2.858262\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118793\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855021\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.990830                \tClf: 2.898018\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118686\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863319\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.231920                \tClf: 3.138693\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119014\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843423\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.896022                \tClf: 2.803790\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118885\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865068\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.945843                \tClf: 2.852528\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119359\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850163\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.023650                \tClf: 2.931081\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119777\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838180\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.049491                \tClf: 2.957521\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118965\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.844912\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.889113                \tClf: 2.796807\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119214\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859361\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.075715                \tClf: 2.982686\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118796\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850385\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.940439                \tClf: 2.847858\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119348\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.843059\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.194501                \tClf: 3.102287\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119257\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855307\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.153496                \tClf: 3.060670\tReg: 0.000061\tFr_p: 0.109782\tFr_r: 0.119489\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835275\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.807394                \tClf: 2.715569\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119165\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.838921\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.188691                \tClf: 2.096684\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118886\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845582\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.975314                \tClf: 2.882974\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414963\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843056\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.958020                \tClf: 2.865806\tReg: 0.000061\tFr_p: 0.109571\tFr_r: 0.118760\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855422\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.912783                \tClf: 2.819951\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118723\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862931\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.225931                \tClf: 3.132724\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.119023\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843154\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.821280                \tClf: 2.729062\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118899\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865440\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.986888                \tClf: 2.893555\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119370\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850380\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.910915                \tClf: 2.818335\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119807\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838124\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.008568                \tClf: 2.916600\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118952\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844445\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.835591                \tClf: 2.743307\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119200\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860132\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.122377                \tClf: 3.029309\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118809\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850120\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.971178                \tClf: 2.878611\tReg: 0.000061\tFr_p: 0.109901\tFr_r: 0.119378\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843027\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.187473                \tClf: 3.095261\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119311\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855854\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.093714                \tClf: 3.000860\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119506\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.834943\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.825296                \tClf: 2.733488\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119164\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839880\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.182492                \tClf: 2.090437\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845456\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.981362                \tClf: 2.889028\tReg: 0.000061\tFr_p: 0.383523\tFr_r: 0.414916\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843521\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.030951                \tClf: 2.938714\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118796\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854392\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.953179                \tClf: 2.860399\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118671\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862629\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.196302                \tClf: 3.103109\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119013\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843369\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.872340                \tClf: 2.780110\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118908\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865130\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.887633                \tClf: 2.794315\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119327\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850940\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.918929                \tClf: 2.826321\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119806\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837738\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.068491                \tClf: 2.976543\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118970\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844632\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.824058                \tClf: 2.731766\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119203\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859883\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.078794                \tClf: 2.985738\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118823\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850169\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.982977                \tClf: 2.890408\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119371\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842204\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.178060                \tClf: 3.085889\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.119266\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.856103\tlr: 0.001000\ttrain acc:97.7750\tLoss: 3.113533                \tClf: 3.020667\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119513\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834623\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.869385                \tClf: 2.777593\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.119146\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839720\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.204471                \tClf: 2.112424\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846468\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.021169                \tClf: 2.928785\tReg: 0.000061\tFr_p: 0.383521\tFr_r: 0.414933\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.844032\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.958029                \tClf: 2.865766\tReg: 0.000061\tFr_p: 0.109593\tFr_r: 0.118809\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854868\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.933702                \tClf: 2.840898\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118690\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863390\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.203503                \tClf: 3.110272\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118968\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842937\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.841225                \tClf: 2.749017\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118898\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865019\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.932226                \tClf: 2.838914\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119348\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850513\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.965075                \tClf: 2.872489\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119820\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.838557\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.049920                \tClf: 2.957931\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118978\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844733\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.845304                \tClf: 2.753007\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119196\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859682\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.171227                \tClf: 3.078182\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118826\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850306\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.902089                \tClf: 2.809513\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119362\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842799\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.219917                \tClf: 3.127716\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.119243\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855876\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.222862                \tClf: 3.130008\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119535\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835225\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.827398                \tClf: 2.735576\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.119148\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839016\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.251204                \tClf: 2.159192\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846280\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.975040                \tClf: 2.882665\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414932\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842809\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.990614                \tClf: 2.898413\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118753\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854435\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.936168                \tClf: 2.843385\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118700\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863113\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.245194                \tClf: 3.151977\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.119018\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842407\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.830911                \tClf: 2.738730\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118856\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865017\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.988533                \tClf: 2.895221\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119352\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850623\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.945503                \tClf: 2.852911\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119833\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837719\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.056501                \tClf: 2.964554\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118948\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845185\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.776699                \tClf: 2.684379\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119181\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860199\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.068829                \tClf: 2.975758\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118815\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850002\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.925861                \tClf: 2.833300\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119346\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842914\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.204830                \tClf: 3.112623\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119244\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855100\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.185876                \tClf: 3.093060\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119503\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835508\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.849210                \tClf: 2.757374\tReg: 0.000061\tFr_p: 0.109786\tFr_r: 0.119195\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839513\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.206827                \tClf: 2.114790\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.845871\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.014428                \tClf: 2.922073\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414945\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843676\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.980473                \tClf: 2.888229\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118789\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855068\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.906236                \tClf: 2.813422\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118742\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862279\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.211203                \tClf: 3.118028\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118985\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843857\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.834822                \tClf: 2.742568\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118901\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.864528\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.991869                \tClf: 2.898582\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119364\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850813\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.936760                \tClf: 2.844158\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119820\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837052\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.009578                \tClf: 2.917664\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118957\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845540\tlr: 0.001000\ttrain acc:97.0000\tLoss: 2.818800                \tClf: 2.726461\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119231\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859949\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.129594                \tClf: 3.036536\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118827\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849744\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.916939                \tClf: 2.824391\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119351\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842863\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.233270                \tClf: 3.141066\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119253\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855257\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.122821                \tClf: 3.029997\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119484\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835594\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.867640                \tClf: 2.775799\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119190\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839727\tlr: 0.001000\ttrain acc:98.4750\tLoss: 2.211930                \tClf: 2.119883\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118923\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845927\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.959650                \tClf: 2.867293\tReg: 0.000061\tFr_p: 0.383578\tFr_r: 0.414946\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843624\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.967319                \tClf: 2.875077\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118755\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854550\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.949239                \tClf: 2.856450\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118701\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863102\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.178848                \tClf: 3.085632\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118989\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842995\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.829104                \tClf: 2.736893\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118887\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865158\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.973014                \tClf: 2.879695\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119356\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850048\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.952189                \tClf: 2.859625\tReg: 0.000061\tFr_p: 0.110023\tFr_r: 0.119790\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837370\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.981440                \tClf: 2.889510\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118971\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844964\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.824157                \tClf: 2.731848\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119207\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860115\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.094892                \tClf: 3.001825\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118849\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849180\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.878291                \tClf: 2.785771\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119324\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842940\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.198036                \tClf: 3.105828\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.119268\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855359\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.124180                \tClf: 3.031351\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119507\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835257\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.874328                \tClf: 2.782504\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119186\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.840059\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.277605                \tClf: 2.185541\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846423\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.991036                \tClf: 2.898654\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414924\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843566\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.974448                \tClf: 2.882208\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118795\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855283\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.879350                \tClf: 2.786525\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118714\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862626\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.191689                \tClf: 3.098497\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118951\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843077\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.761229                \tClf: 2.669014\tReg: 0.000061\tFr_p: 0.109671\tFr_r: 0.118867\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864495\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.959307                \tClf: 2.866021\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119347\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850171\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.912318                \tClf: 2.819749\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119797\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838474\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.001304                \tClf: 2.909319\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118960\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845286\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.850687                \tClf: 2.758362\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119209\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860353\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.150128                \tClf: 3.057049\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118817\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849914\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.937743                \tClf: 2.845186\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119368\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842665\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.246229                \tClf: 3.154034\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119279\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855768\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.145796                \tClf: 3.052946\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119513\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835201\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.909735                \tClf: 2.817914\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119167\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839339\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.213052                \tClf: 2.121024\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846322\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.974327                \tClf: 2.881950\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414960\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843637\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.925656                \tClf: 2.833413\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118771\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854594\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.877340                \tClf: 2.784549\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118712\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862951\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.194790                \tClf: 3.101582\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119009\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842160\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.827497                \tClf: 2.735328\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118878\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865023\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.954093                \tClf: 2.860781\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119374\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849850\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.961487                \tClf: 2.868933\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119807\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.838316\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.027292                \tClf: 2.935316\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118964\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845624\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.824152                \tClf: 2.731810\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119186\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859181\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.141778                \tClf: 3.048758\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118795\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850341\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.912363                \tClf: 2.819785\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119352\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842904\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.178947                \tClf: 3.086741\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119299\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855590\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.189816                \tClf: 3.096976\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119529\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835653\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.868637                \tClf: 2.776793\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119187\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.840338\tlr: 0.001000\ttrain acc:98.4750\tLoss: 2.180816                \tClf: 2.088738\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118906\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.845611\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.992674                \tClf: 2.900332\tReg: 0.000061\tFr_p: 0.383581\tFr_r: 0.414999\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.842802\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.932997                \tClf: 2.840796\tReg: 0.000061\tFr_p: 0.109611\tFr_r: 0.118802\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854853\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.955033                \tClf: 2.862229\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118730\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862587\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.181707                \tClf: 3.088517\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118965\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842946\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.814372                \tClf: 2.722164\tReg: 0.000061\tFr_p: 0.109665\tFr_r: 0.118866\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864927\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.934781                \tClf: 2.841473\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119359\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850376\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.008631                \tClf: 2.916051\tReg: 0.000061\tFr_p: 0.110014\tFr_r: 0.119781\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837521\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.971682                \tClf: 2.879745\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118952\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844869\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.800521                \tClf: 2.708217\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119186\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859844\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.136248                \tClf: 3.043194\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118813\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849727\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.891316                \tClf: 2.798769\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119366\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842334\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.142293                \tClf: 3.050116\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119268\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855556\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.172540                \tClf: 3.079701\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119516\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.834876\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.867189                \tClf: 2.775384\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119157\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839522\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.194494                \tClf: 2.102457\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118892\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.845944\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.971882                \tClf: 2.879524\tReg: 0.000061\tFr_p: 0.383532\tFr_r: 0.414941\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843028\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.992116                \tClf: 2.899903\tReg: 0.000061\tFr_p: 0.109593\tFr_r: 0.118804\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854401\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.889906                \tClf: 2.797124\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118703\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862732\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.264008                \tClf: 3.170810\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119008\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843311\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.860904                \tClf: 2.768677\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118892\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865312\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.991282                \tClf: 2.897955\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119372\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850151\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.962687                \tClf: 2.870118\tReg: 0.000061\tFr_p: 0.110044\tFr_r: 0.119816\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837659\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.066705                \tClf: 2.974761\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118969\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845049\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.837213                \tClf: 2.744899\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119221\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860422\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.098440                \tClf: 3.005358\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118834\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850262\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.945652                \tClf: 2.853078\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119360\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842472\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.226750                \tClf: 3.134565\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.119266\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855693\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.115832                \tClf: 3.022986\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119505\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834818\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.843584                \tClf: 2.751782\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119160\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839669\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.203772                \tClf: 2.111727\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118874\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846057\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.005825                \tClf: 2.913461\tReg: 0.000061\tFr_p: 0.383529\tFr_r: 0.414964\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843381\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.938052                \tClf: 2.845821\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118792\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855145\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.898649                \tClf: 2.805831\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118691\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863434\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.234294                \tClf: 3.141061\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118981\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842173\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.805325                \tClf: 2.713156\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118886\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865958\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.940178                \tClf: 2.846819\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119337\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850243\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.950056                \tClf: 2.857482\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119798\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837291\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.026862                \tClf: 2.934936\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118967\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845476\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.830019                \tClf: 2.737684\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119192\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860048\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.035058                \tClf: 2.941995\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118789\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849722\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.936044                \tClf: 2.843497\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119356\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843079\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.161847                \tClf: 3.069632\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119296\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855511\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.077420                \tClf: 2.984583\tReg: 0.000061\tFr_p: 0.109785\tFr_r: 0.119472\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835044\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.867021                \tClf: 2.775208\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119150\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839824\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.243329                \tClf: 2.151277\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118881\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845708\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.942493                \tClf: 2.850146\tReg: 0.000061\tFr_p: 0.383510\tFr_r: 0.414934\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843668\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.980057                \tClf: 2.887812\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118772\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855218\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.970083                \tClf: 2.877261\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118741\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862845\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.246009                \tClf: 3.152805\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118985\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842581\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.900290                \tClf: 2.808100\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118890\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864093\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.945889                \tClf: 2.852624\tReg: 0.000061\tFr_p: 0.109848\tFr_r: 0.119337\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850098\tlr: 0.001000\ttrain acc:97.0750\tLoss: 2.983748                \tClf: 2.891182\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119805\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837933\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.049316                \tClf: 2.957358\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118976\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844946\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.864573                \tClf: 2.772264\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119187\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859189\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.118591                \tClf: 3.025570\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118788\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850092\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.889959                \tClf: 2.797393\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119386\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842356\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.194199                \tClf: 3.102020\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119306\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855627\tlr: 0.001000\ttrain acc:97.7500\tLoss: 3.066129                \tClf: 2.973286\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119502\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.834980\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.892589                \tClf: 2.800779\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119146\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839986\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.209911                \tClf: 2.117851\tReg: 0.000061\tFr_p: 0.109783\tFr_r: 0.118920\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845876\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.971870                \tClf: 2.879515\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414956\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843266\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.935609                \tClf: 2.843385\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118775\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855449\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.951444                \tClf: 2.858610\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118720\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863581\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.203461                \tClf: 3.110221\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119000\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843074\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.796095                \tClf: 2.703881\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118884\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.866136\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.984834                \tClf: 2.891466\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119349\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.849991\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.995583                \tClf: 2.903023\tReg: 0.000061\tFr_p: 0.110046\tFr_r: 0.119833\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838034\tlr: 0.001000\ttrain acc:97.7750\tLoss: 3.029701                \tClf: 2.937738\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118963\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844943\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.807501                \tClf: 2.715193\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119173\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860562\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.067309                \tClf: 2.974220\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118820\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850224\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.939894                \tClf: 2.847322\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119350\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842484\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.225885                \tClf: 3.133700\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119272\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.856435\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.095214                \tClf: 3.002331\tReg: 0.000061\tFr_p: 0.109786\tFr_r: 0.119504\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835058\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.894397                \tClf: 2.802583\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119159\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839387\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.237092                \tClf: 2.145062\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118904\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846348\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.025178                \tClf: 2.932800\tReg: 0.000061\tFr_p: 0.383538\tFr_r: 0.414983\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843496\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.848530                \tClf: 2.756294\tReg: 0.000061\tFr_p: 0.109572\tFr_r: 0.118772\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.855474\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.893830                \tClf: 2.800995\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118717\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862674\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.188094                \tClf: 3.094899\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118966\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843345\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.838642                \tClf: 2.746413\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118894\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865669\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.891079                \tClf: 2.797734\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119346\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850585\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.952540                \tClf: 2.859949\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119817\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837812\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.041749                \tClf: 2.949797\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118957\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845204\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.848325                \tClf: 2.756004\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119205\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860314\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.078254                \tClf: 2.985177\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118815\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850168\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.972150                \tClf: 2.879581\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119358\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842310\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.146163                \tClf: 3.053986\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119248\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855642\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.116498                \tClf: 3.023655\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119544\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835062\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.880465                \tClf: 2.788651\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119180\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839247\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.145460                \tClf: 2.053437\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845593\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.958718                \tClf: 2.866377\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414946\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842707\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.916142                \tClf: 2.823945\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118743\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854514\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.934804                \tClf: 2.842017\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118696\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863092\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.129636                \tClf: 3.036421\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118995\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843266\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.894640                \tClf: 2.802416\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118873\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.866049\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.888879                \tClf: 2.795516\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119354\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850637\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.946633                \tClf: 2.854040\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119816\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837750\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.086122                \tClf: 2.994173\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118962\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.844962\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.778822                \tClf: 2.686513\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119194\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859026\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.101031                \tClf: 3.008019\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118820\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850748\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.900278                \tClf: 2.807679\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119356\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842815\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.198574                \tClf: 3.106372\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119249\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855528\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.070493                \tClf: 2.977656\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119504\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.834890\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.915851                \tClf: 2.824045\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119159\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839942\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.220828                \tClf: 2.128770\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845509\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.010324                \tClf: 2.917987\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414940\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843311\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.992451                \tClf: 2.900225\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118781\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854266\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.944142                \tClf: 2.851368\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118700\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863023\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.186871                \tClf: 3.093658\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119011\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842936\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.817500                \tClf: 2.725292\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118884\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865208\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.908356                \tClf: 2.815034\tReg: 0.000061\tFr_p: 0.109855\tFr_r: 0.119353\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850415\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.937333                \tClf: 2.844751\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119807\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837498\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.002465                \tClf: 2.910529\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118953\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845841\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.783188                \tClf: 2.690834\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119229\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859194\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.130723                \tClf: 3.037702\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118779\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850048\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.922100                \tClf: 2.829537\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119368\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843244\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.194836                \tClf: 3.102612\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119267\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855582\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.180907                \tClf: 3.088067\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119518\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835699\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.880195                \tClf: 2.788349\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119142\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.838697\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.181957                \tClf: 2.089961\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118850\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845378\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.956127                \tClf: 2.863797\tReg: 0.000061\tFr_p: 0.383511\tFr_r: 0.414912\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843975\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.980951                \tClf: 2.888692\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118773\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854337\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.927831                \tClf: 2.835053\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118705\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863234\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.196670                \tClf: 3.103447\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119021\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842667\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.888548                \tClf: 2.796354\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118868\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865380\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.966066                \tClf: 2.872735\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119347\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850243\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.920845                \tClf: 2.828271\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119809\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838169\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.072523                \tClf: 2.980554\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118971\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845376\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.779443                \tClf: 2.687113\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119219\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860599\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.202604                \tClf: 3.109513\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118812\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.851159\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.955865                \tClf: 2.863246\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119376\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843250\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.172483                \tClf: 3.080260\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119263\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855878\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.093756                \tClf: 3.000901\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119521\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834949\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.881377                \tClf: 2.789569\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119155\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839488\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.202533                \tClf: 2.110497\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845777\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.009371                \tClf: 2.917021\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414941\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842979\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.883247                \tClf: 2.791037\tReg: 0.000061\tFr_p: 0.109600\tFr_r: 0.118792\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855411\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.974276                \tClf: 2.881445\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118705\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863531\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.261209                \tClf: 3.167971\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119006\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843182\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.861969                \tClf: 2.769749\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118856\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865470\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.943017                \tClf: 2.849683\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119360\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850307\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.921761                \tClf: 2.829185\tReg: 0.000061\tFr_p: 0.110058\tFr_r: 0.119824\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837722\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.003493                \tClf: 2.911546\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118948\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844748\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.812400                \tClf: 2.720102\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119197\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860484\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.077490                \tClf: 2.984404\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118837\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850438\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.941090                \tClf: 2.848507\tReg: 0.000061\tFr_p: 0.109889\tFr_r: 0.119384\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.843092\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.194013                \tClf: 3.101797\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119265\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.856435\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.080225                \tClf: 2.987342\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119521\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835807\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.910304                \tClf: 2.818452\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119158\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839008\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.212352                \tClf: 2.120340\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846161\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.967907                \tClf: 2.875538\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414944\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843306\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.887432                \tClf: 2.795206\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118756\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855144\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.942258                \tClf: 2.849440\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118715\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862470\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.210857                \tClf: 3.117672\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118999\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843317\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.860312                \tClf: 2.768085\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118915\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865349\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.976477                \tClf: 2.883148\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119346\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850160\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.954476                \tClf: 2.861907\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119784\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838446\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.028645                \tClf: 2.936662\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118969\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845579\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.812073                \tClf: 2.719733\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119207\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860574\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.100109                \tClf: 3.007020\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118836\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850118\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.958864                \tClf: 2.866297\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119364\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.843078\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.161269                \tClf: 3.069054\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.119250\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855070\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.093027                \tClf: 3.000213\tReg: 0.000061\tFr_p: 0.109784\tFr_r: 0.119474\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834617\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.893544                \tClf: 2.801752\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119143\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839198\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.270041                \tClf: 2.178021\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.118913\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846417\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.917778                \tClf: 2.825396\tReg: 0.000061\tFr_p: 0.383551\tFr_r: 0.414946\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843984\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.952654                \tClf: 2.860394\tReg: 0.000061\tFr_p: 0.109602\tFr_r: 0.118786\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854894\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.878618                \tClf: 2.785812\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118717\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862831\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.172815                \tClf: 3.079612\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118994\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842854\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.839170                \tClf: 2.746966\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118899\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.864343\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.931068                \tClf: 2.837789\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119367\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850652\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.962463                \tClf: 2.869870\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119824\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837576\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.009299                \tClf: 2.917359\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118996\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844739\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.776929                \tClf: 2.684631\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119196\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859790\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.118791                \tClf: 3.025741\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118795\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850047\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.909766                \tClf: 2.817203\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119363\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843684\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.176511                \tClf: 3.084265\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119272\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855552\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.163841                \tClf: 3.071003\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119506\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835049\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.857030                \tClf: 2.765216\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.119165\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839371\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.211351                \tClf: 2.119321\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845850\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.961960                \tClf: 2.869607\tReg: 0.000061\tFr_p: 0.383559\tFr_r: 0.414961\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.842547\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.961704                \tClf: 2.869515\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118771\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855324\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.911722                \tClf: 2.818894\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118713\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863169\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.186978                \tClf: 3.093759\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118991\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843218\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.831792                \tClf: 2.739570\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118903\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864690\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.953922                \tClf: 2.860626\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119367\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850428\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.051154                \tClf: 2.958571\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119823\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837634\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.069368                \tClf: 2.977426\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118982\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844927\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.835793                \tClf: 2.743486\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119188\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860555\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.088806                \tClf: 2.995717\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118848\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849659\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.910710                \tClf: 2.818166\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119374\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842531\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.178384                \tClf: 3.086197\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.119237\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855877\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.130895                \tClf: 3.038040\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119517\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835703\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.808350                \tClf: 2.716504\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119188\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839513\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.189808                \tClf: 2.097771\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118909\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846385\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.961673                \tClf: 2.869293\tReg: 0.000061\tFr_p: 0.383547\tFr_r: 0.414950\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843379\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.907420                \tClf: 2.815190\tReg: 0.000061\tFr_p: 0.109566\tFr_r: 0.118785\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855858\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.979402                \tClf: 2.886548\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118721\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862376\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.146131                \tClf: 3.052951\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118998\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843106\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.845848                \tClf: 2.753632\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118880\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864407\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.922605                \tClf: 2.829323\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119325\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850396\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.959286                \tClf: 2.866705\tReg: 0.000061\tFr_p: 0.110010\tFr_r: 0.119813\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837453\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.037907                \tClf: 2.945973\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118952\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845511\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.830707                \tClf: 2.738371\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119200\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860434\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.120566                \tClf: 3.027483\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118810\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850198\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.903330                \tClf: 2.810759\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119344\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843329\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.197020                \tClf: 3.104792\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.119302\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.856254\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.096958                \tClf: 3.004084\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119521\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835079\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.793795                \tClf: 2.701980\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.119158\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.838729\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.226289                \tClf: 2.134291\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845298\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.004826                \tClf: 2.912500\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414920\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843107\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.917201                \tClf: 2.824985\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118767\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854711\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.954182                \tClf: 2.861386\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118699\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862162\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.198783                \tClf: 3.105614\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118985\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842550\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.839762                \tClf: 2.747573\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118900\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865266\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.953819                \tClf: 2.860495\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119342\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849647\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.027636                \tClf: 2.935092\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119801\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837264\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.012507                \tClf: 2.920582\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118981\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845015\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.781474                \tClf: 2.689162\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119175\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859963\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.027639                \tClf: 2.934579\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118827\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850481\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.982115                \tClf: 2.889530\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119356\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842789\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.187375                \tClf: 3.095174\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119276\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855417\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.099722                \tClf: 3.006890\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119513\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834683\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.834440                \tClf: 2.742645\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.119127\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839454\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.133276                \tClf: 2.041242\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118886\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846085\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.999795                \tClf: 2.907430\tReg: 0.000061\tFr_p: 0.383538\tFr_r: 0.414933\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.842928\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.980010                \tClf: 2.887803\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118784\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854365\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.878630                \tClf: 2.785851\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118720\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.861869\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.169659                \tClf: 3.076505\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118974\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843242\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.872068                \tClf: 2.779845\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118885\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865063\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.942801                \tClf: 2.849487\tReg: 0.000061\tFr_p: 0.109855\tFr_r: 0.119339\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850620\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.934658                \tClf: 2.842065\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119821\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837521\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.036466                \tClf: 2.944529\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118988\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845561\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.803718                \tClf: 2.711378\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119174\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860258\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.157973                \tClf: 3.064899\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118798\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849833\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.966121                \tClf: 2.873568\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119329\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.843056\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.227439                \tClf: 3.135225\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.119299\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855610\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.126809                \tClf: 3.033968\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119524\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835253\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.850506                \tClf: 2.758682\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119155\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839943\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.228935                \tClf: 2.136877\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0973, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846362\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.944895                \tClf: 2.852516\tReg: 0.000061\tFr_p: 0.383505\tFr_r: 0.414942\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843601\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.930847                \tClf: 2.838606\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118780\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855205\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.934445                \tClf: 2.841624\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118718\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863120\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.276263                \tClf: 3.183046\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118991\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842944\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.852364                \tClf: 2.760156\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118881\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865087\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.959052                \tClf: 2.865736\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119352\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849703\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.963119                \tClf: 2.870573\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119787\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837817\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.025152                \tClf: 2.933200\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118960\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845091\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.757148                \tClf: 2.664833\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119197\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859259\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.155569                \tClf: 3.062545\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118811\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849543\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.961616                \tClf: 2.869078\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119334\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842318\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.175095                \tClf: 3.082918\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.119250\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855661\tlr: 0.001000\ttrain acc:97.8000\tLoss: 3.121568                \tClf: 3.028724\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119519\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835404\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.855190                \tClf: 2.763359\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119131\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839356\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.238027                \tClf: 2.145999\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118875\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846415\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.976736                \tClf: 2.884354\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414977\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843431\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.973585                \tClf: 2.881353\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118797\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854807\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.863362                \tClf: 2.770561\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118742\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863376\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.150838                \tClf: 3.057608\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119019\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842680\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.809312                \tClf: 2.717117\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118890\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864755\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.968448                \tClf: 2.875149\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119360\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850057\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.993524                \tClf: 2.900960\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119807\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837940\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.029470                \tClf: 2.937512\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118972\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845383\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.829935                \tClf: 2.737605\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119176\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860096\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.183680                \tClf: 3.090614\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118821\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850514\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.986413                \tClf: 2.893826\tReg: 0.000061\tFr_p: 0.109897\tFr_r: 0.119396\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842904\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.144014                \tClf: 3.051808\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119251\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.856406\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.147515                \tClf: 3.054634\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119516\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835283\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.898755                \tClf: 2.806930\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119185\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839953\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.218083                \tClf: 2.126025\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118907\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846260\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.972735                \tClf: 2.880361\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414964\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842968\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.908194                \tClf: 2.815984\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118796\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855211\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.968740                \tClf: 2.875918\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118729\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862974\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.236441                \tClf: 3.143231\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118975\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843390\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.845908                \tClf: 2.753677\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118893\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865743\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.956981                \tClf: 2.863632\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119340\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850748\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.018334                \tClf: 2.925735\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119808\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838297\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.051728                \tClf: 2.959752\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118973\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845688\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.800450                \tClf: 2.708105\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119210\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860404\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.102097                \tClf: 3.009015\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118847\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849989\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.875842                \tClf: 2.783282\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119332\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842615\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.241322                \tClf: 3.149130\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119264\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.856002\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.146736                \tClf: 3.053875\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119508\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835126\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.827261                \tClf: 2.735444\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119148\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839341\tlr: 0.001000\ttrain acc:98.5000\tLoss: 2.198186                \tClf: 2.106158\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845783\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.988443                \tClf: 2.896093\tReg: 0.000061\tFr_p: 0.383537\tFr_r: 0.414941\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843400\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.894493                \tClf: 2.802262\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118771\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855087\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.957828                \tClf: 2.865012\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118727\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863721\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.205130                \tClf: 3.111882\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118992\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842886\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.790721                \tClf: 2.698515\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118875\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865049\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.901564                \tClf: 2.808251\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119373\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.849112\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.985634                \tClf: 2.893117\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119809\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838006\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.024627                \tClf: 2.932666\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118972\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845555\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.781438                \tClf: 2.689099\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119186\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860103\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.080506                \tClf: 2.987439\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118821\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850343\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.945492                \tClf: 2.852914\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119379\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842728\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.165043                \tClf: 3.072846\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.119247\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855825\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.096495                \tClf: 3.003643\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119523\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835506\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.870177                \tClf: 2.778340\tReg: 0.000061\tFr_p: 0.109785\tFr_r: 0.119182\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839621\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.189115                \tClf: 2.097073\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118847\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845573\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.955069                \tClf: 2.862729\tReg: 0.000061\tFr_p: 0.383547\tFr_r: 0.414937\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842225\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.997647                \tClf: 2.905475\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118776\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854472\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.961403                \tClf: 2.868619\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118707\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862884\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.235361                \tClf: 3.142156\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118977\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843140\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.854615                \tClf: 2.762397\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118880\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865494\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.910436                \tClf: 2.817100\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119364\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.849531\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.990829                \tClf: 2.898291\tReg: 0.000061\tFr_p: 0.110044\tFr_r: 0.119823\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838604\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.045492                \tClf: 2.953500\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118972\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845326\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.829847                \tClf: 2.737519\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119178\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860266\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.093003                \tClf: 2.999928\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118847\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849964\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.957044                \tClf: 2.864485\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119357\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.843154\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.115233                \tClf: 3.023014\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119300\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855722\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.064634                \tClf: 2.971787\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119512\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834569\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.841166                \tClf: 2.749376\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119144\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.838755\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.145346                \tClf: 2.053347\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845696\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.987243                \tClf: 2.894897\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414944\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843970\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.011495                \tClf: 2.919235\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118759\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855787\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.949873                \tClf: 2.857023\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118705\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862997\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.134568                \tClf: 3.041357\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118999\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843058\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.825410                \tClf: 2.733196\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118887\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865581\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.927919                \tClf: 2.834579\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119355\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850578\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.951390                \tClf: 2.858800\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119793\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838023\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.019018                \tClf: 2.927056\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118977\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845581\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.812381                \tClf: 2.720041\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119194\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859526\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.129083                \tClf: 3.036046\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118822\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849972\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.891959                \tClf: 2.799400\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119348\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.843141\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.148600                \tClf: 3.056382\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119285\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855820\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.139914                \tClf: 3.047062\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119510\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.836042\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.824238                \tClf: 2.732375\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119192\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839621\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.243584                \tClf: 2.151542\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846306\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.007935                \tClf: 2.915558\tReg: 0.000061\tFr_p: 0.383573\tFr_r: 0.414947\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843469\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.958175                \tClf: 2.865940\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118768\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854414\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.992740                \tClf: 2.899958\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118687\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863118\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.241857                \tClf: 3.148640\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118975\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843778\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.845172                \tClf: 2.752922\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118889\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864778\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.982178                \tClf: 2.888878\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119366\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850340\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.974715                \tClf: 2.882137\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119815\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837573\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.021272                \tClf: 2.929332\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118952\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845115\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.822174                \tClf: 2.729857\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119186\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859764\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.094144                \tClf: 3.001094\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118822\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849553\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.940900                \tClf: 2.848362\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119363\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842556\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.183524                \tClf: 3.091336\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119266\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855300\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.078955                \tClf: 2.986129\tReg: 0.000061\tFr_p: 0.109783\tFr_r: 0.119493\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835725\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.825333                \tClf: 2.733485\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119153\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839433\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.216762                \tClf: 2.124730\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118909\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846244\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.966215                \tClf: 2.873842\tReg: 0.000061\tFr_p: 0.383572\tFr_r: 0.414952\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843490\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.947465                \tClf: 2.855230\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118796\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854481\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.918031                \tClf: 2.825245\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118728\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862879\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.202771                \tClf: 3.109565\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118981\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843077\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.799217                \tClf: 2.707002\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118876\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865144\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.944062                \tClf: 2.850744\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119341\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849073\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.989438                \tClf: 2.896924\tReg: 0.000061\tFr_p: 0.110012\tFr_r: 0.119801\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837626\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.010979                \tClf: 2.919037\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118991\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845267\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.838466                \tClf: 2.746142\tReg: 0.000061\tFr_p: 0.109829\tFr_r: 0.119215\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859922\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.189743                \tClf: 3.096685\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118781\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849749\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.967929                \tClf: 2.875381\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119366\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843368\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.187102                \tClf: 3.094873\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119263\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.856225\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.077430                \tClf: 2.984558\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119529\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835664\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.949221                \tClf: 2.857377\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119164\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839960\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.214204                \tClf: 2.122145\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.118923\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845880\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.926819                \tClf: 2.834464\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414934\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842406\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.008878                \tClf: 2.916696\tReg: 0.000061\tFr_p: 0.109569\tFr_r: 0.118764\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854388\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.967219                \tClf: 2.874439\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118713\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863039\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.244308                \tClf: 3.151095\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118974\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842910\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.809082                \tClf: 2.716875\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118867\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865003\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.970818                \tClf: 2.877506\tReg: 0.000061\tFr_p: 0.109847\tFr_r: 0.119350\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.851194\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.028669                \tClf: 2.936048\tReg: 0.000061\tFr_p: 0.110037\tFr_r: 0.119818\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.836944\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.055373                \tClf: 2.963465\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118998\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845531\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.792464                \tClf: 2.700126\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119187\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859694\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.106853                \tClf: 3.013807\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118834\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849969\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.918799                \tClf: 2.826239\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119346\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842828\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.134154                \tClf: 3.041951\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119286\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855582\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.153134                \tClf: 3.060294\tReg: 0.000061\tFr_p: 0.109789\tFr_r: 0.119487\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834539\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.881798                \tClf: 2.790009\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.119108\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.840036\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.214883                \tClf: 2.122820\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118888\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.845951\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.992409                \tClf: 2.900050\tReg: 0.000061\tFr_p: 0.383508\tFr_r: 0.414918\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842959\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.939049                \tClf: 2.846840\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118777\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855075\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.878244                \tClf: 2.785429\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118674\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862696\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.160874                \tClf: 3.067678\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118982\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843263\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.857639                \tClf: 2.765415\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118882\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865017\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.893445                \tClf: 2.800133\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119364\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849700\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.983370                \tClf: 2.890824\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119802\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838360\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.028117                \tClf: 2.936138\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118959\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844736\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.822030                \tClf: 2.729733\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119217\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859349\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.114508                \tClf: 3.021480\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118815\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850098\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.979334                \tClf: 2.886768\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119361\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842265\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.156563                \tClf: 3.064389\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119271\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855072\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.125115                \tClf: 3.032300\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119503\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834990\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.834625                \tClf: 2.742814\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.119158\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839517\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.236735                \tClf: 2.144698\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118904\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846270\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.954204                \tClf: 2.861829\tReg: 0.000061\tFr_p: 0.383576\tFr_r: 0.414958\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.842832\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.990080                \tClf: 2.897878\tReg: 0.000061\tFr_p: 0.109581\tFr_r: 0.118789\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855048\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.975467                \tClf: 2.882654\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118707\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863009\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.241591                \tClf: 3.148379\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118995\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842457\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.851405                \tClf: 2.759221\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118877\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865682\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.990456                \tClf: 2.897111\tReg: 0.000061\tFr_p: 0.109855\tFr_r: 0.119370\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850913\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.977189                \tClf: 2.884582\tReg: 0.000061\tFr_p: 0.110048\tFr_r: 0.119836\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.838018\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.066250                \tClf: 2.974288\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118970\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844633\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.833513                \tClf: 2.741220\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119193\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860267\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.132997                \tClf: 3.039922\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118834\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850360\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.929019                \tClf: 2.836440\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119339\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842571\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.142124                \tClf: 3.049934\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119312\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.856020\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.109092                \tClf: 3.016230\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119556\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835517\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.825793                \tClf: 2.733956\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119158\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839590\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.225823                \tClf: 2.133782\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118884\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845967\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.991543                \tClf: 2.899183\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414955\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843078\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.921881                \tClf: 2.829666\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118757\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855134\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.924427                \tClf: 2.831610\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118709\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863046\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.125496                \tClf: 3.032282\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118976\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843027\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.851028                \tClf: 2.758816\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118863\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864551\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.021348                \tClf: 2.928060\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119345\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850335\tlr: 0.001000\ttrain acc:97.7500\tLoss: 3.019891                \tClf: 2.927313\tReg: 0.000061\tFr_p: 0.110038\tFr_r: 0.119816\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837527\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.065523                \tClf: 2.973585\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118956\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844786\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.854212                \tClf: 2.761912\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119202\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.859428\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.118826                \tClf: 3.025794\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118815\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849735\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.959851                \tClf: 2.867303\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119367\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.841769\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.152881                \tClf: 3.060731\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119237\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.856212\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.072373                \tClf: 2.979501\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119509\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835132\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.818080                \tClf: 2.726262\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119162\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839671\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.215432                \tClf: 2.123388\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846195\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.981691                \tClf: 2.889320\tReg: 0.000061\tFr_p: 0.383567\tFr_r: 0.414963\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843669\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.934981                \tClf: 2.842736\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118785\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854913\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.933753                \tClf: 2.840947\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118657\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863177\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.189398                \tClf: 3.096178\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118974\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842286\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.812863                \tClf: 2.720688\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118884\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865218\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.992298                \tClf: 2.898976\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119314\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850298\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.971989                \tClf: 2.879413\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119823\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837537\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.049753                \tClf: 2.957815\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118964\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845674\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.820601                \tClf: 2.728256\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119189\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859950\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.113072                \tClf: 3.020013\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118828\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850666\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.956995                \tClf: 2.864401\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119403\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842894\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.191084                \tClf: 3.098878\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119264\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.856030\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.053397                \tClf: 2.960535\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119531\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835102\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.828570                \tClf: 2.736754\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119173\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839603\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.195113                \tClf: 2.103072\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846008\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.005460                \tClf: 2.913098\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414951\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.842480\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.916807                \tClf: 2.824622\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118764\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854895\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.962288                \tClf: 2.869482\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118710\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862241\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.182418                \tClf: 3.089245\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118994\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842891\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.833684                \tClf: 2.741479\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118882\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865151\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.973030                \tClf: 2.879711\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119349\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850203\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.998760                \tClf: 2.906189\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119812\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837923\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.041653                \tClf: 2.949696\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118996\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844732\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.866555                \tClf: 2.774257\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119207\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860096\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.112492                \tClf: 3.019426\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118835\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849287\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.946348                \tClf: 2.853823\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119350\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843150\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.114231                \tClf: 3.022013\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119258\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855935\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.135906                \tClf: 3.043048\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119507\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835451\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.839221                \tClf: 2.747388\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119178\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839448\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.183666                \tClf: 2.091632\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118903\n",
      "\n",
      "Test set: Average loss: 0.0928, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.845327\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.015346                \tClf: 2.923019\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414914\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.844279\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.904271                \tClf: 2.811996\tReg: 0.000061\tFr_p: 0.109601\tFr_r: 0.118795\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855843\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.948382                \tClf: 2.855529\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118693\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862881\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.177670                \tClf: 3.084464\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119020\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842959\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.848241                \tClf: 2.756032\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118904\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865061\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.937475                \tClf: 2.844161\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119358\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850399\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.992539                \tClf: 2.899958\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119797\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837936\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.002500                \tClf: 2.910542\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118947\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845587\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.848414                \tClf: 2.756073\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119212\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859758\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.139794                \tClf: 3.046745\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118830\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850045\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.937368                \tClf: 2.844805\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119368\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842824\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.187769                \tClf: 3.095567\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119267\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.856049\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.138265                \tClf: 3.045401\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119516\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835343\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.888382                \tClf: 2.796554\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.119185\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839826\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.225805                \tClf: 2.133752\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118913\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846050\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.978637                \tClf: 2.886274\tReg: 0.000061\tFr_p: 0.383551\tFr_r: 0.414962\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842727\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.917950                \tClf: 2.825752\tReg: 0.000061\tFr_p: 0.109567\tFr_r: 0.118752\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854457\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.861012                \tClf: 2.768228\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118710\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863198\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.140341                \tClf: 3.047120\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119018\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842824\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.783610                \tClf: 2.691408\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118902\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865381\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.928448                \tClf: 2.835118\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119338\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850024\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.943568                \tClf: 2.851005\tReg: 0.000061\tFr_p: 0.110046\tFr_r: 0.119824\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.838419\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.952092                \tClf: 2.860110\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118987\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845102\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.787466                \tClf: 2.695150\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119198\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859379\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.079994                \tClf: 2.986964\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118831\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849888\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.920559                \tClf: 2.828004\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119360\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842784\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.215850                \tClf: 3.123649\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.119268\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855800\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.146508                \tClf: 3.053657\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119486\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835578\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.833515                \tClf: 2.741675\tReg: 0.000061\tFr_p: 0.109781\tFr_r: 0.119170\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839764\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.191049                \tClf: 2.098999\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.118936\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846693\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.951328                \tClf: 2.858933\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414951\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842913\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.928946                \tClf: 2.836739\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118786\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854898\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.945278                \tClf: 2.852472\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118717\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863359\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.172666                \tClf: 3.079437\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118971\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843295\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.784474                \tClf: 2.692248\tReg: 0.000061\tFr_p: 0.109665\tFr_r: 0.118895\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.866358\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.969499                \tClf: 2.876120\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119400\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849242\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.988986                \tClf: 2.896463\tReg: 0.000061\tFr_p: 0.110049\tFr_r: 0.119827\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837706\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.001151                \tClf: 2.909204\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118957\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845833\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.813359                \tClf: 2.721006\tReg: 0.000061\tFr_p: 0.109840\tFr_r: 0.119207\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.858868\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.080863                \tClf: 2.987858\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118777\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850343\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.922210                \tClf: 2.829632\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119354\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842645\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.200774                \tClf: 3.108581\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119277\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.854967\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.197232                \tClf: 3.104422\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119533\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835089\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.921269                \tClf: 2.829454\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119160\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839403\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.188328                \tClf: 2.096297\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846063\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.941004                \tClf: 2.848640\tReg: 0.000061\tFr_p: 0.383516\tFr_r: 0.414918\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843168\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.047106                \tClf: 2.954886\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118772\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855126\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.939120                \tClf: 2.846302\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118699\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863334\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.166383                \tClf: 3.073155\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119011\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842264\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.803918                \tClf: 2.711744\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118875\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864540\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.893635                \tClf: 2.800347\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119324\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850305\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.974271                \tClf: 2.881695\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119813\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837664\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.046965                \tClf: 2.955021\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118997\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844994\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.801249                \tClf: 2.708938\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119209\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859969\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.113894                \tClf: 3.020834\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118828\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849727\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.944963                \tClf: 2.852415\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119378\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842535\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.138603                \tClf: 3.046415\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119286\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855632\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.082147                \tClf: 2.989304\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119522\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835545\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.903719                \tClf: 2.811881\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119161\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839114\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.163980                \tClf: 2.071963\tReg: 0.000061\tFr_p: 0.109784\tFr_r: 0.118938\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846838\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.014347                \tClf: 2.921944\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414958\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843821\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.941114                \tClf: 2.848862\tReg: 0.000061\tFr_p: 0.109603\tFr_r: 0.118780\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854982\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.984427                \tClf: 2.891617\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118703\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863392\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.232567                \tClf: 3.139337\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119018\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843250\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.869117                \tClf: 2.776894\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118893\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865164\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.895645                \tClf: 2.802326\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119373\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850029\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.957447                \tClf: 2.864884\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119808\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838286\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.992776                \tClf: 2.900801\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118970\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844949\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.830162                \tClf: 2.737854\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119191\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859957\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.091932                \tClf: 2.998873\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118815\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850243\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.914901                \tClf: 2.822327\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119372\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842993\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.198619                \tClf: 3.106408\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119260\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.856440\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.123483                \tClf: 3.030600\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119500\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834960\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.918825                \tClf: 2.827016\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119172\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839610\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.190006                \tClf: 2.097964\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118928\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846142\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.019135                \tClf: 2.926767\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414963\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842526\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.917050                \tClf: 2.824863\tReg: 0.000061\tFr_p: 0.109563\tFr_r: 0.118754\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854840\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.949707                \tClf: 2.856904\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118726\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862176\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.157094                \tClf: 3.063924\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119017\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843167\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.801999                \tClf: 2.709780\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118881\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865119\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.960870                \tClf: 2.867553\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119350\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850059\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.969175                \tClf: 2.876611\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119804\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837572\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.016380                \tClf: 2.924441\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118972\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844959\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.807549                \tClf: 2.715240\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119180\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860298\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.192355                \tClf: 3.099279\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118809\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850188\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.956358                \tClf: 2.863788\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119363\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842762\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.165147                \tClf: 3.072948\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119271\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855804\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.132290                \tClf: 3.039438\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119541\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835390\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.812366                \tClf: 2.720536\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.119152\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839094\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.207209                \tClf: 2.115193\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0975, Accuracy: 9708/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845553\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.014404                \tClf: 2.922065\tReg: 0.000061\tFr_p: 0.383526\tFr_r: 0.414931\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842929\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.973237                \tClf: 2.881029\tReg: 0.000061\tFr_p: 0.109607\tFr_r: 0.118795\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854993\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.977352                \tClf: 2.884541\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118705\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862842\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.255239                \tClf: 3.162036\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118987\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842618\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.840376                \tClf: 2.748184\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118907\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864647\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.967724                \tClf: 2.874431\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119364\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850540\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.998398                \tClf: 2.905810\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119783\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837591\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.004602                \tClf: 2.912661\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118991\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845004\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.814912                \tClf: 2.722601\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119176\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859725\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.111742                \tClf: 3.018694\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118820\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849711\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.949342                \tClf: 2.856795\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119352\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842914\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.166720                \tClf: 3.074513\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.119250\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855827\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.047830                \tClf: 2.954978\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119495\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835183\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.880762                \tClf: 2.788942\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119138\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839852\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.211488                \tClf: 2.119434\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118892\n",
      "\n",
      "Test set: Average loss: 0.0931, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846076\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.935325                \tClf: 2.842960\tReg: 0.000061\tFr_p: 0.383563\tFr_r: 0.414934\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842262\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.948676                \tClf: 2.856502\tReg: 0.000061\tFr_p: 0.109581\tFr_r: 0.118769\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855162\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.963326                \tClf: 2.870507\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118687\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862710\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.166764                \tClf: 3.073568\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118951\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842700\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.804479                \tClf: 2.712283\tReg: 0.000061\tFr_p: 0.109650\tFr_r: 0.118863\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864994\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.997644                \tClf: 2.904333\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119367\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850441\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.992308                \tClf: 2.899725\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119847\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837647\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.030918                \tClf: 2.938974\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118978\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844663\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.776101                \tClf: 2.683807\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119195\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860333\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.116357                \tClf: 3.023279\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118824\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850701\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.858118                \tClf: 2.765522\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119346\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842801\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.203785                \tClf: 3.111583\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119293\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855536\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.123957                \tClf: 3.031119\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119501\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835280\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.826492                \tClf: 2.734667\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119188\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.840208\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.208503                \tClf: 2.116432\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118919\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845823\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.999365                \tClf: 2.907013\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414953\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843273\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.937506                \tClf: 2.845281\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118776\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.855404\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.947393                \tClf: 2.854562\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118710\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862732\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.231471                \tClf: 3.138274\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118983\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843344\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.850839                \tClf: 2.758610\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118882\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865010\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.976527                \tClf: 2.883215\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119335\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849722\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.956912                \tClf: 2.864365\tReg: 0.000061\tFr_p: 0.110038\tFr_r: 0.119807\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837852\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.012157                \tClf: 2.920203\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118989\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844927\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.817131                \tClf: 2.724823\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119190\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860210\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.069581                \tClf: 2.976509\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118842\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850410\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.897884                \tClf: 2.805302\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119390\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842714\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.181616                \tClf: 3.089419\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119246\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855398\tlr: 0.001000\ttrain acc:97.7750\tLoss: 3.062863                \tClf: 2.970032\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119497\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834943\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.917922                \tClf: 2.826114\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119170\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839873\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.206269                \tClf: 2.114215\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118918\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846493\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.968035                \tClf: 2.875650\tReg: 0.000061\tFr_p: 0.383510\tFr_r: 0.414920\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842779\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.005702                \tClf: 2.913502\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118773\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854735\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.904152                \tClf: 2.811355\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118696\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862694\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.172813                \tClf: 3.079618\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118971\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842960\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.902463                \tClf: 2.810254\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118899\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864859\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.909908                \tClf: 2.816604\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119335\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.849489\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.967879                \tClf: 2.875344\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119812\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837645\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.035146                \tClf: 2.943203\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118950\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844901\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.876949                \tClf: 2.784643\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119193\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860422\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.124626                \tClf: 3.031544\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118841\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850594\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.880023                \tClf: 2.787432\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119344\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.843323\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.210980                \tClf: 3.118753\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.119331\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855534\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.156559                \tClf: 3.063721\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119511\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834937\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.890950                \tClf: 2.799143\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119170\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839656\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.160313                \tClf: 2.068269\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118918\n",
      "\n",
      "Test set: Average loss: 0.0957, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846564\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.012525                \tClf: 2.920136\tReg: 0.000061\tFr_p: 0.383511\tFr_r: 0.414919\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843998\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.937662                \tClf: 2.845401\tReg: 0.000061\tFr_p: 0.109568\tFr_r: 0.118759\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855392\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.905011                \tClf: 2.812180\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118725\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863263\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.259210                \tClf: 3.165986\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118973\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842649\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.847982                \tClf: 2.755788\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118891\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865695\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.940495                \tClf: 2.847149\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119365\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850259\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.920380                \tClf: 2.827806\tReg: 0.000061\tFr_p: 0.110009\tFr_r: 0.119801\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837167\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.989577                \tClf: 2.897657\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118976\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844535\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.830362                \tClf: 2.738074\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119204\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859663\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.093932                \tClf: 3.000888\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118812\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850289\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.919742                \tClf: 2.827166\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119360\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842963\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.231460                \tClf: 3.139251\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119297\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.856490\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.185401                \tClf: 3.092515\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119505\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835095\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.831478                \tClf: 2.739662\tReg: 0.000061\tFr_p: 0.109779\tFr_r: 0.119181\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839604\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.186238                \tClf: 2.094197\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118905\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846323\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.012361                \tClf: 2.919984\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414963\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843525\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.945888                \tClf: 2.853650\tReg: 0.000061\tFr_p: 0.109597\tFr_r: 0.118798\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854886\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.007041                \tClf: 2.914236\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118727\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863314\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.244087                \tClf: 3.150860\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118997\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842960\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.792324                \tClf: 2.700115\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118906\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865314\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.963421                \tClf: 2.870094\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119376\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849660\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.900315                \tClf: 2.807771\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119808\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.838511\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.035817                \tClf: 2.943830\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118988\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845563\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.828979                \tClf: 2.736639\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119222\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860077\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.061249                \tClf: 2.968184\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118846\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850423\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.988050                \tClf: 2.895468\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119380\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843327\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.218533                \tClf: 3.126305\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119296\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855688\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.145033                \tClf: 3.052188\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119530\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835080\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.837086                \tClf: 2.745271\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119165\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839643\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.267627                \tClf: 2.175584\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.118919\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846198\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.005576                \tClf: 2.913205\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414954\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843604\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.957679                \tClf: 2.865437\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118782\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854758\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.907708                \tClf: 2.814909\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118691\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863008\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.220648                \tClf: 3.127436\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119003\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842670\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.803373                \tClf: 2.711179\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118887\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864890\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.969404                \tClf: 2.876098\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119355\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849369\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.988006                \tClf: 2.895476\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119814\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837512\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.024061                \tClf: 2.932124\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118971\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845185\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.811623                \tClf: 2.719303\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119207\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860073\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.108404                \tClf: 3.015340\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118825\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849971\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.945922                \tClf: 2.853362\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119362\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842299\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.165380                \tClf: 3.073204\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119270\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855661\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.102164                \tClf: 3.009320\tReg: 0.000061\tFr_p: 0.109793\tFr_r: 0.119505\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835060\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.820984                \tClf: 2.729170\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119151\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839619\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.234142                \tClf: 2.142100\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118903\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846626\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.976491                \tClf: 2.884098\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414964\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842896\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.005100                \tClf: 2.912894\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118772\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854742\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.849579                \tClf: 2.756781\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118690\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862990\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.212847                \tClf: 3.119637\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118986\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842946\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.855233                \tClf: 2.763025\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118880\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865267\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.941780                \tClf: 2.848455\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119362\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850051\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.948676                \tClf: 2.856112\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119787\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838022\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.018117                \tClf: 2.926155\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118994\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844886\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.825983                \tClf: 2.733678\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119202\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860174\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.086345                \tClf: 2.993275\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118806\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849382\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.965377                \tClf: 2.872847\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119377\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.841843\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.170681                \tClf: 3.078528\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119274\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855713\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.070030                \tClf: 2.977184\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119521\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835642\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.843154                \tClf: 2.751311\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119179\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839578\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.200286                \tClf: 2.108246\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9717/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846411\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.023407                \tClf: 2.931026\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414906\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843056\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.955478                \tClf: 2.863264\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118762\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854725\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.995588                \tClf: 2.902791\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118702\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863064\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.180268                \tClf: 3.087054\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118996\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843254\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.848382                \tClf: 2.756158\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118904\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864507\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.954483                \tClf: 2.861196\tReg: 0.000061\tFr_p: 0.109846\tFr_r: 0.119328\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850539\tlr: 0.001000\ttrain acc:97.1250\tLoss: 2.940967                \tClf: 2.848379\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119813\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837410\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.036077                \tClf: 2.944145\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118944\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845207\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.787423                \tClf: 2.695101\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119208\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860591\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.134376                \tClf: 3.041285\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118828\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850129\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.905328                \tClf: 2.812760\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119356\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842392\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.240851                \tClf: 3.148671\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119286\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855505\tlr: 0.001000\ttrain acc:97.8750\tLoss: 3.051332                \tClf: 2.958495\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119527\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835399\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.852318                \tClf: 2.760487\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119176\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839695\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.197909                \tClf: 2.105863\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846086\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.935662                \tClf: 2.843296\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414958\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843029\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.940987                \tClf: 2.848774\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118781\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854567\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.993173                \tClf: 2.900383\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118694\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862635\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.156543                \tClf: 3.063350\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118980\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842574\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.840273                \tClf: 2.748083\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118884\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865013\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.938431                \tClf: 2.845119\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119374\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850275\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.941137                \tClf: 2.848563\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119818\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837524\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.074881                \tClf: 2.982943\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118968\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845104\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.790233                \tClf: 2.697917\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119176\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859901\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.105791                \tClf: 3.012735\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118809\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850186\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.976630                \tClf: 2.884060\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119364\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842204\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.266456                \tClf: 3.174285\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119293\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855433\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.149909                \tClf: 3.057076\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119508\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835252\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.876616                \tClf: 2.784792\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.119166\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840148\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.189454                \tClf: 2.097386\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0933, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846419\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.021392                \tClf: 2.929010\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414963\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843312\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.023911                \tClf: 2.931684\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118789\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854323\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.945587                \tClf: 2.852809\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118728\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862838\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.209253                \tClf: 3.116050\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118975\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843374\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.806182                \tClf: 2.713952\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118879\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865671\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.975989                \tClf: 2.882644\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119362\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850275\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.929825                \tClf: 2.837250\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119818\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837361\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.974603                \tClf: 2.882674\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118955\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845396\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.849194                \tClf: 2.756863\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119185\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860046\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.100535                \tClf: 3.007472\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118788\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850196\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.911376                \tClf: 2.818805\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119357\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842229\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.222802                \tClf: 3.130629\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.119235\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855581\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.094090                \tClf: 3.001250\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119506\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835047\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.923351                \tClf: 2.831538\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.119158\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.840399\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.207986                \tClf: 2.115905\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846128\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.971940                \tClf: 2.879572\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414949\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843275\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.967603                \tClf: 2.875379\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118747\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854862\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.930771                \tClf: 2.837966\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118701\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862619\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.194660                \tClf: 3.101468\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118990\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842485\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.885374                \tClf: 2.793189\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118899\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865020\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.985271                \tClf: 2.891959\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119338\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849660\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.985475                \tClf: 2.892931\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119813\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837957\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.060046                \tClf: 2.968087\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118948\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844862\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.862543                \tClf: 2.770238\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119202\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860278\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.081401                \tClf: 2.988326\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118807\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849804\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.972299                \tClf: 2.879747\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119346\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842412\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.181116                \tClf: 3.088934\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119254\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855108\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.148846                \tClf: 3.056029\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119528\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835885\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.909557                \tClf: 2.817701\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119193\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839366\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.175145                \tClf: 2.083115\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118923\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845677\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.961420                \tClf: 2.869076\tReg: 0.000061\tFr_p: 0.383562\tFr_r: 0.414934\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843143\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.988453                \tClf: 2.896234\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118773\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854349\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.893658                \tClf: 2.800880\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118713\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862652\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.149919                \tClf: 3.056726\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118971\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843283\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.868908                \tClf: 2.776683\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118884\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864553\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.962440                \tClf: 2.869151\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119364\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850171\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.961669                \tClf: 2.869099\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119809\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838110\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.976792                \tClf: 2.884826\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118980\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845426\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.891723                \tClf: 2.799390\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119199\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859627\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.095640                \tClf: 3.002597\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118825\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850831\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.914205                \tClf: 2.821603\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119368\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842696\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.218057                \tClf: 3.125861\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119302\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855152\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.099846                \tClf: 3.007028\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119505\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835073\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.879866                \tClf: 2.788051\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119176\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.838795\tlr: 0.001000\ttrain acc:98.0250\tLoss: 2.206717                \tClf: 2.114716\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846653\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.021156                \tClf: 2.928762\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414913\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842656\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.963337                \tClf: 2.871143\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118777\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854325\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.963093                \tClf: 2.870316\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118690\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863246\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.177400                \tClf: 3.084177\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118974\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843261\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.871156                \tClf: 2.778932\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118895\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865173\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.972418                \tClf: 2.879098\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119338\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850584\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.002997                \tClf: 2.910406\tReg: 0.000061\tFr_p: 0.110049\tFr_r: 0.119839\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838144\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.057244                \tClf: 2.965275\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118981\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845466\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.862510                \tClf: 2.770176\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119208\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860229\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.129482                \tClf: 3.036409\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118852\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850046\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.922183                \tClf: 2.829620\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119335\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842694\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.202015                \tClf: 3.109819\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119259\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855906\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.152999                \tClf: 3.060142\tReg: 0.000061\tFr_p: 0.109786\tFr_r: 0.119513\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835968\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.879640                \tClf: 2.787780\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119144\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839005\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.204118                \tClf: 2.112107\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118907\n",
      "\n",
      "Test set: Average loss: 0.0932, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846112\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.008631                \tClf: 2.916265\tReg: 0.000061\tFr_p: 0.383523\tFr_r: 0.414927\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843284\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.931132                \tClf: 2.838906\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118778\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854937\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.924654                \tClf: 2.831846\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118713\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863070\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.217324                \tClf: 3.124110\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118983\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842926\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.849528                \tClf: 2.757321\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118881\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864926\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.995791                \tClf: 2.902483\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119351\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850414\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.947962                \tClf: 2.855380\tReg: 0.000061\tFr_p: 0.110034\tFr_r: 0.119805\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837860\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.038012                \tClf: 2.946058\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118979\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845091\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.809959                \tClf: 2.717644\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119201\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860643\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.067329                \tClf: 2.974236\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118812\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850608\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.998112                \tClf: 2.905521\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119359\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842699\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.171248                \tClf: 3.079051\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119259\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855387\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.118600                \tClf: 3.025769\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119517\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835093\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.901080                \tClf: 2.809265\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119175\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839405\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.164911                \tClf: 2.072879\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0974, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846289\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.997067                \tClf: 2.904692\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414948\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842627\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.988750                \tClf: 2.896557\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118797\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.853585\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.932879                \tClf: 2.840138\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118681\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863267\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.100133                \tClf: 3.006909\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119015\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842788\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.831357                \tClf: 2.739157\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118912\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865661\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.866073                \tClf: 2.772728\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119358\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.849518\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.943686                \tClf: 2.851149\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119804\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837981\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.010096                \tClf: 2.918136\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118963\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845297\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.806868                \tClf: 2.714542\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119222\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859755\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.102123                \tClf: 3.009074\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118795\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.851120\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.942877                \tClf: 2.850260\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119373\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842365\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.210434                \tClf: 3.118254\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119285\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856156\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.118904                \tClf: 3.026035\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119519\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835032\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.894209                \tClf: 2.802396\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119154\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839575\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.206025                \tClf: 2.113985\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845398\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.972929                \tClf: 2.880598\tReg: 0.000061\tFr_p: 0.383553\tFr_r: 0.414952\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843082\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.938333                \tClf: 2.846118\tReg: 0.000061\tFr_p: 0.109609\tFr_r: 0.118795\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855318\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.882346                \tClf: 2.789519\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118702\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863360\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.237851                \tClf: 3.144622\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119005\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842558\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.783164                \tClf: 2.690975\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118887\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865247\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.909767                \tClf: 2.816443\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119355\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850983\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.998249                \tClf: 2.905639\tReg: 0.000061\tFr_p: 0.110021\tFr_r: 0.119788\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838041\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.966747                \tClf: 2.874784\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118983\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845398\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.770195                \tClf: 2.677864\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119195\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859480\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.109036                \tClf: 3.016000\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118802\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850028\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.954683                \tClf: 2.862121\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119339\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842857\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.226426                \tClf: 3.134222\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119283\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855912\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.141730                \tClf: 3.048874\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119534\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835197\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.858086                \tClf: 2.766265\tReg: 0.000061\tFr_p: 0.109778\tFr_r: 0.119204\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.840056\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.186663                \tClf: 2.094599\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118881\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846317\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.977960                \tClf: 2.885583\tReg: 0.000061\tFr_p: 0.383512\tFr_r: 0.414943\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843717\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.999849                \tClf: 2.907602\tReg: 0.000061\tFr_p: 0.109570\tFr_r: 0.118755\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.855704\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.921932                \tClf: 2.829085\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118707\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862426\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.180422                \tClf: 3.087240\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118992\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843102\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.860631                \tClf: 2.768414\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118893\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864682\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.978824                \tClf: 2.885529\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119358\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850157\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.977187                \tClf: 2.884618\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119783\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837994\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.995726                \tClf: 2.903766\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118972\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844934\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.802464                \tClf: 2.710157\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119209\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860706\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.081665                \tClf: 2.988568\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118814\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850392\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.893719                \tClf: 2.801138\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119355\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843061\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.198956                \tClf: 3.106741\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119280\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855396\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.096953                \tClf: 3.004122\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119480\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835252\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.805664                \tClf: 2.713840\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119181\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839297\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.224931                \tClf: 2.132905\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118886\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846029\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.979314                \tClf: 2.886952\tReg: 0.000061\tFr_p: 0.383506\tFr_r: 0.414915\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842555\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.933887                \tClf: 2.841698\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118755\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855092\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.967418                \tClf: 2.874602\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118728\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863402\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.165543                \tClf: 3.072312\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118985\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842987\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.814902                \tClf: 2.722691\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118887\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864967\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.976262                \tClf: 2.882953\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119369\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850348\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.958412                \tClf: 2.865834\tReg: 0.000061\tFr_p: 0.110034\tFr_r: 0.119825\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837377\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.957713                \tClf: 2.865783\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118982\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844848\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.849493                \tClf: 2.757190\tReg: 0.000061\tFr_p: 0.109837\tFr_r: 0.119195\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859735\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.116665                \tClf: 3.023617\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118798\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850501\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.939086                \tClf: 2.846500\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119362\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.843226\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.208739                \tClf: 3.116516\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119279\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855434\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.166078                \tClf: 3.073245\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119518\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834649\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.803856                \tClf: 2.712062\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119132\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839835\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.196224                \tClf: 2.104171\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.845923\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.911257                \tClf: 2.818900\tReg: 0.000061\tFr_p: 0.383530\tFr_r: 0.414957\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843434\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.967388                \tClf: 2.875155\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118764\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855328\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.927178                \tClf: 2.834350\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118728\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862495\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.210616                \tClf: 3.117430\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118984\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843060\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.812465                \tClf: 2.720251\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118916\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864874\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.899638                \tClf: 2.806333\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119351\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849804\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.963205                \tClf: 2.870654\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119803\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838240\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.996274                \tClf: 2.904300\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119000\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844521\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.834392                \tClf: 2.742105\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119203\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859953\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.114551                \tClf: 3.021492\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118803\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850346\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.921872                \tClf: 2.829294\tReg: 0.000061\tFr_p: 0.109900\tFr_r: 0.119364\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842971\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.200864                \tClf: 3.108654\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119259\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855690\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.111971                \tClf: 3.019126\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119506\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835295\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.862288                \tClf: 2.770463\tReg: 0.000061\tFr_p: 0.109779\tFr_r: 0.119170\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839727\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.191785                \tClf: 2.099738\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846241\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.998275                \tClf: 2.905901\tReg: 0.000061\tFr_p: 0.383527\tFr_r: 0.414944\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843659\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.994999                \tClf: 2.902755\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118792\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854259\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.990430                \tClf: 2.897656\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118689\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863049\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.227314                \tClf: 3.134101\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119001\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842816\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.811241                \tClf: 2.719039\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118888\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865142\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.998315                \tClf: 2.904997\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119353\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850006\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.012544                \tClf: 2.919982\tReg: 0.000061\tFr_p: 0.110024\tFr_r: 0.119810\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837137\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.975629                \tClf: 2.883711\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118954\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844935\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.800789                \tClf: 2.708481\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119207\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859641\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.125899                \tClf: 3.032856\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118837\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850021\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.867636                \tClf: 2.775074\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119341\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843065\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.209921                \tClf: 3.117707\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119272\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855975\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.094872                \tClf: 3.002013\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119553\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835979\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.892310                \tClf: 2.800450\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119169\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839349\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.222542                \tClf: 2.130513\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846108\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.992185                \tClf: 2.899819\tReg: 0.000061\tFr_p: 0.383560\tFr_r: 0.414969\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.842571\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.962332                \tClf: 2.870143\tReg: 0.000061\tFr_p: 0.109569\tFr_r: 0.118779\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855167\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.890060                \tClf: 2.797241\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118714\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863511\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.155883                \tClf: 3.062647\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119024\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843266\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.798728                \tClf: 2.706503\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118918\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864748\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.872717                \tClf: 2.779418\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119362\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850745\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.918035                \tClf: 2.825437\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119840\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837302\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.033444                \tClf: 2.941518\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118954\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844925\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.818616                \tClf: 2.726309\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119198\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860309\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.102298                \tClf: 3.009222\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118839\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850412\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.897036                \tClf: 2.804455\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119355\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842965\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.191585                \tClf: 3.099376\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.119284\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.856054\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.099860                \tClf: 3.006997\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119524\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834945\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.878728                \tClf: 2.786919\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119152\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839383\tlr: 0.001000\ttrain acc:98.4750\tLoss: 2.214524                \tClf: 2.122494\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846104\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.952013                \tClf: 2.859647\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414963\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843422\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.919225                \tClf: 2.826993\tReg: 0.000061\tFr_p: 0.109604\tFr_r: 0.118790\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855011\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.962151                \tClf: 2.869340\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118685\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863198\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.221192                \tClf: 3.127970\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.119027\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843031\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.820993                \tClf: 2.728781\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118878\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864682\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.935610                \tClf: 2.842315\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119359\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850119\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.887385                \tClf: 2.794818\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119799\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837783\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.017605                \tClf: 2.925654\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118991\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844932\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.806851                \tClf: 2.714543\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119179\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859687\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.078381                \tClf: 2.985336\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118846\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849393\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.916661                \tClf: 2.824130\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119351\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842719\tlr: 0.001000\ttrain acc:96.8250\tLoss: 3.177266                \tClf: 3.085069\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119280\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.854843\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.118405                \tClf: 3.025601\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119477\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.834925\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.850794                \tClf: 2.758987\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119155\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839239\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.229962                \tClf: 2.137939\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118906\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845139\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.983607                \tClf: 2.891289\tReg: 0.000061\tFr_p: 0.383554\tFr_r: 0.414927\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843398\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.958203                \tClf: 2.865972\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118809\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855813\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.891394                \tClf: 2.798542\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118730\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863031\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.181588                \tClf: 3.088375\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118980\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842294\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.905417                \tClf: 2.813241\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118875\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865453\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.968332                \tClf: 2.874998\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119375\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849688\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.041693                \tClf: 2.949147\tReg: 0.000061\tFr_p: 0.110013\tFr_r: 0.119792\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.838443\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.021417                \tClf: 2.929434\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118955\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844557\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.822413                \tClf: 2.730124\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119188\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860197\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.083670                \tClf: 2.990599\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118831\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849892\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.935594                \tClf: 2.843039\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119368\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842423\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.166987                \tClf: 3.074805\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119262\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855168\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.109253                \tClf: 3.016433\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119507\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835498\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.851666                \tClf: 2.759830\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119172\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839291\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.199464                \tClf: 2.107438\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846437\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.966536                \tClf: 2.874153\tReg: 0.000061\tFr_p: 0.383546\tFr_r: 0.414924\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.842899\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.909041                \tClf: 2.816835\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118765\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855018\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.966683                \tClf: 2.873871\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118686\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863208\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.210007                \tClf: 3.116785\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118966\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843605\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.884427                \tClf: 2.792186\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118873\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864799\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.936500                \tClf: 2.843199\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119325\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849896\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.008928                \tClf: 2.916372\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119835\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837862\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.030307                \tClf: 2.938352\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118958\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844955\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.888647                \tClf: 2.796338\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119183\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859786\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.103009                \tClf: 3.009959\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118819\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.851133\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.928173                \tClf: 2.835556\tReg: 0.000061\tFr_p: 0.109893\tFr_r: 0.119379\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842513\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.192070                \tClf: 3.099884\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.119258\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855729\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.109297                \tClf: 3.016449\tReg: 0.000061\tFr_p: 0.109795\tFr_r: 0.119488\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835731\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.830487                \tClf: 2.738640\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119157\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.838815\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.216896                \tClf: 2.124894\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118903\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846304\tlr: 0.001000\ttrain acc:97.7250\tLoss: 3.014269                \tClf: 2.921893\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414948\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842813\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.028326                \tClf: 2.936124\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118773\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855351\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.990399                \tClf: 2.897571\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118730\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862641\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.248952                \tClf: 3.155759\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118979\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843360\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.875688                \tClf: 2.783459\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118903\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865165\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.946347                \tClf: 2.853027\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119331\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849766\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.971640                \tClf: 2.879090\tReg: 0.000061\tFr_p: 0.110024\tFr_r: 0.119797\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837504\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.029256                \tClf: 2.937320\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118939\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845090\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.828482                \tClf: 2.736167\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119179\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859768\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.079752                \tClf: 2.986703\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118830\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850975\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.907064                \tClf: 2.814454\tReg: 0.000061\tFr_p: 0.109914\tFr_r: 0.119409\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842754\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.213203                \tClf: 3.121004\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119264\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855354\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.098081                \tClf: 3.005252\tReg: 0.000061\tFr_p: 0.109791\tFr_r: 0.119477\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834694\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.796637                \tClf: 2.704842\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119162\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839796\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.209802                \tClf: 2.117751\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0931, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845711\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.963395                \tClf: 2.871048\tReg: 0.000061\tFr_p: 0.383564\tFr_r: 0.414921\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843208\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.901454                \tClf: 2.809232\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118795\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854415\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.920001                \tClf: 2.827219\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118706\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863256\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.147107                \tClf: 3.053883\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118985\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843331\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.858138                \tClf: 2.765910\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118894\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864810\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.964260                \tClf: 2.870958\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119321\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850400\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.911354                \tClf: 2.818773\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119827\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.838435\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.049375                \tClf: 2.957392\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118936\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845223\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.801997                \tClf: 2.709675\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119189\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859922\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.039258                \tClf: 2.946201\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118803\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849710\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.926604                \tClf: 2.834057\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119391\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842553\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.178413                \tClf: 3.086224\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119270\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855352\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.097979                \tClf: 3.005150\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119502\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835004\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.888173                \tClf: 2.796361\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119162\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839538\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.231046                \tClf: 2.139008\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118892\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846017\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.925402                \tClf: 2.833040\tReg: 0.000061\tFr_p: 0.383548\tFr_r: 0.414896\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843081\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.946639                \tClf: 2.854424\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118783\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854667\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.976151                \tClf: 2.883356\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118719\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863397\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.173101                \tClf: 3.079870\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.119005\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842597\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.851006                \tClf: 2.758815\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118876\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865159\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.001050                \tClf: 2.907731\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119385\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850543\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.920591                \tClf: 2.828003\tReg: 0.000061\tFr_p: 0.110021\tFr_r: 0.119816\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838960\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.062757                \tClf: 2.970748\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118968\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845533\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.797092                \tClf: 2.704754\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119205\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860129\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.149830                \tClf: 3.056763\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118828\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850279\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.951631                \tClf: 2.859056\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119354\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842746\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.230313                \tClf: 3.138115\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.119308\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.856354\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.111967                \tClf: 3.019088\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119519\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835014\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.859409                \tClf: 2.767598\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.119150\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839408\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.216535                \tClf: 2.124503\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118889\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846185\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.983282                \tClf: 2.890912\tReg: 0.000061\tFr_p: 0.383511\tFr_r: 0.414941\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843294\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.968563                \tClf: 2.876337\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118773\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855135\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.931784                \tClf: 2.838966\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118719\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863426\tlr: 0.001000\ttrain acc:96.7750\tLoss: 3.200296                \tClf: 3.107063\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118999\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843240\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.842999                \tClf: 2.750776\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118871\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865072\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.951602                \tClf: 2.858287\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119376\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850019\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.013326                \tClf: 2.920764\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119815\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838365\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.948676                \tClf: 2.856697\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118971\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844564\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.859721                \tClf: 2.767432\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119196\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860214\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.107743                \tClf: 3.014671\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118827\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849843\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.885168                \tClf: 2.792615\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119351\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842412\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.211264                \tClf: 3.119082\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119270\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855747\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.074796                \tClf: 2.981948\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119543\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835337\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.831156                \tClf: 2.739328\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119170\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.840036\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.197730                \tClf: 2.105667\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.118921\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845899\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.923730                \tClf: 2.831374\tReg: 0.000061\tFr_p: 0.383554\tFr_r: 0.414967\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843216\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.990329                \tClf: 2.898107\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118763\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854937\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.890993                \tClf: 2.798185\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118711\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862593\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.145541                \tClf: 3.052350\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118977\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842998\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.806818                \tClf: 2.714607\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118874\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864933\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.956277                \tClf: 2.862969\tReg: 0.000061\tFr_p: 0.109848\tFr_r: 0.119328\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.849733\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.932051                \tClf: 2.839503\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119806\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838535\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.001944                \tClf: 2.909956\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118949\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844662\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.836501                \tClf: 2.744207\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119196\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859989\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.143162                \tClf: 3.050102\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118807\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850284\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.963870                \tClf: 2.871294\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119355\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842559\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.160068                \tClf: 3.067879\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119281\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.856044\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.082285                \tClf: 2.989422\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119515\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835233\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.867107                \tClf: 2.775284\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119188\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839742\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.222094                \tClf: 2.130046\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845783\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.967478                \tClf: 2.875128\tReg: 0.000061\tFr_p: 0.383498\tFr_r: 0.414904\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843226\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.025728                \tClf: 2.933506\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118787\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855465\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.951792                \tClf: 2.858957\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118708\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863602\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.256071                \tClf: 3.162830\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119022\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843574\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.839575                \tClf: 2.747335\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118895\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864506\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.965467                \tClf: 2.872180\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119347\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850211\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.980336                \tClf: 2.887765\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119807\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838468\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.068343                \tClf: 2.976358\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118978\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845790\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.763257                \tClf: 2.670907\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119200\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860658\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.084067                \tClf: 2.990973\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118843\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850031\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.937001                \tClf: 2.844438\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119339\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842243\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.187614                \tClf: 3.095440\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.119282\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855765\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.094498                \tClf: 3.001648\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119541\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835158\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.853905                \tClf: 2.762086\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119156\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839739\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.218462                \tClf: 2.126414\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846862\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.960721                \tClf: 2.868317\tReg: 0.000061\tFr_p: 0.383557\tFr_r: 0.414939\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842958\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.932085                \tClf: 2.839876\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118789\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855151\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.924811                \tClf: 2.831992\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118719\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862827\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.214021                \tClf: 3.120819\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118986\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842162\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.859631                \tClf: 2.767462\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118849\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865568\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.983729                \tClf: 2.890389\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119359\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850100\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.951704                \tClf: 2.859138\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119794\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837788\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.037565                \tClf: 2.945614\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118965\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845673\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.827424                \tClf: 2.735079\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119187\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859920\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.114263                \tClf: 3.021206\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118801\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849834\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.939704                \tClf: 2.847151\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119371\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842596\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.213386                \tClf: 3.121195\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.119251\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855921\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.142746                \tClf: 3.049889\tReg: 0.000061\tFr_p: 0.109793\tFr_r: 0.119507\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835045\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.838369                \tClf: 2.746556\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.119139\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839332\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.221646                \tClf: 2.129619\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118862\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846589\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.006670                \tClf: 2.914279\tReg: 0.000061\tFr_p: 0.383539\tFr_r: 0.414964\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843450\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.908474                \tClf: 2.816241\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118800\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854324\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.956041                \tClf: 2.863263\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118684\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863064\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.221442                \tClf: 3.128228\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118996\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843460\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.866710                \tClf: 2.774476\tReg: 0.000061\tFr_p: 0.109664\tFr_r: 0.118897\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865516\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.946898                \tClf: 2.853561\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119355\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849968\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.980542                \tClf: 2.887982\tReg: 0.000061\tFr_p: 0.110010\tFr_r: 0.119824\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838116\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.058540                \tClf: 2.966573\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118967\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844396\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.782519                \tClf: 2.690238\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119186\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859951\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.123090                \tClf: 3.030032\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118816\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850127\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.950006                \tClf: 2.857438\tReg: 0.000061\tFr_p: 0.109905\tFr_r: 0.119366\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843065\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.209284                \tClf: 3.117070\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119274\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855161\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.073588                \tClf: 2.980769\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.119469\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835673\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.801595                \tClf: 2.709750\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119176\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839571\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.225141                \tClf: 2.133102\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846871\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.967714                \tClf: 2.875309\tReg: 0.000061\tFr_p: 0.383530\tFr_r: 0.414948\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843044\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.910363                \tClf: 2.818150\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118763\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854879\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.948659                \tClf: 2.855854\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118722\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862428\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.201725                \tClf: 3.108543\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119020\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842752\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.872704                \tClf: 2.780505\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118891\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864688\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.987799                \tClf: 2.894503\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119345\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850504\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.907418                \tClf: 2.814832\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119836\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837618\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.035276                \tClf: 2.943334\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118959\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845670\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.840568                \tClf: 2.748223\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119181\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860095\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.077849                \tClf: 2.984784\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118801\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850638\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.878603                \tClf: 2.786010\tReg: 0.000061\tFr_p: 0.109893\tFr_r: 0.119375\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842568\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.141810                \tClf: 3.049621\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119259\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.856026\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.119181                \tClf: 3.026318\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119524\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835268\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.925210                \tClf: 2.833386\tReg: 0.000061\tFr_p: 0.109788\tFr_r: 0.119165\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839825\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.236845                \tClf: 2.144792\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.118920\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846130\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.924060                \tClf: 2.831692\tReg: 0.000061\tFr_p: 0.383520\tFr_r: 0.414900\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842898\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.950712                \tClf: 2.858506\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118794\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854794\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.884738                \tClf: 2.791937\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118700\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862566\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.234402                \tClf: 3.141213\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118987\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843129\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.833791                \tClf: 2.741573\tReg: 0.000061\tFr_p: 0.109671\tFr_r: 0.118874\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864540\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.911099                \tClf: 2.817811\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119378\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849719\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.943871                \tClf: 2.851324\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119808\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837676\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.974025                \tClf: 2.882080\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118978\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845233\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.755860                \tClf: 2.663538\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119185\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860397\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.114638                \tClf: 3.021557\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118796\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849626\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.989184                \tClf: 2.896642\tReg: 0.000061\tFr_p: 0.109847\tFr_r: 0.119332\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842948\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.179472                \tClf: 3.087263\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119255\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855055\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.076096                \tClf: 2.983282\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119488\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834769\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.785263                \tClf: 2.693463\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119153\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839574\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.224970                \tClf: 2.132930\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0932, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846188\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.980209                \tClf: 2.887838\tReg: 0.000061\tFr_p: 0.383527\tFr_r: 0.414930\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843378\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.957244                \tClf: 2.865014\tReg: 0.000061\tFr_p: 0.109597\tFr_r: 0.118808\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855080\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.930184                \tClf: 2.837369\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118708\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863020\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.238370                \tClf: 3.145158\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118991\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843416\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.828427                \tClf: 2.736195\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118884\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.864782\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.919660                \tClf: 2.826359\tReg: 0.000061\tFr_p: 0.109840\tFr_r: 0.119326\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850085\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.925384                \tClf: 2.832819\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119790\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837852\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.987740                \tClf: 2.895786\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118935\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.843965\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.837340                \tClf: 2.745081\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119166\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859664\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.129075                \tClf: 3.036030\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118836\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850358\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.889906                \tClf: 2.797327\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119343\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842914\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.207068                \tClf: 3.114861\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119237\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855485\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.036881                \tClf: 2.944046\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119509\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835309\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.811327                \tClf: 2.719501\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119141\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839249\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.200097                \tClf: 2.108074\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845725\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.912341                \tClf: 2.819994\tReg: 0.000061\tFr_p: 0.383577\tFr_r: 0.414932\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843307\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.940388                \tClf: 2.848162\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118780\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855067\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.844782                \tClf: 2.751967\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118730\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862592\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.212946                \tClf: 3.119756\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118990\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842546\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.831033                \tClf: 2.738844\tReg: 0.000061\tFr_p: 0.109666\tFr_r: 0.118877\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864778\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.000139                \tClf: 2.906839\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119373\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850508\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.994721                \tClf: 2.902135\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119851\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837678\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.027940                \tClf: 2.935995\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118972\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844839\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.819359                \tClf: 2.727056\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119180\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860308\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.151454                \tClf: 3.058378\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118797\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850396\tlr: 0.001000\ttrain acc:97.0500\tLoss: 2.896806                \tClf: 2.804225\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119381\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842950\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.166266                \tClf: 3.074057\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119272\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855392\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.110580                \tClf: 3.017749\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119512\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834817\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.862732                \tClf: 2.770930\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119169\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839809\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.190473                \tClf: 2.098422\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846518\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.987802                \tClf: 2.895415\tReg: 0.000061\tFr_p: 0.383553\tFr_r: 0.414932\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842729\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.910959                \tClf: 2.818761\tReg: 0.000061\tFr_p: 0.109562\tFr_r: 0.118759\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854920\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.936488                \tClf: 2.843681\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118713\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863271\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.147691                \tClf: 3.054466\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119004\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842952\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.869528                \tClf: 2.777319\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118871\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865229\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.957224                \tClf: 2.863901\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119378\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849937\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.982016                \tClf: 2.889458\tReg: 0.000061\tFr_p: 0.110034\tFr_r: 0.119802\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838253\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.986977                \tClf: 2.895003\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118981\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844449\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.838091                \tClf: 2.745808\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119199\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859509\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.113654                \tClf: 3.020618\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118817\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849607\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.921832                \tClf: 2.829290\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119333\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842705\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.163481                \tClf: 3.071285\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119274\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.854973\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.123709                \tClf: 3.030899\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119509\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835402\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.849868                \tClf: 2.758037\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119172\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.840076\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.215488                \tClf: 2.123423\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118909\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846199\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.940222                \tClf: 2.847851\tReg: 0.000061\tFr_p: 0.383502\tFr_r: 0.414930\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.842795\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.949496                \tClf: 2.857295\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118775\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855027\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.890648                \tClf: 2.797836\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118702\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863020\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.186918                \tClf: 3.093706\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118997\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843335\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.814084                \tClf: 2.721856\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118895\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865302\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.982451                \tClf: 2.889125\tReg: 0.000061\tFr_p: 0.109841\tFr_r: 0.119347\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850294\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.014609                \tClf: 2.922033\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119806\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837753\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.031764                \tClf: 2.939815\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118945\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845325\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.823470                \tClf: 2.731142\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119183\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860193\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.094461                \tClf: 3.001390\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118813\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849610\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.959845                \tClf: 2.867303\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119332\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842588\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.222768                \tClf: 3.130577\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119274\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855089\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.125247                \tClf: 3.032431\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119491\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834871\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.816879                \tClf: 2.725075\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119177\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839275\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.227427                \tClf: 2.135402\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118892\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846353\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.972790                \tClf: 2.880412\tReg: 0.000061\tFr_p: 0.383539\tFr_r: 0.414946\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843667\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.029451                \tClf: 2.937207\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118795\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854696\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.887641                \tClf: 2.794845\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118720\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863145\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.218289                \tClf: 3.125070\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118989\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842623\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.815791                \tClf: 2.723599\tReg: 0.000061\tFr_p: 0.109666\tFr_r: 0.118892\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864782\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.904066                \tClf: 2.810766\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119344\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850634\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.962510                \tClf: 2.869917\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119853\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.838109\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.048676                \tClf: 2.956710\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118964\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845241\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.826877                \tClf: 2.734554\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119194\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860435\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.130289                \tClf: 3.037206\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118824\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849624\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.931831                \tClf: 2.839289\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119369\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842543\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.151931                \tClf: 3.059743\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119298\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.854787\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.106695                \tClf: 3.013894\tReg: 0.000061\tFr_p: 0.109790\tFr_r: 0.119462\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835443\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.840005                \tClf: 2.748172\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119178\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840442\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.187189                \tClf: 2.095105\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846046\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.994189                \tClf: 2.901825\tReg: 0.000061\tFr_p: 0.383554\tFr_r: 0.414945\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843072\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.934718                \tClf: 2.842504\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118785\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854717\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.922150                \tClf: 2.829353\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118698\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863556\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.216887                \tClf: 3.123648\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118993\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843503\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.826391                \tClf: 2.734155\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118887\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864947\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.960702                \tClf: 2.867394\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119354\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849737\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.008520                \tClf: 2.915972\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119799\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.836913\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.043055                \tClf: 2.951148\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118979\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845199\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.879051                \tClf: 2.786730\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119192\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859581\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.113856                \tClf: 3.020816\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118781\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849430\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.908102                \tClf: 2.815569\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119363\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842560\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.236602                \tClf: 3.144413\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119263\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855692\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.059135                \tClf: 2.966289\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119498\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835092\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.853821                \tClf: 2.762005\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119163\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839655\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.171763                \tClf: 2.079720\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845901\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.916112                \tClf: 2.823756\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414930\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843452\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.007212                \tClf: 2.914979\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118758\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855198\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.956956                \tClf: 2.864135\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118710\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863164\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.179290                \tClf: 3.086071\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119021\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843252\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.849328                \tClf: 2.757105\tReg: 0.000061\tFr_p: 0.109665\tFr_r: 0.118890\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865493\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.941255                \tClf: 2.847919\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119357\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850532\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.970360                \tClf: 2.877772\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119788\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837148\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.989629                \tClf: 2.897710\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118984\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844930\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.859436                \tClf: 2.767129\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119204\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860334\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.155248                \tClf: 3.062170\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118854\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849216\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.933205                \tClf: 2.840684\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119349\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842529\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.157513                \tClf: 3.065325\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119283\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855730\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.087811                \tClf: 2.994963\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119534\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.834832\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.809700                \tClf: 2.717897\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119194\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839415\tlr: 0.001000\ttrain acc:98.0250\tLoss: 2.221484                \tClf: 2.129452\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118888\n",
      "\n",
      "Test set: Average loss: 0.0928, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846825\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.925093                \tClf: 2.832690\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414964\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842685\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.942198                \tClf: 2.850003\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118778\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855213\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.905693                \tClf: 2.812871\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118695\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863399\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.239201                \tClf: 3.145970\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118994\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843254\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.823790                \tClf: 2.731566\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118892\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865731\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.949760                \tClf: 2.856412\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119360\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850313\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.974036                \tClf: 2.881459\tReg: 0.000061\tFr_p: 0.110002\tFr_r: 0.119777\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837542\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.987622                \tClf: 2.895684\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118961\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844419\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.785878                \tClf: 2.693596\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119202\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860120\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.126992                \tClf: 3.033925\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118857\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849894\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.952241                \tClf: 2.859685\tReg: 0.000061\tFr_p: 0.109877\tFr_r: 0.119341\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843524\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.162394                \tClf: 3.070157\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.119297\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855252\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.079684                \tClf: 2.986860\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119524\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835306\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.809359                \tClf: 2.717533\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119176\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839266\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.220040                \tClf: 2.128016\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846033\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.969991                \tClf: 2.877629\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414947\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843485\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.904592                \tClf: 2.812356\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118805\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855083\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.953259                \tClf: 2.860443\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118699\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862784\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.182255                \tClf: 3.089055\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118997\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842591\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.863673                \tClf: 2.771482\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118889\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865035\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.953031                \tClf: 2.859718\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119363\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.849193\tlr: 0.001000\ttrain acc:97.0000\tLoss: 2.970361                \tClf: 2.877841\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119795\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837467\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.032126                \tClf: 2.940192\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118951\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844475\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.866284                \tClf: 2.773999\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119207\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860057\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.105054                \tClf: 3.011990\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118831\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850294\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.887662                \tClf: 2.795086\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119332\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.843133\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.174495                \tClf: 3.082277\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119291\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855755\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.120739                \tClf: 3.027890\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119532\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834668\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.859586                \tClf: 2.767791\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.119160\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839145\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.229440                \tClf: 2.137421\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0973, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845536\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.973722                \tClf: 2.881384\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414966\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842659\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.973289                \tClf: 2.881095\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118774\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.855627\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.987598                \tClf: 2.894755\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118709\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862647\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.219800                \tClf: 3.126607\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118996\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842863\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.819227                \tClf: 2.727023\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118882\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864602\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.996912                \tClf: 2.903621\tReg: 0.000061\tFr_p: 0.109847\tFr_r: 0.119322\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850269\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.950054                \tClf: 2.857479\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119790\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837584\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.995813                \tClf: 2.903872\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118975\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844906\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.815445                \tClf: 2.723139\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119174\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860182\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.178663                \tClf: 3.085593\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118803\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850491\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.949921                \tClf: 2.857335\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119345\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842420\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.150995                \tClf: 3.058813\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119297\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855215\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.127710                \tClf: 3.034888\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119496\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835228\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.904165                \tClf: 2.812343\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119162\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839810\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.217386                \tClf: 2.125334\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846276\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.916421                \tClf: 2.824046\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414941\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842850\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.969991                \tClf: 2.877788\tReg: 0.000061\tFr_p: 0.109595\tFr_r: 0.118787\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854379\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.977211                \tClf: 2.884431\tReg: 0.000061\tFr_p: 0.109661\tFr_r: 0.118682\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863232\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.233582                \tClf: 3.140359\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118989\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843079\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.793042                \tClf: 2.700827\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118869\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864918\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.952367                \tClf: 2.859060\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119332\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850382\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.949007                \tClf: 2.856427\tReg: 0.000061\tFr_p: 0.110049\tFr_r: 0.119857\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.836953\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.030619                \tClf: 2.938710\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118982\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845265\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.787162                \tClf: 2.694838\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119215\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859572\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.165376                \tClf: 3.072336\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118815\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850402\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.973949                \tClf: 2.881368\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119383\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.843003\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.229336                \tClf: 3.137125\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119279\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855791\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.137141                \tClf: 3.044290\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119523\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.834605\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.848605                \tClf: 2.756813\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119160\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839700\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.215098                \tClf: 2.123052\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118926\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846237\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.944804                \tClf: 2.852432\tReg: 0.000061\tFr_p: 0.383561\tFr_r: 0.414959\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843218\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.035915                \tClf: 2.943693\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118781\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854603\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.966460                \tClf: 2.873668\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118721\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862847\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.175660                \tClf: 3.082456\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118977\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842822\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.820012                \tClf: 2.727809\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118898\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864575\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.974666                \tClf: 2.881376\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119369\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850127\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.912048                \tClf: 2.819481\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119795\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838081\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.018932                \tClf: 2.926967\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118986\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845405\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.867689                \tClf: 2.775358\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119187\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860048\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.114578                \tClf: 3.021515\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118834\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850151\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.925480                \tClf: 2.832912\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119345\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843387\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.181844                \tClf: 3.089614\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.119306\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855761\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.087774                \tClf: 2.994925\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119517\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.834992\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.893375                \tClf: 2.801565\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119144\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839828\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.197220                \tClf: 2.105167\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118924\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846752\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.980423                \tClf: 2.888024\tReg: 0.000061\tFr_p: 0.383553\tFr_r: 0.414955\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843515\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.966133                \tClf: 2.873896\tReg: 0.000061\tFr_p: 0.109601\tFr_r: 0.118789\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854502\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.918640                \tClf: 2.825854\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118713\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862440\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.138590                \tClf: 3.045407\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119006\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842507\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.844677                \tClf: 2.752491\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118872\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864267\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.038158                \tClf: 2.944884\tReg: 0.000061\tFr_p: 0.109837\tFr_r: 0.119316\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.849821\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.958745                \tClf: 2.866193\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119824\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837405\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.000245                \tClf: 2.908314\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118964\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844688\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.792119                \tClf: 2.699824\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119162\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860607\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.105049                \tClf: 3.011957\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118840\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850441\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.923710                \tClf: 2.831127\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119363\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842521\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.201103                \tClf: 3.108916\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119276\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855009\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.100436                \tClf: 3.007624\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119499\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835783\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.835177                \tClf: 2.743327\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119166\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839002\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.206783                \tClf: 2.114772\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118896\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845696\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.013289                \tClf: 2.920943\tReg: 0.000061\tFr_p: 0.383559\tFr_r: 0.414943\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842328\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.921126                \tClf: 2.828948\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118767\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855033\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.914652                \tClf: 2.821839\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118707\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862759\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.160433                \tClf: 3.067234\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118971\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843055\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.822723                \tClf: 2.730509\tReg: 0.000061\tFr_p: 0.109666\tFr_r: 0.118878\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865498\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.974695                \tClf: 2.881359\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119364\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.849703\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.944111                \tClf: 2.851565\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119817\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837619\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.011309                \tClf: 2.919367\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118965\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845595\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.871532                \tClf: 2.779191\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119207\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860111\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.139303                \tClf: 3.046237\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118791\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849932\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.954170                \tClf: 2.861612\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119324\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842799\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.184866                \tClf: 3.092664\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119262\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.856163\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.134352                \tClf: 3.041482\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119526\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835005\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.831360                \tClf: 2.739548\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119162\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839048\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.224156                \tClf: 2.132143\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118873\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846939\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.984158                \tClf: 2.891750\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414931\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843471\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.949266                \tClf: 2.857031\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118759\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855416\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.985222                \tClf: 2.892390\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118705\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863054\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.227915                \tClf: 3.134701\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118993\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842717\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.861228                \tClf: 2.769032\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118870\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864810\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.973452                \tClf: 2.880150\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119358\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849988\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.940402                \tClf: 2.847842\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119803\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837787\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.073391                \tClf: 2.981440\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118967\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844801\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.795855                \tClf: 2.703554\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119196\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860396\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.136269                \tClf: 3.043188\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118817\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850858\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.926035                \tClf: 2.833431\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119356\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842584\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.141739                \tClf: 3.049549\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119300\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855596\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.109287                \tClf: 3.016446\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119526\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835598\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.933292                \tClf: 2.841451\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119183\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839597\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.232500                \tClf: 2.140459\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.118936\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846570\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.958915                \tClf: 2.866526\tReg: 0.000061\tFr_p: 0.383511\tFr_r: 0.414927\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843271\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.968649                \tClf: 2.876424\tReg: 0.000061\tFr_p: 0.109601\tFr_r: 0.118808\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854060\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.895403                \tClf: 2.802639\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118684\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862329\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.180643                \tClf: 3.087466\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118965\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843824\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.837062                \tClf: 2.744810\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118892\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865676\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.005922                \tClf: 2.912577\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119350\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850783\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.983463                \tClf: 2.890862\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119828\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837829\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.062700                \tClf: 2.970748\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119000\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845409\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.871776                \tClf: 2.779445\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119195\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860047\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.112609                \tClf: 3.019546\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118811\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849667\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.924234                \tClf: 2.831690\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119361\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842462\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.223891                \tClf: 3.131706\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119272\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855736\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.147468                \tClf: 3.054620\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119518\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835077\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.879726                \tClf: 2.787912\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119190\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839477\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.206247                \tClf: 2.114212\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845921\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.021158                \tClf: 2.928801\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414936\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.842884\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.946742                \tClf: 2.854537\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118761\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855329\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.883885                \tClf: 2.791058\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118706\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862996\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.135381                \tClf: 3.042170\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.119014\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842938\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.863211                \tClf: 2.771003\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118892\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865254\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.954719                \tClf: 2.861395\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119341\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850126\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.049813                \tClf: 2.957246\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119806\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838118\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.973826                \tClf: 2.881859\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118996\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844683\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.797206                \tClf: 2.704911\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119197\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860244\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.148386                \tClf: 3.055312\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118816\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849295\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.976317                \tClf: 2.883791\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119346\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842625\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.232487                \tClf: 3.140295\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119246\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855703\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.068706                \tClf: 2.975860\tReg: 0.000061\tFr_p: 0.109795\tFr_r: 0.119488\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834974\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.848926                \tClf: 2.757117\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119170\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839034\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.213622                \tClf: 2.121609\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846019\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.960674                \tClf: 2.868312\tReg: 0.000061\tFr_p: 0.383537\tFr_r: 0.414921\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843613\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.920412                \tClf: 2.828170\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118793\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854012\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.856162                \tClf: 2.763400\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118737\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862714\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.170651                \tClf: 3.077455\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118980\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843068\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.831494                \tClf: 2.739279\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118876\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865201\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.984649                \tClf: 2.891328\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119375\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849786\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.922609                \tClf: 2.830058\tReg: 0.000061\tFr_p: 0.110037\tFr_r: 0.119818\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838015\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.001218                \tClf: 2.909257\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118964\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844996\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.843800                \tClf: 2.751489\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119182\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859921\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.110973                \tClf: 3.017916\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118813\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850884\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.937398                \tClf: 2.844793\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119367\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843214\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.153186                \tClf: 3.060965\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119284\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855630\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.128321                \tClf: 3.035479\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119512\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835455\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.812354                \tClf: 2.720520\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119152\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839745\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.205460                \tClf: 2.113411\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118892\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846511\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.030348                \tClf: 2.937961\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414911\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.844088\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.941964                \tClf: 2.849698\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118776\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855170\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.933448                \tClf: 2.840629\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118719\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863125\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.178633                \tClf: 3.085415\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118981\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842800\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.834621                \tClf: 2.742420\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118883\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864920\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.974670                \tClf: 2.881363\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119357\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850659\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.990055                \tClf: 2.897461\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119809\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837629\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.026186                \tClf: 2.934243\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118945\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845286\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.844733                \tClf: 2.752408\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119228\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860789\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.119510                \tClf: 3.026409\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118828\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850323\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.953134                \tClf: 2.860556\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119357\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843034\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.187755                \tClf: 3.095542\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.119301\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855780\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.084959                \tClf: 2.992109\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119519\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835202\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.897442                \tClf: 2.805620\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119190\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839290\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.249882                \tClf: 2.157856\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9726/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846069\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.001654                \tClf: 2.909290\tReg: 0.000061\tFr_p: 0.383568\tFr_r: 0.414963\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843611\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.976996                \tClf: 2.884755\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118791\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854732\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.978091                \tClf: 2.885293\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118701\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862637\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.174536                \tClf: 3.081343\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118966\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843397\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.847168                \tClf: 2.754937\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118875\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864730\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.948596                \tClf: 2.855298\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119362\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850340\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.970973                \tClf: 2.878395\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119786\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837456\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.971941                \tClf: 2.880007\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118999\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845077\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.814654                \tClf: 2.722339\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119204\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860391\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.101110                \tClf: 3.008029\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118834\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849628\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.900250                \tClf: 2.807707\tReg: 0.000061\tFr_p: 0.109901\tFr_r: 0.119377\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.843004\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.158587                \tClf: 3.066376\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119259\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855839\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.151176                \tClf: 3.058323\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119530\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835373\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.896628                \tClf: 2.804798\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119166\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839105\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.235220                \tClf: 2.143204\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846149\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.922667                \tClf: 2.830298\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414929\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843065\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.983465                \tClf: 2.891250\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118774\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855213\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.896337                \tClf: 2.803515\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118704\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863494\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.229404                \tClf: 3.136168\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118951\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842775\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.896066                \tClf: 2.803866\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118905\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865269\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.985072                \tClf: 2.891747\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119350\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849736\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.019052                \tClf: 2.926504\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119782\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.838595\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.028069                \tClf: 2.936078\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118990\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845318\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.825249                \tClf: 2.732922\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119174\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860081\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.132809                \tClf: 3.039744\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118818\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849837\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.985459                \tClf: 2.892906\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119354\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.843154\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.189640                \tClf: 3.097421\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.119287\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.856213\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.086702                \tClf: 2.993830\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119533\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835478\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.872809                \tClf: 2.780974\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119159\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839511\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.172626                \tClf: 2.080590\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0930, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845511\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.972169                \tClf: 2.879832\tReg: 0.000061\tFr_p: 0.383570\tFr_r: 0.414945\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843167\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.913114                \tClf: 2.820894\tReg: 0.000061\tFr_p: 0.109597\tFr_r: 0.118807\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855204\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.860788                \tClf: 2.767967\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118710\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863383\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.267214                \tClf: 3.173983\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118983\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842585\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.854425                \tClf: 2.762235\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118872\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865274\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.946561                \tClf: 2.853236\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119350\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849647\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.985809                \tClf: 2.893266\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119809\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.838431\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.052744                \tClf: 2.960761\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118991\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845683\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.823493                \tClf: 2.731148\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119220\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860152\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.096594                \tClf: 3.003526\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118823\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850336\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.961824                \tClf: 2.869246\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119375\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.841919\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.166339                \tClf: 3.074182\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119267\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.856059\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.114313                \tClf: 3.021449\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119521\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835043\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.896720                \tClf: 2.804907\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119178\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839312\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.205106                \tClf: 2.113079\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118888\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846427\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.975903                \tClf: 2.883521\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414949\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843128\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.981930                \tClf: 2.889712\tReg: 0.000061\tFr_p: 0.109570\tFr_r: 0.118778\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854577\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.919666                \tClf: 2.826876\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118706\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863143\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.245052                \tClf: 3.151834\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119018\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843336\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.765005                \tClf: 2.672777\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118922\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865286\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.926832                \tClf: 2.833506\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119357\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850525\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.950760                \tClf: 2.858173\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119788\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837524\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.009085                \tClf: 2.917147\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118965\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845568\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.825103                \tClf: 2.732763\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119212\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860339\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.115020                \tClf: 3.021941\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118827\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849232\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.920831                \tClf: 2.828308\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119361\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843864\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.156688                \tClf: 3.064433\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119278\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855868\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.144290                \tClf: 3.051436\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119534\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835662\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.915246                \tClf: 2.823402\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119175\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839466\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.201552                \tClf: 2.109518\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.118931\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846269\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.996233                \tClf: 2.903859\tReg: 0.000061\tFr_p: 0.383561\tFr_r: 0.414961\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842702\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.907800                \tClf: 2.815604\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118776\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854238\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.959169                \tClf: 2.866396\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118716\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863284\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.188162                \tClf: 3.094937\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119011\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842946\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.874182                \tClf: 2.781974\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118876\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.864693\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.960102                \tClf: 2.866806\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119342\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850322\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.948372                \tClf: 2.855795\tReg: 0.000061\tFr_p: 0.110007\tFr_r: 0.119810\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837514\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.018180                \tClf: 2.926243\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118982\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844654\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.826832                \tClf: 2.734538\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119202\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860355\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.101164                \tClf: 3.008085\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118837\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849169\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.917354                \tClf: 2.824834\tReg: 0.000061\tFr_p: 0.109889\tFr_r: 0.119351\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842167\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.179882                \tClf: 3.087713\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119249\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855385\tlr: 0.001000\ttrain acc:97.7250\tLoss: 3.158814                \tClf: 3.065984\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119481\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834917\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.832529                \tClf: 2.740723\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119151\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.838369\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.226537                \tClf: 2.134557\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118907\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846113\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.003821                \tClf: 2.911455\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414981\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842490\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.944095                \tClf: 2.851910\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118761\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854553\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.859703                \tClf: 2.766914\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118675\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863751\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.180625                \tClf: 3.087377\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118994\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842853\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.824596                \tClf: 2.732392\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118871\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865221\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.942264                \tClf: 2.848942\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119368\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850075\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.870836                \tClf: 2.778271\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119804\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837665\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.038254                \tClf: 2.946309\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118960\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845618\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.757529                \tClf: 2.665187\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119197\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860446\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.116725                \tClf: 3.023641\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118806\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849635\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.941552                \tClf: 2.849009\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119336\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842934\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.262045                \tClf: 3.169838\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119292\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.856113\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.088891                \tClf: 2.996025\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119566\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835426\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.883714                \tClf: 2.791882\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119161\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839338\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.198946                \tClf: 2.106918\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846353\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.009900                \tClf: 2.917521\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414943\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843149\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.953430                \tClf: 2.861211\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118785\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855127\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.897119                \tClf: 2.804302\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118709\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862952\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.184907                \tClf: 3.091698\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118991\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842477\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.857387                \tClf: 2.765202\tReg: 0.000061\tFr_p: 0.109666\tFr_r: 0.118871\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864573\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.008602                \tClf: 2.915312\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119350\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850198\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.910233                \tClf: 2.817662\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119815\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837744\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.996833                \tClf: 2.904885\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118969\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845349\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.796473                \tClf: 2.704144\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119193\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860105\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.137430                \tClf: 3.044364\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118826\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850438\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.862917                \tClf: 2.770334\tReg: 0.000061\tFr_p: 0.109892\tFr_r: 0.119385\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.843006\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.204220                \tClf: 3.112009\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119270\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856203\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.131692                \tClf: 3.038821\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119520\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.834910\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.857830                \tClf: 2.766024\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119169\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839445\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.176256                \tClf: 2.084222\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0934, Accuracy: 9744/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845936\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.025256                \tClf: 2.932898\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414959\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843305\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.963260                \tClf: 2.871033\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118786\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854628\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.909677                \tClf: 2.816885\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118710\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863958\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.206497                \tClf: 3.113238\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119008\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843399\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.849724                \tClf: 2.757493\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118887\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864522\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.924953                \tClf: 2.831665\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119360\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.849912\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.990110                \tClf: 2.897553\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119807\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838112\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.977069                \tClf: 2.885103\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118972\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845381\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.819854                \tClf: 2.727524\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119209\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859417\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.136841                \tClf: 3.043809\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118824\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850147\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.902044                \tClf: 2.809475\tReg: 0.000061\tFr_p: 0.109892\tFr_r: 0.119369\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842836\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.174834                \tClf: 3.082631\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119263\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855864\tlr: 0.001000\ttrain acc:97.7500\tLoss: 3.095885                \tClf: 3.003031\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119495\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835115\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.854660                \tClf: 2.762844\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119181\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.840179\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.185458                \tClf: 2.093388\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118907\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846702\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.006684                \tClf: 2.914288\tReg: 0.000061\tFr_p: 0.383563\tFr_r: 0.414959\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.842603\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.991857                \tClf: 2.899665\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118764\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854871\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.932574                \tClf: 2.839769\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118723\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863246\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.220375                \tClf: 3.127152\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118977\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842584\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.836805                \tClf: 2.744615\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118893\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864251\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.006461                \tClf: 2.913187\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119333\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850127\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.952868                \tClf: 2.860300\tReg: 0.000061\tFr_p: 0.110044\tFr_r: 0.119818\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838144\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.027381                \tClf: 2.935413\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118960\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845357\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.775444                \tClf: 2.683115\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119199\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860267\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.132279                \tClf: 3.039205\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118842\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850401\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.841369                \tClf: 2.748787\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119364\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842273\tlr: 0.001000\ttrain acc:96.7500\tLoss: 3.230074                \tClf: 3.137900\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119285\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855705\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.106484                \tClf: 3.013638\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119503\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835018\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.909195                \tClf: 2.817383\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119160\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.840252\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.224145                \tClf: 2.132072\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118924\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845636\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.987895                \tClf: 2.895552\tReg: 0.000061\tFr_p: 0.383525\tFr_r: 0.414933\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843201\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.991860                \tClf: 2.899639\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118792\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855276\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.922745                \tClf: 2.829920\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118710\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863488\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.212224                \tClf: 3.118989\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119003\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842979\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.811952                \tClf: 2.719742\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118921\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865084\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.956036                \tClf: 2.862720\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119360\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850091\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.960551                \tClf: 2.867985\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119825\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.836875\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.031204                \tClf: 2.939299\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118987\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845200\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.854661                \tClf: 2.762340\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119198\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860399\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.108824                \tClf: 3.015743\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118837\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850042\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.884582                \tClf: 2.792019\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119341\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842416\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.182801                \tClf: 3.090619\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119266\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855286\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.100748                \tClf: 3.007923\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119494\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834745\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.879863                \tClf: 2.788064\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.119173\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839252\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.272381                \tClf: 2.180357\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118886\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846105\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.979598                \tClf: 2.887231\tReg: 0.000061\tFr_p: 0.383547\tFr_r: 0.414955\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842993\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.974640                \tClf: 2.882429\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118800\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854837\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.963100                \tClf: 2.870297\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118702\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862345\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.296253                \tClf: 3.203075\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119003\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842511\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.816998                \tClf: 2.724811\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118919\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864807\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.981743                \tClf: 2.888441\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119361\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850002\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.970852                \tClf: 2.878291\tReg: 0.000061\tFr_p: 0.110010\tFr_r: 0.119793\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837947\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.045430                \tClf: 2.953472\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118968\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844694\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.816572                \tClf: 2.724276\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119205\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859448\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.110074                \tClf: 3.017041\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118814\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849446\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.893439                \tClf: 2.800905\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119359\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842673\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.168882                \tClf: 3.076687\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119288\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855852\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.150163                \tClf: 3.057310\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119529\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835110\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.852104                \tClf: 2.760288\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119170\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839880\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.231676                \tClf: 2.139621\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118929\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846628\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.976829                \tClf: 2.884437\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414932\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.842926\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.966750                \tClf: 2.874543\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118774\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854408\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.951248                \tClf: 2.858467\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118711\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863429\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.240946                \tClf: 3.147713\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118991\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842563\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.794942                \tClf: 2.702752\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118897\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865122\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.958569                \tClf: 2.865252\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119359\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850009\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.912578                \tClf: 2.820016\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119813\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837406\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.993753                \tClf: 2.901822\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118989\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845275\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.831425                \tClf: 2.739100\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119205\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860228\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.156703                \tClf: 3.063631\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118822\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850183\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.955058                \tClf: 2.862488\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119375\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842877\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.181915                \tClf: 3.089710\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119288\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855820\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.137280                \tClf: 3.044428\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119527\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834977\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.864762                \tClf: 2.772952\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119180\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.838986\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.183623                \tClf: 2.091613\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118867\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845737\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.025856                \tClf: 2.933508\tReg: 0.000061\tFr_p: 0.383551\tFr_r: 0.414960\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.842877\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.925280                \tClf: 2.833075\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118776\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854798\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.942032                \tClf: 2.849231\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118711\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863784\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.204364                \tClf: 3.111114\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118995\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842396\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.827581                \tClf: 2.735400\tReg: 0.000061\tFr_p: 0.109669\tFr_r: 0.118861\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865478\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.004127                \tClf: 2.910792\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119342\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849408\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.997188                \tClf: 2.904656\tReg: 0.000061\tFr_p: 0.110015\tFr_r: 0.119806\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838314\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.036480                \tClf: 2.944503\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118988\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.846505\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.800547                \tClf: 2.708161\tReg: 0.000061\tFr_p: 0.109847\tFr_r: 0.119213\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860243\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.118810                \tClf: 3.025737\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118854\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850069\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.949960                \tClf: 2.857395\tReg: 0.000061\tFr_p: 0.109898\tFr_r: 0.119369\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842792\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.234823                \tClf: 3.142622\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119284\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855578\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.196288                \tClf: 3.103448\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119516\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835076\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.839624                \tClf: 2.747810\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119186\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839287\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.189713                \tClf: 2.097688\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0930, Accuracy: 9741/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846069\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.982866                \tClf: 2.890502\tReg: 0.000061\tFr_p: 0.383561\tFr_r: 0.414946\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843463\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.970457                \tClf: 2.878223\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118778\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855461\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.919888                \tClf: 2.827054\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118710\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862392\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.240458                \tClf: 3.147277\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118991\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843148\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.844992                \tClf: 2.752773\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118863\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864973\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.936673                \tClf: 2.843363\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119363\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849074\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.897103                \tClf: 2.804588\tReg: 0.000061\tFr_p: 0.110015\tFr_r: 0.119789\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837758\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.030829                \tClf: 2.938880\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118967\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844767\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.828905                \tClf: 2.736606\tReg: 0.000061\tFr_p: 0.109823\tFr_r: 0.119210\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860086\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.118279                \tClf: 3.025214\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118813\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850009\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.933456                \tClf: 2.840895\tReg: 0.000061\tFr_p: 0.109901\tFr_r: 0.119365\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842355\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.198585                \tClf: 3.106406\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119281\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.856227\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.133267                \tClf: 3.040395\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119502\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835765\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.858812                \tClf: 2.766963\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119169\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839419\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.248851                \tClf: 2.156819\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845525\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.985184                \tClf: 2.892847\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414958\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843200\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.951563                \tClf: 2.859342\tReg: 0.000061\tFr_p: 0.109608\tFr_r: 0.118801\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854799\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.920797                \tClf: 2.827996\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118688\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863133\tlr: 0.001000\ttrain acc:96.7750\tLoss: 3.218361                \tClf: 3.125143\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119007\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843192\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.809340                \tClf: 2.717120\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118913\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864801\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.906968                \tClf: 2.813667\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119368\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849983\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.915119                \tClf: 2.822559\tReg: 0.000061\tFr_p: 0.110015\tFr_r: 0.119803\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837775\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.056562                \tClf: 2.964612\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118945\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845015\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.866658                \tClf: 2.774346\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119178\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860048\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.089264                \tClf: 2.996200\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118823\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850108\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.989728                \tClf: 2.897162\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119328\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842457\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.185113                \tClf: 3.092929\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119273\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855329\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.150979                \tClf: 3.058151\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119482\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835043\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.827947                \tClf: 2.736133\tReg: 0.000061\tFr_p: 0.109779\tFr_r: 0.119173\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839701\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.184036                \tClf: 2.091990\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118877\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846769\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.001180                \tClf: 2.908780\tReg: 0.000061\tFr_p: 0.383559\tFr_r: 0.414943\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843321\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.965101                \tClf: 2.872874\tReg: 0.000061\tFr_p: 0.109609\tFr_r: 0.118802\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854761\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.885687                \tClf: 2.792888\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118713\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863005\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.218246                \tClf: 3.125035\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118972\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842677\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.800735                \tClf: 2.708540\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118904\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864671\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.977259                \tClf: 2.883964\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119344\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.851097\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.984681                \tClf: 2.892065\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119819\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837513\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.040408                \tClf: 2.948471\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118989\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845123\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.868350                \tClf: 2.776033\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119217\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860120\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.091321                \tClf: 2.998254\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118805\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850039\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.038863                \tClf: 2.946300\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119356\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842907\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.129770                \tClf: 3.037564\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.119254\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855844\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.106987                \tClf: 3.014134\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119518\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835315\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.856386                \tClf: 2.764559\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119156\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839511\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.216005                \tClf: 2.123969\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846028\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.012478                \tClf: 2.920115\tReg: 0.000061\tFr_p: 0.383501\tFr_r: 0.414922\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843320\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.065143                \tClf: 2.972916\tReg: 0.000061\tFr_p: 0.109615\tFr_r: 0.118803\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854134\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.889416                \tClf: 2.796648\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118681\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863402\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.164054                \tClf: 3.070822\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119003\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843028\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.835147                \tClf: 2.742934\tReg: 0.000061\tFr_p: 0.109661\tFr_r: 0.118861\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864653\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.943144                \tClf: 2.849850\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119365\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850032\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.932469                \tClf: 2.839906\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119823\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.838516\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.009173                \tClf: 2.917186\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118962\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845296\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.784078                \tClf: 2.691753\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119193\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860125\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.110790                \tClf: 3.017723\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118801\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850475\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.911272                \tClf: 2.818687\tReg: 0.000061\tFr_p: 0.109892\tFr_r: 0.119380\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842389\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.174396                \tClf: 3.082215\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119289\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.854740\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.079284                \tClf: 2.986486\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119489\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835227\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.824925                \tClf: 2.733103\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119183\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839209\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.224259                \tClf: 2.132237\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846071\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.985940                \tClf: 2.893575\tReg: 0.000061\tFr_p: 0.383517\tFr_r: 0.414927\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843046\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.955693                \tClf: 2.863479\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118791\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855255\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.898152                \tClf: 2.805328\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118704\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862408\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.221691                \tClf: 3.128509\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.119002\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842244\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.819224                \tClf: 2.727051\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118905\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864757\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.931555                \tClf: 2.838256\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119359\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850026\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.009717                \tClf: 2.917155\tReg: 0.000061\tFr_p: 0.110010\tFr_r: 0.119804\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837694\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.073362                \tClf: 2.981416\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118958\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844995\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.755441                \tClf: 2.663130\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119196\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859630\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.054149                \tClf: 2.961107\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118834\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849673\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.881037                \tClf: 2.788492\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119369\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843347\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.184307                \tClf: 3.092079\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119269\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855196\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.145320                \tClf: 3.052499\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119511\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835107\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.844715                \tClf: 2.752899\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119135\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839626\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.142945                \tClf: 2.050903\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845581\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.917229                \tClf: 2.824889\tReg: 0.000061\tFr_p: 0.383517\tFr_r: 0.414941\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842484\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.879186                \tClf: 2.787000\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118795\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854515\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.991494                \tClf: 2.898707\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118701\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862763\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.241958                \tClf: 3.148759\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119005\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842226\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.881432                \tClf: 2.789259\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118875\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864764\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.963350                \tClf: 2.870051\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119359\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850086\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.965654                \tClf: 2.873089\tReg: 0.000061\tFr_p: 0.110024\tFr_r: 0.119809\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838147\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.049317                \tClf: 2.957349\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118979\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845245\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.800994                \tClf: 2.708671\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119212\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860115\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.091988                \tClf: 2.998921\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118844\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849749\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.942403                \tClf: 2.849854\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119360\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843321\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.139125                \tClf: 3.046898\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119280\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855777\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.120436                \tClf: 3.027587\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119517\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835658\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.860781                \tClf: 2.768937\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119198\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839509\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.234922                \tClf: 2.142886\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846334\tlr: 0.001000\ttrain acc:97.8250\tLoss: 3.003708                \tClf: 2.911331\tReg: 0.000061\tFr_p: 0.383585\tFr_r: 0.414975\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.842659\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.898773                \tClf: 2.806579\tReg: 0.000061\tFr_p: 0.109566\tFr_r: 0.118750\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855308\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.967142                \tClf: 2.874316\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118715\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863846\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.210183                \tClf: 3.116930\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119004\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842753\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.855173                \tClf: 2.762975\tReg: 0.000061\tFr_p: 0.109663\tFr_r: 0.118870\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865040\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.900646                \tClf: 2.807333\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119373\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.849506\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.979018                \tClf: 2.886482\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119826\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837577\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.062802                \tClf: 2.970862\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118968\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845643\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.841086                \tClf: 2.748743\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119242\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860721\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.162807                \tClf: 3.069710\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118837\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849599\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.911249                \tClf: 2.818708\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119365\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842480\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.134540                \tClf: 3.042355\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119244\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855521\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.094529                \tClf: 3.001692\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119477\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835139\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.837547                \tClf: 2.745729\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119166\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839475\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.194299                \tClf: 2.102264\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846082\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.963369                \tClf: 2.871004\tReg: 0.000061\tFr_p: 0.383530\tFr_r: 0.414924\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843897\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.970405                \tClf: 2.878149\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118770\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854862\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.881245                \tClf: 2.788441\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118706\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862640\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.226622                \tClf: 3.133429\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118997\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843297\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.806129                \tClf: 2.713903\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118884\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865703\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.929810                \tClf: 2.836464\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119348\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850087\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.919467                \tClf: 2.826902\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119809\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837318\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.067911                \tClf: 2.975984\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118965\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845065\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.828118                \tClf: 2.735804\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119233\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860306\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.155237                \tClf: 3.062161\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118816\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850336\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.957828                \tClf: 2.865250\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119348\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842629\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.185761                \tClf: 3.093568\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119281\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.856245\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.141816                \tClf: 3.048943\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119520\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834774\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.863676                \tClf: 2.771876\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119149\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839761\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.238393                \tClf: 2.146344\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9720/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845863\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.996536                \tClf: 2.904182\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414935\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843353\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.925704                \tClf: 2.833475\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118788\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854695\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.982049                \tClf: 2.889253\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118701\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863059\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.143114                \tClf: 3.049900\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119009\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842545\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.801301                \tClf: 2.709113\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118897\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865209\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.002138                \tClf: 2.908816\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119347\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.851277\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.947804                \tClf: 2.855179\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119817\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837555\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.031377                \tClf: 2.939438\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118967\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845789\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.759926                \tClf: 2.667576\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119182\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860005\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.105654                \tClf: 3.012592\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118817\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850551\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.856112                \tClf: 2.763524\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119353\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842577\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.215952                \tClf: 3.123762\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.119274\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856127\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.167977                \tClf: 3.075110\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119504\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835137\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.863312                \tClf: 2.771494\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119168\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839157\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.229300                \tClf: 2.137281\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118895\n",
      "\n",
      "Test set: Average loss: 0.0925, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845792\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.954128                \tClf: 2.861778\tReg: 0.000061\tFr_p: 0.383561\tFr_r: 0.414917\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842954\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.948079                \tClf: 2.855870\tReg: 0.000061\tFr_p: 0.109555\tFr_r: 0.118746\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855178\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.910594                \tClf: 2.817774\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118701\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862710\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.147123                \tClf: 3.053926\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118987\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842523\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.823150                \tClf: 2.730963\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118865\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864561\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.937286                \tClf: 2.843997\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119350\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850017\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.996141                \tClf: 2.903579\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119823\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837757\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.053166                \tClf: 2.961217\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118954\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844252\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.777001                \tClf: 2.684727\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119199\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859784\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.079366                \tClf: 2.986316\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118825\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850483\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.934273                \tClf: 2.841687\tReg: 0.000061\tFr_p: 0.109877\tFr_r: 0.119353\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843315\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.209102                \tClf: 3.116875\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119271\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855668\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.155437                \tClf: 3.062592\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119508\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.834672\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.877641                \tClf: 2.785846\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119151\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839514\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.240117                \tClf: 2.148080\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846022\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.973191                \tClf: 2.880828\tReg: 0.000061\tFr_p: 0.383572\tFr_r: 0.414939\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.842734\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.897104                \tClf: 2.804907\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118766\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854564\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.918049                \tClf: 2.825259\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118742\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862918\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.137711                \tClf: 3.044504\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118994\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842837\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.813071                \tClf: 2.720868\tReg: 0.000061\tFr_p: 0.109663\tFr_r: 0.118887\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864128\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.882030                \tClf: 2.788763\tReg: 0.000061\tFr_p: 0.109846\tFr_r: 0.119365\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850647\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.929727                \tClf: 2.837134\tReg: 0.000061\tFr_p: 0.110015\tFr_r: 0.119821\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838464\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.012630                \tClf: 2.920646\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118984\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845150\tlr: 0.001000\ttrain acc:97.0250\tLoss: 2.864132                \tClf: 2.771813\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119206\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859640\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.102690                \tClf: 3.009647\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118830\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849719\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.907544                \tClf: 2.814997\tReg: 0.000061\tFr_p: 0.109896\tFr_r: 0.119365\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843283\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.225144                \tClf: 3.132919\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119302\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855773\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.063672                \tClf: 2.970823\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119528\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834682\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.866238                \tClf: 2.774443\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.119150\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839481\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.193182                \tClf: 2.101146\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845862\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.968339                \tClf: 2.875985\tReg: 0.000061\tFr_p: 0.383525\tFr_r: 0.414933\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842632\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.972842                \tClf: 2.880649\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118767\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854915\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.925737                \tClf: 2.832930\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118703\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863266\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.203287                \tClf: 3.110063\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.119001\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843365\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.829989                \tClf: 2.737759\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118873\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865032\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.867274                \tClf: 2.773961\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119351\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850395\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.941464                \tClf: 2.848883\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119819\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837782\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.048822                \tClf: 2.956872\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118961\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845086\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.839279                \tClf: 2.746964\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119193\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860279\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.103391                \tClf: 3.010316\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118804\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849507\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.898238                \tClf: 2.805702\tReg: 0.000061\tFr_p: 0.109892\tFr_r: 0.119380\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842625\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.176825                \tClf: 3.084633\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119262\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855375\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.102653                \tClf: 3.009823\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119508\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835348\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.888309                \tClf: 2.796481\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119171\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.838948\tlr: 0.001000\ttrain acc:98.5750\tLoss: 2.224428                \tClf: 2.132420\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846613\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.962575                \tClf: 2.870184\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414933\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843733\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.958537                \tClf: 2.866289\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118779\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855124\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.998275                \tClf: 2.905458\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118731\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863207\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.162352                \tClf: 3.069131\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119019\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843101\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.847207                \tClf: 2.754991\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118905\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865230\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.848385                \tClf: 2.755063\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119347\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849669\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.977859                \tClf: 2.885314\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119808\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837529\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.070417                \tClf: 2.978480\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118970\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845105\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.828753                \tClf: 2.736437\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119179\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859623\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.115797                \tClf: 3.022754\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118818\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850622\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.923651                \tClf: 2.831059\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119345\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842964\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.158142                \tClf: 3.065933\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119290\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855795\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.018301                \tClf: 2.925450\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119545\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835604\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.838512                \tClf: 2.746670\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.119155\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839842\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.183630                \tClf: 2.091576\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846617\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.983454                \tClf: 2.891062\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414962\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843761\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.913570                \tClf: 2.821321\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118782\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854731\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.935023                \tClf: 2.842226\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118727\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862409\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.161704                \tClf: 3.068522\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119005\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842895\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.871554                \tClf: 2.779348\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118888\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865160\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.901071                \tClf: 2.807752\tReg: 0.000061\tFr_p: 0.109847\tFr_r: 0.119355\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849931\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.973988                \tClf: 2.881430\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119829\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.838158\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.046385                \tClf: 2.954416\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118965\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845397\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.866885                \tClf: 2.774554\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119194\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860638\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.097610                \tClf: 3.004517\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118826\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850142\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.957669                \tClf: 2.865100\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119341\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843209\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.136945                \tClf: 3.044723\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119269\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855466\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.094928                \tClf: 3.002093\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119495\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835699\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.861651                \tClf: 2.769805\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119161\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839841\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.231266                \tClf: 2.139213\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846666\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.980433                \tClf: 2.888038\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414962\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843352\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.928154                \tClf: 2.835925\tReg: 0.000061\tFr_p: 0.109570\tFr_r: 0.118783\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854984\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.951462                \tClf: 2.858652\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118735\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862385\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.211302                \tClf: 3.118122\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118969\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843507\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.790261                \tClf: 2.698024\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118901\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865248\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.943674                \tClf: 2.850351\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119356\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849622\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.012253                \tClf: 2.919711\tReg: 0.000061\tFr_p: 0.110048\tFr_r: 0.119823\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837757\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.045991                \tClf: 2.954042\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118937\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844608\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.765603                \tClf: 2.673312\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119195\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860736\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.170547                \tClf: 3.077449\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118842\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849606\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.927085                \tClf: 2.834544\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119348\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842623\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.164896                \tClf: 3.072704\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119261\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855727\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.096124                \tClf: 3.003277\tReg: 0.000061\tFr_p: 0.109798\tFr_r: 0.119505\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834922\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.789026                \tClf: 2.697219\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.119156\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.840056\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.219978                \tClf: 2.127915\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846460\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.968249                \tClf: 2.875865\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414941\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842418\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.982557                \tClf: 2.890375\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118777\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855144\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.935872                \tClf: 2.843054\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118701\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863481\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.231092                \tClf: 3.137857\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118984\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842905\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.871501                \tClf: 2.779295\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118888\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865671\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.889887                \tClf: 2.796543\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119368\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850213\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.949244                \tClf: 2.856672\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119806\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837668\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.058855                \tClf: 2.966910\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118972\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845751\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.794038                \tClf: 2.701689\tReg: 0.000061\tFr_p: 0.109841\tFr_r: 0.119189\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860640\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.141971                \tClf: 3.048878\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118822\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850008\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.871995                \tClf: 2.779433\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119370\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842929\tlr: 0.001000\ttrain acc:96.8250\tLoss: 3.202733                \tClf: 3.110526\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.119259\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855898\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.149488                \tClf: 3.056632\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119510\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.834740\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.855855                \tClf: 2.764057\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119154\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839253\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.198715                \tClf: 2.106691\tReg: 0.000061\tFr_p: 0.109782\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846799\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.974501                \tClf: 2.882100\tReg: 0.000061\tFr_p: 0.383526\tFr_r: 0.414959\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843836\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.935851                \tClf: 2.843598\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118767\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855448\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.914163                \tClf: 2.821329\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118696\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862634\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.216940                \tClf: 3.123747\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118985\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843084\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.801230                \tClf: 2.709015\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118885\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865158\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.920963                \tClf: 2.827644\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119344\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.851039\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.959238                \tClf: 2.866625\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119835\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837598\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.038894                \tClf: 2.946953\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118971\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844932\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.820186                \tClf: 2.727879\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119187\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860292\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.114720                \tClf: 3.021644\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118832\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849681\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.950585                \tClf: 2.858040\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119325\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842905\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.142156                \tClf: 3.049950\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119296\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855216\tlr: 0.001000\ttrain acc:97.8750\tLoss: 3.060196                \tClf: 2.967374\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119499\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835313\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.867956                \tClf: 2.776130\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119175\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839116\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.247316                \tClf: 2.155299\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118900\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846038\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.958792                \tClf: 2.866429\tReg: 0.000061\tFr_p: 0.383523\tFr_r: 0.414961\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843325\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.885783                \tClf: 2.793555\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118777\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854623\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.926964                \tClf: 2.834172\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118702\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863078\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.186759                \tClf: 3.093544\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118996\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842942\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.824763                \tClf: 2.732555\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118886\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865020\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.951263                \tClf: 2.857951\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119374\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849395\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.999978                \tClf: 2.907448\tReg: 0.000061\tFr_p: 0.110038\tFr_r: 0.119827\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837875\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.056905                \tClf: 2.964950\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118971\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845747\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.811990                \tClf: 2.719641\tReg: 0.000061\tFr_p: 0.109837\tFr_r: 0.119210\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860149\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.085516                \tClf: 2.992447\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118814\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850128\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.943754                \tClf: 2.851187\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119377\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842302\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.155074                \tClf: 3.062898\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.119253\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855902\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.084162                \tClf: 2.991306\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119525\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835100\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.788511                \tClf: 2.696694\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119147\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840136\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.214733                \tClf: 2.122665\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.118927\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846238\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.996647                \tClf: 2.904274\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414913\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843218\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.959985                \tClf: 2.867763\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118779\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854634\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.928201                \tClf: 2.835408\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118698\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863492\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.185991                \tClf: 3.092756\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118988\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842783\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.900867                \tClf: 2.808667\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118883\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865265\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.893125                \tClf: 2.799800\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119361\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850182\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.974795                \tClf: 2.882224\tReg: 0.000061\tFr_p: 0.110042\tFr_r: 0.119826\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837312\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.024423                \tClf: 2.932496\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118977\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845156\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.843378                \tClf: 2.751059\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119200\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859737\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.084662                \tClf: 2.991614\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118804\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850349\tlr: 0.001000\ttrain acc:97.0250\tLoss: 2.931130                \tClf: 2.838552\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119385\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842911\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.158343                \tClf: 3.066136\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.119275\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.856195\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.124084                \tClf: 3.031214\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119524\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834689\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.889728                \tClf: 2.797933\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119165\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839528\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.231060                \tClf: 2.139022\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845926\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.926380                \tClf: 2.834023\tReg: 0.000061\tFr_p: 0.383546\tFr_r: 0.414954\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842797\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.976091                \tClf: 2.883890\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118758\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854977\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.892270                \tClf: 2.799460\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118711\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863021\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.208890                \tClf: 3.115678\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119003\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843763\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.899559                \tClf: 2.807310\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118912\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865507\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.994933                \tClf: 2.901596\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119331\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850056\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.949687                \tClf: 2.857123\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119822\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837157\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.975810                \tClf: 2.883891\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118941\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845947\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.820011                \tClf: 2.727653\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119193\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860090\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.101350                \tClf: 3.008284\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118826\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849576\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.935768                \tClf: 2.843228\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119368\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842481\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.224377                \tClf: 3.132192\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119264\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855785\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.161195                \tClf: 3.068345\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119512\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835311\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.834382                \tClf: 2.742556\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.119165\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.840183\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.189351                \tClf: 2.097281\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9719/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846575\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.951872                \tClf: 2.859482\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414953\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843489\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.953984                \tClf: 2.861749\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118773\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855099\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.922925                \tClf: 2.830109\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118726\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862628\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.266221                \tClf: 3.173028\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118990\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842990\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.841393                \tClf: 2.749182\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118893\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865352\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.957622                \tClf: 2.864293\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119358\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.849643\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.983869                \tClf: 2.891326\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119802\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838106\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.008953                \tClf: 2.916987\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118987\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845560\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.823827                \tClf: 2.731488\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119199\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859545\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.105258                \tClf: 3.012220\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118800\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.851028\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.869840                \tClf: 2.777228\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119362\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842765\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.191923                \tClf: 3.099723\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119291\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855666\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.097143                \tClf: 3.004299\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119493\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835405\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.822199                \tClf: 2.730368\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119150\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.838872\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.174445                \tClf: 2.082440\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118933\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846172\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.938073                \tClf: 2.845704\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414954\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843140\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.009851                \tClf: 2.917633\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118773\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855017\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.910205                \tClf: 2.817393\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118712\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863316\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.183237                \tClf: 3.090011\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118998\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843310\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.813247                \tClf: 2.721021\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118890\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.864607\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.922138                \tClf: 2.828846\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119348\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850150\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.977050                \tClf: 2.884482\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119794\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837036\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.036561                \tClf: 2.944648\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118957\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845393\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.876036                \tClf: 2.783705\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119197\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859749\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.136489                \tClf: 3.043440\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118802\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849877\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.966198                \tClf: 2.873643\tReg: 0.000061\tFr_p: 0.109899\tFr_r: 0.119350\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842896\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.167859                \tClf: 3.075654\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119240\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855451\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.125988                \tClf: 3.033154\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119520\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835951\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.801530                \tClf: 2.709672\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119164\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.840055\tlr: 0.001000\ttrain acc:98.3250\tLoss: 2.239378                \tClf: 2.147314\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118927\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846708\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.958203                \tClf: 2.865807\tReg: 0.000061\tFr_p: 0.383532\tFr_r: 0.414972\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842956\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.875175                \tClf: 2.782967\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118781\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.855172\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.897222                \tClf: 2.804403\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118705\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863106\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.177188                \tClf: 3.083971\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118970\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843071\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.876359                \tClf: 2.784144\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118888\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865220\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.943713                \tClf: 2.850391\tReg: 0.000061\tFr_p: 0.109855\tFr_r: 0.119357\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850351\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.926189                \tClf: 2.833611\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119792\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837558\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.053927                \tClf: 2.961988\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118997\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.846021\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.788394                \tClf: 2.696032\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119185\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860097\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.121066                \tClf: 3.028000\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118801\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850261\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.944622                \tClf: 2.852048\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119378\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842865\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.208439                \tClf: 3.116235\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119291\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855816\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.073368                \tClf: 2.980517\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119517\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835034\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.872412                \tClf: 2.780599\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.119143\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839669\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.251876                \tClf: 2.159831\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845894\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.968125                \tClf: 2.875769\tReg: 0.000061\tFr_p: 0.383519\tFr_r: 0.414935\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843511\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.996266                \tClf: 2.904029\tReg: 0.000061\tFr_p: 0.109569\tFr_r: 0.118762\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855021\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.985467                \tClf: 2.892655\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118727\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862788\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.235843                \tClf: 3.142643\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119002\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842583\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.860766                \tClf: 2.768576\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118869\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864826\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.967738                \tClf: 2.874436\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119349\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.849860\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.972858                \tClf: 2.880304\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119812\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838227\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.029510                \tClf: 2.937537\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118983\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.844970\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.810268                \tClf: 2.717958\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119198\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860014\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.065227                \tClf: 2.972165\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118820\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849943\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.958363                \tClf: 2.865805\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119396\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842320\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.176640                \tClf: 3.084463\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119286\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855993\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.068051                \tClf: 2.975190\tReg: 0.000061\tFr_p: 0.109787\tFr_r: 0.119496\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835179\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.924340                \tClf: 2.832520\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119167\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839645\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.188211                \tClf: 2.096168\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118926\n",
      "\n",
      "Test set: Average loss: 0.0987, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845797\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.932951                \tClf: 2.840601\tReg: 0.000061\tFr_p: 0.383544\tFr_r: 0.414939\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843513\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.931980                \tClf: 2.839743\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118777\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855325\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.959554                \tClf: 2.866726\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118711\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863210\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.203945                \tClf: 3.110723\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118987\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843749\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.847725                \tClf: 2.755476\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118914\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865119\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.941647                \tClf: 2.848330\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119339\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850334\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.005620                \tClf: 2.913042\tReg: 0.000061\tFr_p: 0.110037\tFr_r: 0.119817\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837598\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.972214                \tClf: 2.880273\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118969\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845604\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.812896                \tClf: 2.720555\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119195\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860230\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.106769                \tClf: 3.013696\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118815\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850713\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.931291                \tClf: 2.838695\tReg: 0.000061\tFr_p: 0.109895\tFr_r: 0.119383\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842860\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.172167                \tClf: 3.079963\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119301\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855692\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.101380                \tClf: 3.008535\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119522\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835135\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.854933                \tClf: 2.763115\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119150\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839928\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.160032                \tClf: 2.067975\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845865\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.949054                \tClf: 2.856700\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414934\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843231\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.005847                \tClf: 2.913624\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118775\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854902\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.870460                \tClf: 2.777654\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118684\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863089\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.202140                \tClf: 3.108925\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118978\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842901\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.890951                \tClf: 2.798745\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118884\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865397\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.961545                \tClf: 2.868214\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119367\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850422\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.902240                \tClf: 2.809658\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119801\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837131\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.049450                \tClf: 2.957532\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118973\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845646\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.842637                \tClf: 2.750293\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119188\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860025\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.042625                \tClf: 2.949563\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118803\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850815\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.947686                \tClf: 2.855084\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119374\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842746\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.185851                \tClf: 3.093652\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.119283\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855598\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.114169                \tClf: 3.021328\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119529\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834754\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.916712                \tClf: 2.824913\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119143\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839192\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.192395                \tClf: 2.100375\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118884\n",
      "\n",
      "Test set: Average loss: 0.0927, Accuracy: 9740/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845995\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.991114                \tClf: 2.898753\tReg: 0.000061\tFr_p: 0.383546\tFr_r: 0.414957\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843136\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.951722                \tClf: 2.859504\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118781\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855225\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.906013                \tClf: 2.813190\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118717\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862715\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.170392                \tClf: 3.077196\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119006\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843322\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.808229                \tClf: 2.716002\tReg: 0.000061\tFr_p: 0.109674\tFr_r: 0.118901\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865425\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.007388                \tClf: 2.914055\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119370\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850609\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.953958                \tClf: 2.861367\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119814\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837647\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.000293                \tClf: 2.908350\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118973\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844713\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.823229                \tClf: 2.730932\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119186\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859850\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.090203                \tClf: 2.997149\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118794\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849789\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.920060                \tClf: 2.827510\tReg: 0.000061\tFr_p: 0.109904\tFr_r: 0.119376\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.843005\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.206467                \tClf: 3.114255\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119263\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855759\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.154429                \tClf: 3.061579\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119523\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835742\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.797152                \tClf: 2.705304\tReg: 0.000061\tFr_p: 0.109780\tFr_r: 0.119165\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839942\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.158674                \tClf: 2.066616\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118901\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9742/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.845528\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.978883                \tClf: 2.886546\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414931\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843393\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.961347                \tClf: 2.869116\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118797\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854438\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.935349                \tClf: 2.842566\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118700\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863040\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.196428                \tClf: 3.103215\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118978\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843014\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.827670                \tClf: 2.735458\tReg: 0.000061\tFr_p: 0.109658\tFr_r: 0.118863\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865072\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.976158                \tClf: 2.882843\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119355\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849651\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.980011                \tClf: 2.887467\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119808\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838210\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.005134                \tClf: 2.913163\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118984\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844913\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.839574                \tClf: 2.747267\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119202\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860057\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.131082                \tClf: 3.038018\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118818\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849861\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.996417                \tClf: 2.903863\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119373\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842309\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.210098                \tClf: 3.117922\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119278\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855680\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.204060                \tClf: 3.111215\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119501\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835020\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.918797                \tClf: 2.826985\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119159\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839310\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.219315                \tClf: 2.127288\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118904\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.847070\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.977879                \tClf: 2.885465\tReg: 0.000061\tFr_p: 0.383560\tFr_r: 0.414956\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.842918\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.984835                \tClf: 2.892628\tReg: 0.000061\tFr_p: 0.109586\tFr_r: 0.118774\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854646\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.945168                \tClf: 2.852374\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118707\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862725\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.231776                \tClf: 3.138578\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118993\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843469\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.850783                \tClf: 2.758548\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118912\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865569\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.988477                \tClf: 2.895138\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119315\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849890\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.963679                \tClf: 2.871124\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119811\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.838715\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.015952                \tClf: 2.923955\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118967\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844985\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.807145                \tClf: 2.714835\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119172\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859584\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.192966                \tClf: 3.099926\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118817\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850268\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.972588                \tClf: 2.880013\tReg: 0.000061\tFr_p: 0.109892\tFr_r: 0.119369\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842539\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.251984                \tClf: 3.159796\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119276\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855996\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.081684                \tClf: 2.988823\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119488\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.836007\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.845120                \tClf: 2.753259\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.119183\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839433\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.143799                \tClf: 2.051766\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846099\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.965373                \tClf: 2.873007\tReg: 0.000061\tFr_p: 0.383545\tFr_r: 0.414954\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843130\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.920149                \tClf: 2.827932\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118798\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854954\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.959255                \tClf: 2.866446\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118703\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863112\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.193864                \tClf: 3.100647\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118975\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843293\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.859990                \tClf: 2.767764\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118908\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864956\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.889542                \tClf: 2.796233\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119361\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850953\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.939909                \tClf: 2.847300\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119836\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837807\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.038863                \tClf: 2.946911\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118971\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844954\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.814417                \tClf: 2.722108\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119192\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860347\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.100440                \tClf: 3.007362\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118827\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849881\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.883160                \tClf: 2.790605\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119352\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842768\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.139559                \tClf: 3.047360\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119249\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.856094\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.104907                \tClf: 3.012041\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119515\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835217\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.806868                \tClf: 2.715046\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119153\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839491\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.221594                \tClf: 2.129558\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0962, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846274\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.005067                \tClf: 2.912693\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414936\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.844098\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.932968                \tClf: 2.840702\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118796\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854977\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.878007                \tClf: 2.785197\tReg: 0.000061\tFr_p: 0.109686\tFr_r: 0.118695\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863166\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.200039                \tClf: 3.106819\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119004\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842933\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.880072                \tClf: 2.787864\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118891\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865329\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.986639                \tClf: 2.893311\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119365\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850141\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.949315                \tClf: 2.856746\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119839\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.838114\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.018375                \tClf: 2.926408\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118973\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845558\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.821721                \tClf: 2.729382\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119205\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860216\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.057524                \tClf: 2.964452\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118819\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850128\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.005034                \tClf: 2.912467\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119350\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843121\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.219444                \tClf: 3.127227\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119284\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855575\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.062959                \tClf: 2.970119\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119517\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835434\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.822450                \tClf: 2.730618\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119172\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839373\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.202883                \tClf: 2.110853\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.118922\n",
      "\n",
      "Test set: Average loss: 0.0946, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845942\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.996795                \tClf: 2.904437\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414942\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843242\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.917840                \tClf: 2.825617\tReg: 0.000061\tFr_p: 0.109581\tFr_r: 0.118784\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855168\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.902745                \tClf: 2.809926\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118750\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862689\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.147389                \tClf: 3.054193\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119026\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842880\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.849126                \tClf: 2.756921\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118888\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865289\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.973610                \tClf: 2.880284\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119367\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850815\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.982728                \tClf: 2.890126\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119853\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837597\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.083804                \tClf: 2.991863\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118963\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844334\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.855605                \tClf: 2.763327\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119162\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859795\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.100607                \tClf: 3.007556\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118819\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849585\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.902817                \tClf: 2.810277\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119358\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842967\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.132509                \tClf: 3.040300\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119301\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.856045\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.032690                \tClf: 2.939826\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119519\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834660\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.858860                \tClf: 2.767066\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119172\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839416\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.238786                \tClf: 2.146754\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118945\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9736/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846579\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.884981                \tClf: 2.792591\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414942\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843386\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.981144                \tClf: 2.888914\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118790\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855303\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.916582                \tClf: 2.823755\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118710\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862939\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.226685                \tClf: 3.133477\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119016\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843581\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.873952                \tClf: 2.781712\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118910\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865069\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.957384                \tClf: 2.864069\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119349\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850604\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.896077                \tClf: 2.803486\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119802\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.838519\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.004428                \tClf: 2.912441\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118968\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844626\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.813386                \tClf: 2.721094\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119181\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860495\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.117381                \tClf: 3.024295\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118802\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850064\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.926388                \tClf: 2.833824\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119378\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842915\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.235106                \tClf: 3.142899\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119265\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855350\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.076663                \tClf: 2.983834\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119508\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.834955\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.798928                \tClf: 2.707119\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119165\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839822\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.234114                \tClf: 2.142062\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118891\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846467\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.986271                \tClf: 2.893887\tReg: 0.000061\tFr_p: 0.383544\tFr_r: 0.414938\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842484\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.951983                \tClf: 2.859798\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118751\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854754\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.949970                \tClf: 2.857172\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118701\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863282\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.164262                \tClf: 3.071037\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.119026\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842974\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.820156                \tClf: 2.727946\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118896\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864783\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.965571                \tClf: 2.872271\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119352\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850152\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.999459                \tClf: 2.906890\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119810\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837664\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.979222                \tClf: 2.887278\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118958\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845054\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.848377                \tClf: 2.756063\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119185\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860107\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.076616                \tClf: 2.983549\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118806\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849930\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.941676                \tClf: 2.849118\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119355\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.843251\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.183374                \tClf: 3.091150\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119291\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855749\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.090038                \tClf: 2.997189\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119530\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835575\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.859082                \tClf: 2.767242\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119176\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839794\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.185895                \tClf: 2.093844\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846811\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.010811                \tClf: 2.918409\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414955\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843161\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.013112                \tClf: 2.920893\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118782\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854536\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.925945                \tClf: 2.833157\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118726\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863488\tlr: 0.001000\ttrain acc:96.8500\tLoss: 3.172890                \tClf: 3.079654\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119007\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843266\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.853877                \tClf: 2.761653\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118892\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864746\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.002676                \tClf: 2.909378\tReg: 0.000061\tFr_p: 0.109850\tFr_r: 0.119353\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850369\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.976831                \tClf: 2.884252\tReg: 0.000061\tFr_p: 0.110040\tFr_r: 0.119828\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837048\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.096408                \tClf: 3.004495\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118953\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844588\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.810565                \tClf: 2.718274\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119189\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859709\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.088784                \tClf: 2.995738\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118823\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850119\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.853741                \tClf: 2.761174\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119347\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843140\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.206492                \tClf: 3.114274\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119281\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.856198\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.134911                \tClf: 3.042040\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119516\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835420\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.868179                \tClf: 2.776347\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119167\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839566\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.218585                \tClf: 2.126546\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118880\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846275\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.967550                \tClf: 2.875175\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414953\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843197\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.015184                \tClf: 2.922964\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118782\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855044\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.940915                \tClf: 2.848102\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118693\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862937\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.188532                \tClf: 3.095324\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118962\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842601\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.855214                \tClf: 2.763022\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118856\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865140\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.939141                \tClf: 2.845823\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119312\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850056\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.009812                \tClf: 2.917249\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119805\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838275\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.034868                \tClf: 2.942894\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118971\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845519\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.861635                \tClf: 2.769298\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119188\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.859485\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.081453                \tClf: 2.988418\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118830\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849608\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.914880                \tClf: 2.822339\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119357\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842350\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.128941                \tClf: 3.036762\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119258\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855745\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.084557                \tClf: 2.991709\tReg: 0.000061\tFr_p: 0.109789\tFr_r: 0.119494\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.834813\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.863493                \tClf: 2.771691\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119182\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839471\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.232827                \tClf: 2.140792\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118904\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845577\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.008001                \tClf: 2.915661\tReg: 0.000061\tFr_p: 0.383546\tFr_r: 0.414960\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843029\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.942446                \tClf: 2.850233\tReg: 0.000061\tFr_p: 0.109597\tFr_r: 0.118799\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854805\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.951904                \tClf: 2.859102\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118699\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863097\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.292893                \tClf: 3.199678\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118992\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842438\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.834207                \tClf: 2.742024\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118893\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865451\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.980466                \tClf: 2.887132\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119377\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850294\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.928461                \tClf: 2.835885\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119816\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838079\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.017802                \tClf: 2.925837\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118977\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845442\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.840773                \tClf: 2.748440\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119199\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860168\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.110490                \tClf: 3.017420\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118817\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850248\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.938826                \tClf: 2.846252\tReg: 0.000061\tFr_p: 0.109889\tFr_r: 0.119362\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842991\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.207231                \tClf: 3.115021\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119268\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855042\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.245123                \tClf: 3.152309\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119501\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835384\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.819952                \tClf: 2.728122\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119179\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839378\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.225337                \tClf: 2.133307\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118873\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846203\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.002905                \tClf: 2.910534\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414966\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843087\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.007416                \tClf: 2.915200\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118781\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.855522\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.972136                \tClf: 2.879299\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118712\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863580\tlr: 0.001000\ttrain acc:96.8500\tLoss: 3.327674                \tClf: 3.234434\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118997\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842599\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.834508                \tClf: 2.742317\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118881\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865326\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.922580                \tClf: 2.829252\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119334\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.849924\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.967315                \tClf: 2.874758\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119809\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838031\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.975144                \tClf: 2.883181\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118956\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844843\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.816681                \tClf: 2.724378\tReg: 0.000061\tFr_p: 0.109848\tFr_r: 0.119203\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860147\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.126103                \tClf: 3.033034\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118826\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850155\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.963960                \tClf: 2.871391\tReg: 0.000061\tFr_p: 0.109913\tFr_r: 0.119365\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842709\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.225406                \tClf: 3.133210\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119264\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855117\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.037358                \tClf: 2.944541\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119508\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834698\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.831389                \tClf: 2.739593\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119145\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.838827\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.202003                \tClf: 2.110001\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.846387\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.016064                \tClf: 2.923683\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414911\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842385\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.982257                \tClf: 2.890076\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118773\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854884\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.949584                \tClf: 2.856778\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118715\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863173\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.210329                \tClf: 3.117109\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118997\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843137\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.851841                \tClf: 2.759623\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118911\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865458\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.967412                \tClf: 2.874078\tReg: 0.000061\tFr_p: 0.109874\tFr_r: 0.119359\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.849512\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.017742                \tClf: 2.925206\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119811\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838231\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.017864                \tClf: 2.925891\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118973\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845479\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.864006                \tClf: 2.771670\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119210\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860382\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.138744                \tClf: 3.045663\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118829\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849734\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.907504                \tClf: 2.814956\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119329\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842270\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.153002                \tClf: 3.060828\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119263\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855499\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.119455                \tClf: 3.026619\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119537\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835224\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.891945                \tClf: 2.800123\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119183\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839565\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.261638                \tClf: 2.169598\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118883\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845658\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.957209                \tClf: 2.864865\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414967\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843554\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.968405                \tClf: 2.876166\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118784\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855134\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.962848                \tClf: 2.870030\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118713\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863678\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.193642                \tClf: 3.100397\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119002\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843358\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.840103                \tClf: 2.747874\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118904\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865329\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.019261                \tClf: 2.925933\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119360\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850182\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.973391                \tClf: 2.880820\tReg: 0.000061\tFr_p: 0.110057\tFr_r: 0.119816\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837383\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.041093                \tClf: 2.949162\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118989\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845647\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.835701                \tClf: 2.743358\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119187\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860009\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.033294                \tClf: 2.940233\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118828\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849972\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.926731                \tClf: 2.834171\tReg: 0.000061\tFr_p: 0.109901\tFr_r: 0.119390\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842746\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.190075                \tClf: 3.097877\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.119267\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855270\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.123973                \tClf: 3.031148\tReg: 0.000061\tFr_p: 0.109805\tFr_r: 0.119533\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835056\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.851223                \tClf: 2.759409\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.119166\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839429\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.144237                \tClf: 2.052204\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118898\n",
      "\n",
      "Test set: Average loss: 0.0971, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845703\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.026340                \tClf: 2.933994\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414937\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843146\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.006949                \tClf: 2.914731\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118760\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854848\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.902679                \tClf: 2.809876\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118708\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862850\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.209315                \tClf: 3.116112\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.119020\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843339\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.836761                \tClf: 2.744533\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118874\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864243\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.968509                \tClf: 2.875236\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119346\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850019\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.967065                \tClf: 2.874503\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119784\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838179\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.027257                \tClf: 2.935287\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118977\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845703\tlr: 0.001000\ttrain acc:97.9250\tLoss: 2.850090                \tClf: 2.757744\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119170\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859778\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.197830                \tClf: 3.104780\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118819\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850231\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.941088                \tClf: 2.848516\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119360\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842647\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.232122                \tClf: 3.139928\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119268\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855784\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.154451                \tClf: 3.061601\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119489\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835803\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.839365                \tClf: 2.747514\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119167\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839774\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.220008                \tClf: 2.127958\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118891\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845568\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.953934                \tClf: 2.861595\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414961\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843145\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.948894                \tClf: 2.856676\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118777\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854408\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.873074                \tClf: 2.780293\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118693\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862497\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.206719                \tClf: 3.113533\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118985\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843101\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.843298                \tClf: 2.751082\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118894\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865434\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.960292                \tClf: 2.866959\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119363\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850339\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.990311                \tClf: 2.897733\tReg: 0.000061\tFr_p: 0.109998\tFr_r: 0.119777\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837654\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.964884                \tClf: 2.872941\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118954\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845579\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.812671                \tClf: 2.720331\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119217\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.860447\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.149411                \tClf: 3.056327\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118827\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849880\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.960386                \tClf: 2.867831\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119346\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842892\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.247623                \tClf: 3.155417\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119289\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855633\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.196024                \tClf: 3.103181\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119513\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835434\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.863263                \tClf: 2.771430\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119153\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839991\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.219016                \tClf: 2.126955\tReg: 0.000061\tFr_p: 0.109784\tFr_r: 0.118941\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9716/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846476\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.942883                \tClf: 2.850498\tReg: 0.000061\tFr_p: 0.383546\tFr_r: 0.414952\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843221\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.980913                \tClf: 2.888691\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118808\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854943\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.982991                \tClf: 2.890183\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118704\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.863444\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.221294                \tClf: 3.128060\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118973\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842784\tlr: 0.001000\ttrain acc:98.0250\tLoss: 2.854128                \tClf: 2.761928\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118874\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.865169\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.922350                \tClf: 2.829031\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119364\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850496\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.951388                \tClf: 2.858802\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119794\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837312\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.064614                \tClf: 2.972687\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118980\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845400\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.784641                \tClf: 2.692310\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119203\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859955\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.122674                \tClf: 3.029615\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118807\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850243\tlr: 0.001000\ttrain acc:97.0500\tLoss: 2.945966                \tClf: 2.853393\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119355\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842435\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.126889                \tClf: 3.034706\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119264\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855698\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.152416                \tClf: 3.059570\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119500\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835363\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.846305                \tClf: 2.754476\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119185\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839508\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.211905                \tClf: 2.119868\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0940, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.845577\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.981088                \tClf: 2.888748\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414945\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843968\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.968923                \tClf: 2.876663\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118803\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854910\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.967489                \tClf: 2.874682\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118731\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862651\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.208577                \tClf: 3.115384\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118979\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842892\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.834454                \tClf: 2.742248\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118882\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.864748\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.903141                \tClf: 2.809842\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119360\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849998\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.961992                \tClf: 2.869431\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119826\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837816\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.074167                \tClf: 2.982215\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118993\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845694\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.789979                \tClf: 2.697633\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119199\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860012\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.125296                \tClf: 3.032234\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118810\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850258\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.949119                \tClf: 2.856545\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119350\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842525\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.179639                \tClf: 3.087452\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119286\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855575\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.075835                \tClf: 2.982995\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119528\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834931\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.845545                \tClf: 2.753737\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119176\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.838954\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.171448                \tClf: 2.079440\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118874\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845531\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.970621                \tClf: 2.878283\tReg: 0.000061\tFr_p: 0.383517\tFr_r: 0.414900\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843263\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.942602                \tClf: 2.850378\tReg: 0.000061\tFr_p: 0.109595\tFr_r: 0.118777\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854863\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.913240                \tClf: 2.820436\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118721\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862802\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.173102                \tClf: 3.079901\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119000\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843723\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.847055                \tClf: 2.754808\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118888\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864497\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.867384                \tClf: 2.774098\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119372\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850374\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.944167                \tClf: 2.851587\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119847\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837746\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.029962                \tClf: 2.938014\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118954\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845199\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.911137                \tClf: 2.818816\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119212\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.859644\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.125068                \tClf: 3.032024\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118791\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849778\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.897995                \tClf: 2.805445\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119368\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.843115\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.161561                \tClf: 3.069344\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119267\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855513\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.162024                \tClf: 3.069187\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119513\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835361\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.870989                \tClf: 2.779160\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119162\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839704\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.170882                \tClf: 2.078835\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118913\n",
      "\n",
      "Test set: Average loss: 0.0970, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846490\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.972216                \tClf: 2.879831\tReg: 0.000061\tFr_p: 0.383558\tFr_r: 0.414941\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843596\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.954122                \tClf: 2.861882\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118793\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.856165\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.924673                \tClf: 2.831803\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118738\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863545\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.234041                \tClf: 3.140803\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118995\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842874\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.871309                \tClf: 2.779105\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118885\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865468\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.932618                \tClf: 2.839284\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119352\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850657\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.982451                \tClf: 2.889857\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119816\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837364\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.022936                \tClf: 2.931006\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118958\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845102\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.763520                \tClf: 2.671204\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119202\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859377\tlr: 0.001000\ttrain acc:96.7750\tLoss: 3.157727                \tClf: 3.064697\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.118818\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849922\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.856709                \tClf: 2.764152\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119360\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843200\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.211700                \tClf: 3.119478\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119293\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.856064\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.079738                \tClf: 2.986874\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119498\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835292\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.877932                \tClf: 2.786106\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119159\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839340\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.218945                \tClf: 2.126917\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118940\n",
      "\n",
      "Test set: Average loss: 0.0965, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846348\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.030166                \tClf: 2.937788\tReg: 0.000061\tFr_p: 0.383528\tFr_r: 0.414948\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.842983\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.987759                \tClf: 2.895548\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118797\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854869\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.882541                \tClf: 2.789737\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118709\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862941\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.165299                \tClf: 3.072091\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119042\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842932\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.870266                \tClf: 2.778059\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118883\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865328\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.951887                \tClf: 2.858560\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119371\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850511\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.988462                \tClf: 2.895875\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119805\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837389\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.987721                \tClf: 2.895791\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118952\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.844690\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.794704                \tClf: 2.702409\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119156\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860654\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.139367                \tClf: 3.046273\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118821\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850525\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.949810                \tClf: 2.857223\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119362\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842801\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.190887                \tClf: 3.098686\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.119292\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855120\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.045008                \tClf: 2.952191\tReg: 0.000061\tFr_p: 0.109793\tFr_r: 0.119500\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835265\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.835658                \tClf: 2.743834\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119171\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839230\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.189753                \tClf: 2.097730\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118905\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846029\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.004029                \tClf: 2.911667\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414929\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843826\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.919988                \tClf: 2.827736\tReg: 0.000061\tFr_p: 0.109608\tFr_r: 0.118827\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854945\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.946029                \tClf: 2.853220\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118689\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862820\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.188533                \tClf: 3.095331\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118986\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842373\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.841873                \tClf: 2.749694\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118879\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865224\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.992825                \tClf: 2.899503\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119364\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850258\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.943730                \tClf: 2.851156\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119821\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837541\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.011796                \tClf: 2.919858\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118942\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844875\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.842388                \tClf: 2.750083\tReg: 0.000061\tFr_p: 0.109837\tFr_r: 0.119226\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860800\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.110454                \tClf: 3.017353\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118816\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.851022\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.917386                \tClf: 2.824774\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119354\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842635\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.188996                \tClf: 3.096804\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119266\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.854671\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.167899                \tClf: 3.075104\tReg: 0.000061\tFr_p: 0.109794\tFr_r: 0.119503\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835179\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.783183                \tClf: 2.691363\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119159\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840072\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.187091                \tClf: 2.095026\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118885\n",
      "\n",
      "Test set: Average loss: 0.0960, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846226\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.973936                \tClf: 2.881564\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414934\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843293\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.982143                \tClf: 2.889917\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118769\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855097\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.022505                \tClf: 2.929689\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118701\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862898\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.198713                \tClf: 3.105507\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118969\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842388\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.808435                \tClf: 2.716255\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118893\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865504\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.906782                \tClf: 2.813445\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119361\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850161\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.978366                \tClf: 2.885797\tReg: 0.000061\tFr_p: 0.110048\tFr_r: 0.119796\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.838478\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.026427                \tClf: 2.934442\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118976\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.844948\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.843680                \tClf: 2.751372\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119195\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860228\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.140889                \tClf: 3.047816\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118814\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850285\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.884222                \tClf: 2.791647\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119353\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842387\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.132578                \tClf: 3.040397\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.119256\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855522\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.109989                \tClf: 3.017152\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119519\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835506\tlr: 0.001000\ttrain acc:97.9500\tLoss: 2.810609                \tClf: 2.718773\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.119184\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839836\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.209166                \tClf: 2.117113\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.845339\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.971740                \tClf: 2.879412\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414957\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843522\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.929771                \tClf: 2.837534\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118785\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854787\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.942996                \tClf: 2.850196\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118743\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862771\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.152363                \tClf: 3.059163\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118995\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842768\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.839414                \tClf: 2.747214\tReg: 0.000061\tFr_p: 0.109678\tFr_r: 0.118867\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865137\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.971609                \tClf: 2.878291\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119348\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850839\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.927514                \tClf: 2.834911\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119816\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837909\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.012140                \tClf: 2.920183\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118986\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845716\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.781351                \tClf: 2.689004\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119184\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859552\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.066171                \tClf: 2.973133\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118816\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850301\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.900191                \tClf: 2.807615\tReg: 0.000061\tFr_p: 0.109900\tFr_r: 0.119372\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842422\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.193141                \tClf: 3.100959\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.119281\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855297\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.094588                \tClf: 3.001762\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119515\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835559\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.876815                \tClf: 2.784976\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.119176\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839712\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.180605                \tClf: 2.088559\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845816\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.925125                \tClf: 2.832773\tReg: 0.000061\tFr_p: 0.383564\tFr_r: 0.414939\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843433\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.967977                \tClf: 2.875744\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118780\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854580\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.920657                \tClf: 2.827867\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118690\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863386\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.189194                \tClf: 3.095963\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119017\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.842598\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.867824                \tClf: 2.775633\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118880\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.866001\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.949364                \tClf: 2.856003\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119352\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850070\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.933331                \tClf: 2.840766\tReg: 0.000061\tFr_p: 0.110052\tFr_r: 0.119821\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837590\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.013999                \tClf: 2.922059\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118969\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844624\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.817757                \tClf: 2.725464\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119206\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860065\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.128077                \tClf: 3.035012\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118803\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849771\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.936314                \tClf: 2.843765\tReg: 0.000061\tFr_p: 0.109877\tFr_r: 0.119347\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842565\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.209019                \tClf: 3.116829\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119279\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855828\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.085309                \tClf: 2.992456\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119526\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835421\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.847377                \tClf: 2.755545\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.119183\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.840151\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.172746                \tClf: 2.080678\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.118919\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845673\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.925502                \tClf: 2.833157\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414943\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843039\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.964906                \tClf: 2.872693\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118764\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855452\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.943794                \tClf: 2.850960\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118725\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863036\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.183282                \tClf: 3.090069\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118989\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843024\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.798794                \tClf: 2.706581\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118918\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865038\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.927534                \tClf: 2.834220\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119332\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850895\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.966449                \tClf: 2.873843\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119829\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837296\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.018049                \tClf: 2.926123\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118994\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845365\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.798866                \tClf: 2.706537\tReg: 0.000061\tFr_p: 0.109840\tFr_r: 0.119219\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860305\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.139768                \tClf: 3.046691\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118823\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849784\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.954813                \tClf: 2.862263\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119363\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842517\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.155605                \tClf: 3.063418\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.119266\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855159\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.070533                \tClf: 2.977714\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119512\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835672\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.815848                \tClf: 2.724003\tReg: 0.000061\tFr_p: 0.109785\tFr_r: 0.119173\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839331\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.181777                \tClf: 2.089749\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846446\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.968054                \tClf: 2.875671\tReg: 0.000061\tFr_p: 0.383540\tFr_r: 0.414955\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842962\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.976254                \tClf: 2.884045\tReg: 0.000061\tFr_p: 0.109571\tFr_r: 0.118777\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854677\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.870847                \tClf: 2.778052\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118700\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863135\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.177518                \tClf: 3.084300\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118983\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842833\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.866115                \tClf: 2.773913\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118863\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864203\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.942522                \tClf: 2.849250\tReg: 0.000061\tFr_p: 0.109844\tFr_r: 0.119345\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850461\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.003495                \tClf: 2.910911\tReg: 0.000061\tFr_p: 0.110038\tFr_r: 0.119796\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838211\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.022688                \tClf: 2.930717\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118965\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845525\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.838233                \tClf: 2.745896\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119206\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859912\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.114369                \tClf: 3.021312\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118807\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849853\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.950144                \tClf: 2.857591\tReg: 0.000061\tFr_p: 0.109895\tFr_r: 0.119380\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842289\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.191996                \tClf: 3.099821\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119284\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855223\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.088432                \tClf: 2.995610\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119500\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834943\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.866542                \tClf: 2.774734\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119150\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839654\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.228433                \tClf: 2.136389\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845963\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.926052                \tClf: 2.833693\tReg: 0.000061\tFr_p: 0.383521\tFr_r: 0.414946\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842978\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.944434                \tClf: 2.852224\tReg: 0.000061\tFr_p: 0.109577\tFr_r: 0.118762\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854238\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.943130                \tClf: 2.850357\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118677\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862326\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.223439                \tClf: 3.130261\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118969\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842783\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.878494                \tClf: 2.786294\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.118900\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.864939\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.964660                \tClf: 2.871352\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119352\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850004\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.927718                \tClf: 2.835156\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119816\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837742\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.954643                \tClf: 2.862695\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118950\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845239\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.799192                \tClf: 2.706869\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119217\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860428\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.084621                \tClf: 2.991538\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118843\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850688\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.903803                \tClf: 2.811207\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119376\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842637\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.195887                \tClf: 3.103694\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119250\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855997\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.156422                \tClf: 3.063561\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119520\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835310\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.873855                \tClf: 2.782028\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119174\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839447\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.204760                \tClf: 2.112726\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846242\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.970323                \tClf: 2.877950\tReg: 0.000061\tFr_p: 0.383529\tFr_r: 0.414931\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.843371\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.978084                \tClf: 2.885854\tReg: 0.000061\tFr_p: 0.109602\tFr_r: 0.118820\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854695\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.968522                \tClf: 2.875726\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118731\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863520\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.248804                \tClf: 3.155567\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119006\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.842527\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.811709                \tClf: 2.719521\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118881\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865934\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.954374                \tClf: 2.861016\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119334\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850446\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.048054                \tClf: 2.955471\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119822\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838164\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.055528                \tClf: 2.963559\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118999\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.845033\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.850723                \tClf: 2.758410\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119220\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860181\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.094840                \tClf: 3.001770\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118820\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850602\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.956317                \tClf: 2.863726\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119336\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842088\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.192756                \tClf: 3.100590\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119280\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855338\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.113860                \tClf: 3.021032\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119489\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835089\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.819731                \tClf: 2.727915\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119154\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.840470\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.226622                \tClf: 2.134537\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.118923\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846124\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.945020                \tClf: 2.852653\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414947\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843290\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.917274                \tClf: 2.825048\tReg: 0.000061\tFr_p: 0.109568\tFr_r: 0.118767\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.855359\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.936215                \tClf: 2.843386\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118713\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863049\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.259749                \tClf: 3.166536\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118979\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842959\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.893455                \tClf: 2.801246\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118895\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865387\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.003146                \tClf: 2.909816\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119355\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850107\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.924885                \tClf: 2.832319\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119821\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.838241\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.033796                \tClf: 2.941823\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.119003\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844163\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.868792                \tClf: 2.776522\tReg: 0.000061\tFr_p: 0.109841\tFr_r: 0.119207\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859499\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.161691                \tClf: 3.068655\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118794\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850487\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.928453                \tClf: 2.835868\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119350\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842347\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.174061                \tClf: 3.081883\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119256\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855214\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.125955                \tClf: 3.033133\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119501\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834944\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.847683                \tClf: 2.755875\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.119167\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839529\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.247762                \tClf: 2.155724\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0954, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845456\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.031007                \tClf: 2.938673\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414899\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842900\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.998559                \tClf: 2.906353\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118787\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854179\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.912287                \tClf: 2.819517\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118693\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862712\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.209822                \tClf: 3.116625\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118985\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842995\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.820912                \tClf: 2.728701\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118869\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864864\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.913383                \tClf: 2.820078\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119343\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850677\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.946813                \tClf: 2.854218\tReg: 0.000061\tFr_p: 0.110047\tFr_r: 0.119852\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837887\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.056186                \tClf: 2.964230\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118935\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844970\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.812234                \tClf: 2.719925\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119193\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859820\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.086468                \tClf: 2.993416\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118819\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849587\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.871350                \tClf: 2.778810\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119378\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842777\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.202857                \tClf: 3.110657\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119256\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855887\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.186343                \tClf: 3.093488\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119530\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835621\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.870396                \tClf: 2.778554\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119178\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839776\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.185539                \tClf: 2.093489\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846545\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.941942                \tClf: 2.849554\tReg: 0.000061\tFr_p: 0.383544\tFr_r: 0.414946\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842819\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.001656                \tClf: 2.909454\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118777\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855041\tlr: 0.001000\ttrain acc:97.7250\tLoss: 3.000313                \tClf: 2.907500\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118732\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862658\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.183076                \tClf: 3.089882\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118986\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843296\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.873793                \tClf: 2.781567\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118883\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864939\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.962955                \tClf: 2.869647\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119344\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.849875\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.917095                \tClf: 2.824540\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119821\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837754\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.093377                \tClf: 3.001428\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118977\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.845628\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.850476                \tClf: 2.758134\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119178\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.859824\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.145792                \tClf: 3.052740\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118798\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850239\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.882357                \tClf: 2.789784\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119350\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842586\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.205066                \tClf: 3.112876\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119262\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855344\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.106845                \tClf: 3.014016\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119495\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834996\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.815388                \tClf: 2.723577\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.119183\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839076\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.193578                \tClf: 2.101563\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846996\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.971448                \tClf: 2.879037\tReg: 0.000061\tFr_p: 0.383564\tFr_r: 0.414953\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843023\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.009034                \tClf: 2.916821\tReg: 0.000061\tFr_p: 0.109589\tFr_r: 0.118785\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.854771\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.922861                \tClf: 2.830061\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118712\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863282\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.197862                \tClf: 3.104636\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.119020\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842677\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.871590                \tClf: 2.779395\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118914\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865210\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.971103                \tClf: 2.877782\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119363\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849744\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.013565                \tClf: 2.921016\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119821\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837625\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.026306                \tClf: 2.934364\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.118955\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844978\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.753964                \tClf: 2.661654\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119226\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860125\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.084253                \tClf: 2.991186\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118797\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849832\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.877267                \tClf: 2.784715\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119320\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843659\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.212871                \tClf: 3.120627\tReg: 0.000061\tFr_p: 0.109729\tFr_r: 0.119268\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855686\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.146079                \tClf: 3.053234\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119470\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834734\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.814154                \tClf: 2.722356\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.119144\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839394\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.221130                \tClf: 2.129099\tReg: 0.000061\tFr_p: 0.109770\tFr_r: 0.118933\n",
      "\n",
      "Test set: Average loss: 0.0947, Accuracy: 9716/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845495\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.964247                \tClf: 2.871911\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414902\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843489\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.966489                \tClf: 2.874254\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118777\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854547\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.928420                \tClf: 2.835631\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118694\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863018\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.231535                \tClf: 3.138323\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118997\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843028\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.833261                \tClf: 2.741049\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118898\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.864498\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.898421                \tClf: 2.805135\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119343\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849789\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.889323                \tClf: 2.796772\tReg: 0.000061\tFr_p: 0.110049\tFr_r: 0.119841\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837720\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.047974                \tClf: 2.956027\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118954\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844747\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.817245                \tClf: 2.724947\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119212\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860443\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.090647                \tClf: 2.997563\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118817\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849741\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.979794                \tClf: 2.887246\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119386\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.843406\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.145370                \tClf: 3.053139\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119285\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855719\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.073005                \tClf: 2.980158\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119532\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834970\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.841316                \tClf: 2.749507\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119174\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839283\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.209896                \tClf: 2.117871\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118879\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9723/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846057\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.988815                \tClf: 2.896451\tReg: 0.000061\tFr_p: 0.383505\tFr_r: 0.414946\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842681\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.970739                \tClf: 2.878544\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118785\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855110\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.962512                \tClf: 2.869696\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118693\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.863169\tlr: 0.001000\ttrain acc:96.9250\tLoss: 3.209364                \tClf: 3.116144\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.119028\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843126\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.885891                \tClf: 2.793673\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118888\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.864376\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.932274                \tClf: 2.838994\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119357\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850572\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.952619                \tClf: 2.860030\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119830\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837956\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.013067                \tClf: 2.921109\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118966\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844982\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.829921                \tClf: 2.737611\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119204\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859635\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.082634                \tClf: 2.989591\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118808\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850630\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.941785                \tClf: 2.849193\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119362\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843280\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.189264                \tClf: 3.097039\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.119287\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855803\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.143130                \tClf: 3.050279\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119506\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835345\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.902485                \tClf: 2.810656\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119156\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839505\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.199071                \tClf: 2.107035\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118885\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846926\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.995698                \tClf: 2.903290\tReg: 0.000061\tFr_p: 0.383574\tFr_r: 0.414931\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843039\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.968331                \tClf: 2.876118\tReg: 0.000061\tFr_p: 0.109595\tFr_r: 0.118804\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855474\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.884859                \tClf: 2.792024\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118713\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862414\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.167400                \tClf: 3.074218\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118997\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.843014\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.816582                \tClf: 2.724370\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118898\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865811\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.974938                \tClf: 2.881586\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119332\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850512\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.956187                \tClf: 2.863600\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119805\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837301\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.012591                \tClf: 2.920665\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118955\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845109\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.878955                \tClf: 2.786639\tReg: 0.000061\tFr_p: 0.109828\tFr_r: 0.119183\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860287\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.118870                \tClf: 3.025795\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118840\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.850108\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.964595                \tClf: 2.872029\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119332\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843189\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.180092                \tClf: 3.087871\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119273\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855849\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.119471                \tClf: 3.026618\tReg: 0.000061\tFr_p: 0.109801\tFr_r: 0.119525\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834516\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.817724                \tClf: 2.725937\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.119149\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839016\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.232002                \tClf: 2.139991\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118893\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846353\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.988374                \tClf: 2.895996\tReg: 0.000061\tFr_p: 0.383564\tFr_r: 0.414977\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.842649\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.998270                \tClf: 2.906076\tReg: 0.000061\tFr_p: 0.109583\tFr_r: 0.118778\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854674\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.876712                \tClf: 2.783917\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118703\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863000\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.211849                \tClf: 3.118637\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118991\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843444\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.822036                \tClf: 2.729803\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118894\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.865118\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.959057                \tClf: 2.865740\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119351\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850335\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.987069                \tClf: 2.894491\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119818\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837763\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.047197                \tClf: 2.955248\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118973\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845406\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.795961                \tClf: 2.703629\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119195\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859797\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.161518                \tClf: 3.068467\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118835\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849833\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.905238                \tClf: 2.812686\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119347\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.843150\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.124842                \tClf: 3.032624\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119265\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855235\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.137555                \tClf: 3.044733\tReg: 0.000061\tFr_p: 0.109799\tFr_r: 0.119477\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835316\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.880349                \tClf: 2.788522\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.119135\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.840015\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.270456                \tClf: 2.178394\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118884\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846668\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.965500                \tClf: 2.873106\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414956\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842887\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.009082                \tClf: 2.916877\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118779\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854717\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.920021                \tClf: 2.827224\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118715\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863142\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.250072                \tClf: 3.156854\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118983\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842620\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.746327                \tClf: 2.654135\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118904\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864709\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.006592                \tClf: 2.913296\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119353\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.849605\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.910450                \tClf: 2.817908\tReg: 0.000061\tFr_p: 0.110013\tFr_r: 0.119806\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837456\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.050654                \tClf: 2.958720\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118964\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845035\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.844657                \tClf: 2.752344\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119208\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860217\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.075741                \tClf: 2.982669\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118844\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850246\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.908938                \tClf: 2.816365\tReg: 0.000061\tFr_p: 0.109895\tFr_r: 0.119336\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842281\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.204959                \tClf: 3.112784\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119284\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.856548\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.090475                \tClf: 2.997587\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119485\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834990\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.840076                \tClf: 2.748266\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.119149\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839256\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.166686                \tClf: 2.074662\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118889\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846070\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.002449                \tClf: 2.910084\tReg: 0.000061\tFr_p: 0.383521\tFr_r: 0.414930\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842752\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.919783                \tClf: 2.827584\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118758\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855694\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.922708                \tClf: 2.829863\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118699\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863124\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.288651                \tClf: 3.195434\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119004\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842528\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.860598                \tClf: 2.768411\tReg: 0.000061\tFr_p: 0.109681\tFr_r: 0.118905\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.864265\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.965225                \tClf: 2.871950\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119322\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.849858\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.971344                \tClf: 2.878790\tReg: 0.000061\tFr_p: 0.110017\tFr_r: 0.119792\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837308\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.006640                \tClf: 2.914714\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119010\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845219\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.859426                \tClf: 2.767104\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119192\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860377\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.079147                \tClf: 2.986067\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118803\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849729\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.892079                \tClf: 2.799531\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119310\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.843300\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.182140                \tClf: 3.089914\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119259\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.856253\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.068169                \tClf: 2.975296\tReg: 0.000061\tFr_p: 0.109812\tFr_r: 0.119535\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835511\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.856711                \tClf: 2.764875\tReg: 0.000061\tFr_p: 0.109769\tFr_r: 0.119167\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.840493\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.203128                \tClf: 2.111042\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118929\n",
      "\n",
      "Test set: Average loss: 0.0935, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845794\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.023323                \tClf: 2.930972\tReg: 0.000061\tFr_p: 0.383524\tFr_r: 0.414937\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842953\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.011077                \tClf: 2.918869\tReg: 0.000061\tFr_p: 0.109572\tFr_r: 0.118773\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854077\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.909767                \tClf: 2.817002\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118687\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863152\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.266333                \tClf: 3.173114\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118974\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843239\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.892452                \tClf: 2.800229\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118871\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864775\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.996787                \tClf: 2.903487\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119350\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850155\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.928455                \tClf: 2.835886\tReg: 0.000061\tFr_p: 0.110034\tFr_r: 0.119826\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837703\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.092083                \tClf: 3.000137\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118970\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844538\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.795408                \tClf: 2.703120\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119199\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860246\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.195422                \tClf: 3.102348\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118845\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850485\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.975385                \tClf: 2.882800\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119381\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842891\tlr: 0.001000\ttrain acc:96.8000\tLoss: 3.192029                \tClf: 3.099823\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119258\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.856417\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.138774                \tClf: 3.045892\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119527\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835007\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.844033                \tClf: 2.752221\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119134\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839451\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.221849                \tClf: 2.129815\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846497\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.951216                \tClf: 2.858830\tReg: 0.000061\tFr_p: 0.383527\tFr_r: 0.414927\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843184\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.953407                \tClf: 2.861187\tReg: 0.000061\tFr_p: 0.109584\tFr_r: 0.118793\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854571\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.902952                \tClf: 2.810162\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118687\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862947\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.187969                \tClf: 3.094761\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119000\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.843014\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.861715                \tClf: 2.769504\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118908\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865150\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.992934                \tClf: 2.899615\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119353\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850300\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.932324                \tClf: 2.839748\tReg: 0.000061\tFr_p: 0.110034\tFr_r: 0.119821\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.838068\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.027709                \tClf: 2.935744\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118972\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.845003\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.796795                \tClf: 2.704484\tReg: 0.000061\tFr_p: 0.109825\tFr_r: 0.119194\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.860028\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.104435                \tClf: 3.011372\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118836\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850408\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.948836                \tClf: 2.856254\tReg: 0.000061\tFr_p: 0.109877\tFr_r: 0.119373\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842469\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.218233                \tClf: 3.126048\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119248\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855234\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.094760                \tClf: 3.001938\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119511\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835489\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.885634                \tClf: 2.793799\tReg: 0.000061\tFr_p: 0.109787\tFr_r: 0.119161\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839580\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.224294                \tClf: 2.132254\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846000\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.976817                \tClf: 2.884456\tReg: 0.000061\tFr_p: 0.383526\tFr_r: 0.414923\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843139\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.967865                \tClf: 2.875647\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118784\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.855343\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.908930                \tClf: 2.816102\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118733\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863185\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.192518                \tClf: 3.099298\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118996\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842905\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.869055                \tClf: 2.776849\tReg: 0.000061\tFr_p: 0.109671\tFr_r: 0.118873\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.864726\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.976776                \tClf: 2.883479\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119353\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850133\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.984661                \tClf: 2.892093\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119787\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.836903\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.028926                \tClf: 2.937020\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118967\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845378\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.797056                \tClf: 2.704726\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119198\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.860170\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.066465                \tClf: 2.973395\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118811\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850337\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.967779                \tClf: 2.875201\tReg: 0.000061\tFr_p: 0.109903\tFr_r: 0.119401\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842313\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.171028                \tClf: 3.078851\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119278\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855173\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.106647                \tClf: 3.013827\tReg: 0.000061\tFr_p: 0.109814\tFr_r: 0.119524\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835854\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.871263                \tClf: 2.779409\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119164\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839249\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.218007                \tClf: 2.125984\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118920\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.847116\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.990414                \tClf: 2.897997\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414943\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842927\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.935744                \tClf: 2.843536\tReg: 0.000061\tFr_p: 0.109557\tFr_r: 0.118742\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.854961\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.911646                \tClf: 2.818837\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118710\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863093\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.241384                \tClf: 3.148169\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118991\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843064\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.839987                \tClf: 2.747773\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118858\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865061\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.948860                \tClf: 2.855546\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119339\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850558\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.996792                \tClf: 2.904203\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119802\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837989\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.011910                \tClf: 2.919950\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118970\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845130\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.832133                \tClf: 2.739815\tReg: 0.000061\tFr_p: 0.109842\tFr_r: 0.119190\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860062\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.171170                \tClf: 3.078105\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118844\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.850556\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.910988                \tClf: 2.818399\tReg: 0.000061\tFr_p: 0.109899\tFr_r: 0.119387\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842221\tlr: 0.001000\ttrain acc:96.9000\tLoss: 3.143176                \tClf: 3.051004\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119267\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.856160\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.134776                \tClf: 3.041907\tReg: 0.000061\tFr_p: 0.109830\tFr_r: 0.119517\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835283\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.862321                \tClf: 2.770496\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119155\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839110\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.195856                \tClf: 2.103839\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118886\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845721\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.940600                \tClf: 2.848253\tReg: 0.000061\tFr_p: 0.383525\tFr_r: 0.414918\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842888\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.993017                \tClf: 2.900811\tReg: 0.000061\tFr_p: 0.109568\tFr_r: 0.118768\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854356\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.950672                \tClf: 2.857893\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118704\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862605\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.182781                \tClf: 3.089590\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118996\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843043\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.834202                \tClf: 2.741988\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118901\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865166\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.931301                \tClf: 2.837982\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119377\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.850475\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.023173                \tClf: 2.930588\tReg: 0.000061\tFr_p: 0.110043\tFr_r: 0.119803\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837782\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.997072                \tClf: 2.905122\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118977\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844465\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.797476                \tClf: 2.705192\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119196\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860208\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.129355                \tClf: 3.036284\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118807\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.849802\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.921539                \tClf: 2.828988\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119351\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843574\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.224155                \tClf: 3.131915\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.119299\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855887\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.149333                \tClf: 3.056477\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119495\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.834167\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.801735                \tClf: 2.709966\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119133\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839486\tlr: 0.001000\ttrain acc:98.3500\tLoss: 2.162252                \tClf: 2.070216\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.118904\n",
      "\n",
      "Test set: Average loss: 0.0938, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845708\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.923860                \tClf: 2.831514\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414942\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843517\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.973124                \tClf: 2.880887\tReg: 0.000061\tFr_p: 0.109603\tFr_r: 0.118794\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.855199\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.881842                \tClf: 2.789021\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118722\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862507\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.216079                \tClf: 3.122893\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.119017\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842646\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.789455                \tClf: 2.697262\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118911\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865117\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.918443                \tClf: 2.825126\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119353\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850177\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.916785                \tClf: 2.824215\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119797\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837613\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.063755                \tClf: 2.971813\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118989\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.844633\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.763261                \tClf: 2.670968\tReg: 0.000061\tFr_p: 0.109800\tFr_r: 0.119174\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859568\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.087858                \tClf: 2.994819\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118805\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.849920\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.985829                \tClf: 2.893271\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119366\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842944\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.255086                \tClf: 3.162878\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.119261\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855642\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.119108                \tClf: 3.026265\tReg: 0.000061\tFr_p: 0.109791\tFr_r: 0.119485\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834732\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.878298                \tClf: 2.786501\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.119156\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.840446\tlr: 0.001000\ttrain acc:98.0500\tLoss: 2.220944                \tClf: 2.128860\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118920\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846010\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.998642                \tClf: 2.906281\tReg: 0.000061\tFr_p: 0.383563\tFr_r: 0.414957\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843225\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.929545                \tClf: 2.837323\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118761\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854519\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.915466                \tClf: 2.822679\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118709\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.861802\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.162144                \tClf: 3.068993\tReg: 0.000061\tFr_p: 0.109692\tFr_r: 0.118974\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.843311\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.861403                \tClf: 2.769176\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118923\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.864724\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.979079                \tClf: 2.885781\tReg: 0.000061\tFr_p: 0.109865\tFr_r: 0.119367\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850070\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.994620                \tClf: 2.902056\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119825\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837774\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.025373                \tClf: 2.933423\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118966\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845130\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.776424                \tClf: 2.684106\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119224\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859973\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.063557                \tClf: 2.970497\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118819\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850408\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.899076                \tClf: 2.806494\tReg: 0.000061\tFr_p: 0.109893\tFr_r: 0.119377\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842357\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.158867                \tClf: 3.066688\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.119269\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855174\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.095707                \tClf: 3.002887\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119514\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835724\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.809871                \tClf: 2.718024\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119190\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839549\tlr: 0.001000\ttrain acc:98.5250\tLoss: 2.207171                \tClf: 2.115132\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118879\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846175\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.942969                \tClf: 2.850599\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414917\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.843629\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.991537                \tClf: 2.899295\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118775\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.855510\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.962774                \tClf: 2.869937\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118734\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862730\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.182448                \tClf: 3.089250\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118973\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842978\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.801597                \tClf: 2.709387\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118925\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864857\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.012577                \tClf: 2.919273\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119376\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850595\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.890071                \tClf: 2.797481\tReg: 0.000061\tFr_p: 0.110033\tFr_r: 0.119819\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.838925\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.037118                \tClf: 2.945111\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118959\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844122\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.814729                \tClf: 2.722461\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119171\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860167\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.128359                \tClf: 3.035289\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118826\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850588\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.867170                \tClf: 2.774579\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119357\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842449\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.164424                \tClf: 3.072241\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119309\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855730\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.079071                \tClf: 2.986224\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119534\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.834879\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.824346                \tClf: 2.732541\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119169\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839104\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.181767                \tClf: 2.089751\tReg: 0.000061\tFr_p: 0.109725\tFr_r: 0.118879\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846357\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.024755                \tClf: 2.932376\tReg: 0.000061\tFr_p: 0.383550\tFr_r: 0.414946\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843024\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.954425                \tClf: 2.862213\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118785\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855539\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.900818                \tClf: 2.807980\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118735\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862969\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.193593                \tClf: 3.100384\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.118969\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843110\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.834730                \tClf: 2.742514\tReg: 0.000061\tFr_p: 0.109679\tFr_r: 0.118865\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865304\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.982690                \tClf: 2.889364\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119341\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849948\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.983734                \tClf: 2.891175\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119798\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837555\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.011614                \tClf: 2.919675\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.118967\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844487\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.812868                \tClf: 2.720583\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119179\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.860343\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.063457                \tClf: 2.970379\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118827\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850297\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.972462                \tClf: 2.879886\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119365\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842784\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.173451                \tClf: 3.081251\tReg: 0.000061\tFr_p: 0.109726\tFr_r: 0.119283\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855151\tlr: 0.001000\ttrain acc:97.7500\tLoss: 3.060008                \tClf: 2.967190\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119503\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.835060\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.869967                \tClf: 2.778153\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119153\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.839696\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.180702                \tClf: 2.088656\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118881\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.845907\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.971337                \tClf: 2.878981\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414951\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843300\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.987020                \tClf: 2.894794\tReg: 0.000061\tFr_p: 0.109607\tFr_r: 0.118777\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855447\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.899797                \tClf: 2.806963\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.118733\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862620\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.180257                \tClf: 3.087065\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118978\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842698\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.832423                \tClf: 2.740227\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118908\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865439\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.965471                \tClf: 2.872138\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119365\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850384\tlr: 0.001000\ttrain acc:97.1750\tLoss: 2.987452                \tClf: 2.894872\tReg: 0.000061\tFr_p: 0.110016\tFr_r: 0.119824\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.837655\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.957627                \tClf: 2.865683\tReg: 0.000061\tFr_p: 0.109738\tFr_r: 0.118984\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845130\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.794966                \tClf: 2.702649\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119196\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860405\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.192050                \tClf: 3.098969\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118840\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.849637\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.941173                \tClf: 2.848630\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119387\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842831\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.177661                \tClf: 3.085458\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119268\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855298\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.138558                \tClf: 3.045732\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119495\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835402\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.870317                \tClf: 2.778485\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.119197\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839500\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.146247                \tClf: 2.054211\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0943, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846050\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.000343                \tClf: 2.907979\tReg: 0.000061\tFr_p: 0.383565\tFr_r: 0.414958\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842915\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.980998                \tClf: 2.888791\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118768\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.855362\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.979289                \tClf: 2.886460\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118722\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862836\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.184846                \tClf: 3.091643\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118985\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.843079\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.734338                \tClf: 2.642123\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118917\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865237\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.961149                \tClf: 2.867826\tReg: 0.000061\tFr_p: 0.109851\tFr_r: 0.119345\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.849838\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.942306                \tClf: 2.849753\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119817\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837478\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.994803                \tClf: 2.902868\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118961\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844729\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.793760                \tClf: 2.701463\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119194\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859134\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.145541                \tClf: 3.052523\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118812\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850299\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.868703                \tClf: 2.776127\tReg: 0.000061\tFr_p: 0.109900\tFr_r: 0.119397\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842737\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.212530                \tClf: 3.120332\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119264\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855493\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.102940                \tClf: 3.010104\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119499\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835491\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.879362                \tClf: 2.787526\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.119187\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839790\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.279293                \tClf: 2.187242\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118911\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846314\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.993169                \tClf: 2.900792\tReg: 0.000061\tFr_p: 0.383554\tFr_r: 0.414964\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843103\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.977060                \tClf: 2.884844\tReg: 0.000061\tFr_p: 0.109596\tFr_r: 0.118792\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854643\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.952576                \tClf: 2.859782\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118717\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.862685\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.158017                \tClf: 3.064821\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118989\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842857\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.798713                \tClf: 2.706509\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118886\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864743\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.966045                \tClf: 2.872747\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119356\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.850318\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.007177                \tClf: 2.914600\tReg: 0.000061\tFr_p: 0.110029\tFr_r: 0.119823\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837503\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.058722                \tClf: 2.966785\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118956\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845290\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.784011                \tClf: 2.691685\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119210\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859623\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.102141                \tClf: 3.009099\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118814\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849731\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.859626                \tClf: 2.767079\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119376\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.842948\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.144919                \tClf: 3.052711\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119266\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855036\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.059400                \tClf: 2.966587\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119464\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.834677\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.865202                \tClf: 2.773407\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119184\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.839456\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.257013                \tClf: 2.164980\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118896\n",
      "\n",
      "Test set: Average loss: 0.0966, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846160\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.994868                \tClf: 2.902499\tReg: 0.000061\tFr_p: 0.383538\tFr_r: 0.414920\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843281\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.950144                \tClf: 2.857919\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118771\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.854865\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.956586                \tClf: 2.863781\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118724\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.863464\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.125335                \tClf: 3.032101\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118997\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.843417\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.849409                \tClf: 2.757177\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118882\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864900\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.935937                \tClf: 2.842631\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119363\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849766\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.934095                \tClf: 2.841545\tReg: 0.000061\tFr_p: 0.110007\tFr_r: 0.119815\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837723\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.015393                \tClf: 2.923445\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118994\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844653\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.863591                \tClf: 2.771297\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119185\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859776\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.121014                \tClf: 3.027964\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118808\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.849250\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.868947                \tClf: 2.776423\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119354\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842410\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.184609                \tClf: 3.092427\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119279\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.856281\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.186810                \tClf: 3.093935\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119524\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835168\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.846770                \tClf: 2.754951\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119148\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839352\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.213849                \tClf: 2.121820\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118915\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.846531\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.957681                \tClf: 2.865293\tReg: 0.000061\tFr_p: 0.383531\tFr_r: 0.414960\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.842721\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.016177                \tClf: 2.923980\tReg: 0.000061\tFr_p: 0.109590\tFr_r: 0.118757\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855343\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.937375                \tClf: 2.844546\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118690\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.863373\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.175070                \tClf: 3.081840\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.118974\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842996\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.859829                \tClf: 2.767619\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118877\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864870\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.998166                \tClf: 2.904862\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119340\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850231\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.957885                \tClf: 2.865313\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119807\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837278\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.003552                \tClf: 2.911627\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118958\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.844727\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.796981                \tClf: 2.704684\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119178\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.860345\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.086539                \tClf: 2.993460\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118801\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.849736\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.953185                \tClf: 2.860637\tReg: 0.000061\tFr_p: 0.109893\tFr_r: 0.119370\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.843601\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.178249                \tClf: 3.086007\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119262\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.856023\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.116298                \tClf: 3.023436\tReg: 0.000061\tFr_p: 0.109804\tFr_r: 0.119519\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.836085\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.832158                \tClf: 2.740293\tReg: 0.000061\tFr_p: 0.109739\tFr_r: 0.119163\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839718\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.228578                \tClf: 2.136531\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0948, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846518\tlr: 0.001000\ttrain acc:97.7500\tLoss: 3.015954                \tClf: 2.923567\tReg: 0.000061\tFr_p: 0.383556\tFr_r: 0.414936\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843952\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.006279                \tClf: 2.914020\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118768\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.855257\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.876473                \tClf: 2.783649\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118715\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862417\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.244386                \tClf: 3.151204\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118999\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.843150\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.829841                \tClf: 2.737623\tReg: 0.000061\tFr_p: 0.109667\tFr_r: 0.118900\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865812\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.916805                \tClf: 2.823453\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119381\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850054\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.984048                \tClf: 2.891484\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119835\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.838089\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.037719                \tClf: 2.945753\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118966\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844614\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.806671                \tClf: 2.714379\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119180\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859736\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.072812                \tClf: 2.979764\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118800\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850591\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.929051                \tClf: 2.836461\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119370\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842414\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.166628                \tClf: 3.074446\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.119257\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855158\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.082469                \tClf: 2.989650\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119505\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835804\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.820452                \tClf: 2.728600\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119163\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839312\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.195787                \tClf: 2.103760\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118890\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846165\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.934978                \tClf: 2.842609\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414913\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842575\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.956210                \tClf: 2.864020\tReg: 0.000061\tFr_p: 0.109575\tFr_r: 0.118778\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.854461\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.977167                \tClf: 2.884383\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118729\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863181\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.213602                \tClf: 3.120382\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118974\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843251\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.876934                \tClf: 2.784710\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118892\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.864769\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.920958                \tClf: 2.827658\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119354\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.849786\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.974393                \tClf: 2.881842\tReg: 0.000061\tFr_p: 0.110028\tFr_r: 0.119822\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838056\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.040256                \tClf: 2.948292\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118967\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.845138\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.852804                \tClf: 2.760486\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119187\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860272\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.187880                \tClf: 3.094806\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118858\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849828\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.925593                \tClf: 2.833041\tReg: 0.000061\tFr_p: 0.109896\tFr_r: 0.119366\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842651\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.212653                \tClf: 3.120459\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.119242\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.856087\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.078200                \tClf: 2.985334\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119521\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835389\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.955959                \tClf: 2.864128\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.119151\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839597\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.146334                \tClf: 2.054293\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118875\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846148\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.032193                \tClf: 2.939824\tReg: 0.000061\tFr_p: 0.383569\tFr_r: 0.414963\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843190\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.931726                \tClf: 2.839505\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118786\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855226\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.960132                \tClf: 2.867310\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118734\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863384\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.232776                \tClf: 3.139545\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.119012\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842929\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.868776                \tClf: 2.776568\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118898\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865462\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.936357                \tClf: 2.843023\tReg: 0.000061\tFr_p: 0.109849\tFr_r: 0.119342\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850357\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.934214                \tClf: 2.841636\tReg: 0.000061\tFr_p: 0.110035\tFr_r: 0.119832\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837434\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.972593                \tClf: 2.880660\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118964\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844701\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.805275                \tClf: 2.712978\tReg: 0.000061\tFr_p: 0.109791\tFr_r: 0.119185\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859931\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.172941                \tClf: 3.079883\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118841\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.849526\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.896423                \tClf: 2.803886\tReg: 0.000061\tFr_p: 0.109870\tFr_r: 0.119331\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.843121\tlr: 0.001000\ttrain acc:96.8750\tLoss: 3.152792                \tClf: 3.060575\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119284\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855500\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.078778                \tClf: 2.985942\tReg: 0.000061\tFr_p: 0.109818\tFr_r: 0.119507\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.835421\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.858635                \tClf: 2.766802\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119159\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839513\tlr: 0.001000\ttrain acc:98.0750\tLoss: 2.244988                \tClf: 2.152951\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.118913\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.845761\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.936440                \tClf: 2.844091\tReg: 0.000061\tFr_p: 0.383523\tFr_r: 0.414941\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843091\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.965650                \tClf: 2.873435\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118807\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854828\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.932649                \tClf: 2.839846\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118711\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862811\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.176100                \tClf: 3.082898\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118998\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842614\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.916298                \tClf: 2.824106\tReg: 0.000061\tFr_p: 0.109675\tFr_r: 0.118881\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865319\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.963329                \tClf: 2.870002\tReg: 0.000061\tFr_p: 0.109880\tFr_r: 0.119381\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850160\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.978058                \tClf: 2.885489\tReg: 0.000061\tFr_p: 0.110055\tFr_r: 0.119843\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.837497\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.022670                \tClf: 2.930734\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118956\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844663\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.829079                \tClf: 2.736784\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119221\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859605\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.073594                \tClf: 2.980552\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.118824\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.850263\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.932440                \tClf: 2.839866\tReg: 0.000061\tFr_p: 0.109867\tFr_r: 0.119333\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842799\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.226517                \tClf: 3.134316\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.119256\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855696\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.140887                \tClf: 3.048041\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119499\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.836537\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.832181                \tClf: 2.740293\tReg: 0.000061\tFr_p: 0.109774\tFr_r: 0.119171\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839968\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.211380                \tClf: 2.119320\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118906\n",
      "\n",
      "Test set: Average loss: 0.0968, Accuracy: 9707/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845761\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.980272                \tClf: 2.887922\tReg: 0.000061\tFr_p: 0.383543\tFr_r: 0.414913\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.843558\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.960886                \tClf: 2.868647\tReg: 0.000061\tFr_p: 0.109574\tFr_r: 0.118759\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855607\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.934227                \tClf: 2.841386\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118703\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.863023\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.184004                \tClf: 3.090792\tReg: 0.000061\tFr_p: 0.109704\tFr_r: 0.118985\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.843389\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.814623                \tClf: 2.722392\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118899\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865124\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.924962                \tClf: 2.831644\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119391\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.849915\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.900946                \tClf: 2.808389\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119801\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.837719\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.007375                \tClf: 2.915428\tReg: 0.000061\tFr_p: 0.109762\tFr_r: 0.118981\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845389\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.808874                \tClf: 2.716544\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119200\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.859440\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.064183                \tClf: 2.971150\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118818\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849985\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.926536                \tClf: 2.833976\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119338\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842373\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.169090                \tClf: 3.076910\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119279\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855786\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.178847                \tClf: 3.085997\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119509\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.835390\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.822881                \tClf: 2.731051\tReg: 0.000061\tFr_p: 0.109771\tFr_r: 0.119162\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839630\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.202247                \tClf: 2.110205\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.118876\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 9721/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.846089\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.004146                \tClf: 2.911781\tReg: 0.000061\tFr_p: 0.383542\tFr_r: 0.414945\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842965\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.015113                \tClf: 2.922904\tReg: 0.000061\tFr_p: 0.109568\tFr_r: 0.118759\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.854895\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.975222                \tClf: 2.882416\tReg: 0.000061\tFr_p: 0.109727\tFr_r: 0.118725\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.863366\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.206054                \tClf: 3.112825\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118992\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843214\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.801763                \tClf: 2.709542\tReg: 0.000061\tFr_p: 0.109670\tFr_r: 0.118879\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.864313\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.920256                \tClf: 2.826979\tReg: 0.000061\tFr_p: 0.109889\tFr_r: 0.119354\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.849932\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.033249                \tClf: 2.940691\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119800\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.837493\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.021967                \tClf: 2.930032\tReg: 0.000061\tFr_p: 0.109742\tFr_r: 0.118969\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844905\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.854726                \tClf: 2.762420\tReg: 0.000061\tFr_p: 0.109836\tFr_r: 0.119196\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.860267\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.097197                \tClf: 3.004122\tReg: 0.000061\tFr_p: 0.109730\tFr_r: 0.118793\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850079\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.944890                \tClf: 2.852325\tReg: 0.000061\tFr_p: 0.109868\tFr_r: 0.119375\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.843142\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.132080                \tClf: 3.039861\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119269\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855185\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.088913                \tClf: 2.996093\tReg: 0.000061\tFr_p: 0.109800\tFr_r: 0.119514\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.835413\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.872579                \tClf: 2.780747\tReg: 0.000061\tFr_p: 0.109778\tFr_r: 0.119178\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839276\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.175870                \tClf: 2.083845\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118894\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.845548\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.941460                \tClf: 2.849122\tReg: 0.000061\tFr_p: 0.383535\tFr_r: 0.414929\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.842900\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.984322                \tClf: 2.892116\tReg: 0.000061\tFr_p: 0.109580\tFr_r: 0.118763\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.855073\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.981098                \tClf: 2.888284\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118703\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.862866\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.217809                \tClf: 3.124605\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118983\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.843379\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.813585                \tClf: 2.721355\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118886\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.864958\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.936705                \tClf: 2.843396\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119353\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.850156\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.942360                \tClf: 2.849791\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119798\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837407\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.069885                \tClf: 2.977953\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118982\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845438\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.813221                \tClf: 2.720888\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119186\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.860481\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.100274                \tClf: 3.007189\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.118807\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850272\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.933722                \tClf: 2.841147\tReg: 0.000061\tFr_p: 0.109885\tFr_r: 0.119380\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.843194\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.209147                \tClf: 3.116926\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119265\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.855522\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.092730                \tClf: 2.999893\tReg: 0.000061\tFr_p: 0.109797\tFr_r: 0.119502\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.835354\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.818220                \tClf: 2.726391\tReg: 0.000061\tFr_p: 0.109757\tFr_r: 0.119155\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839199\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.192159                \tClf: 2.100138\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.118896\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.845299\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.975951                \tClf: 2.883625\tReg: 0.000061\tFr_p: 0.383508\tFr_r: 0.414935\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.842888\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.022823                \tClf: 2.930617\tReg: 0.000061\tFr_p: 0.109565\tFr_r: 0.118750\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.854789\tlr: 0.001000\ttrain acc:97.2500\tLoss: 2.963190                \tClf: 2.870390\tReg: 0.000061\tFr_p: 0.109688\tFr_r: 0.118690\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.863403\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.198194                \tClf: 3.104963\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118986\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842702\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.873508                \tClf: 2.781311\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118862\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865645\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.927234                \tClf: 2.833891\tReg: 0.000061\tFr_p: 0.109863\tFr_r: 0.119371\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850465\tlr: 0.001000\ttrain acc:97.3000\tLoss: 2.925287                \tClf: 2.832703\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119820\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.838392\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.047138                \tClf: 2.955157\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118991\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.844950\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.785019                \tClf: 2.692710\tReg: 0.000061\tFr_p: 0.109815\tFr_r: 0.119209\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859298\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.115534                \tClf: 3.022508\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118770\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.849530\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.911248                \tClf: 2.818710\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119359\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.842268\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.149087                \tClf: 3.056912\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119283\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.855520\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.158544                \tClf: 3.065707\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119511\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.835229\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.812730                \tClf: 2.720907\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.119159\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839143\tlr: 0.001000\ttrain acc:98.4000\tLoss: 2.216383                \tClf: 2.124365\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118879\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9722/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846325\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.974274                \tClf: 2.881896\tReg: 0.000061\tFr_p: 0.383554\tFr_r: 0.414934\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842776\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.964069                \tClf: 2.871869\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118791\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854796\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.973492                \tClf: 2.880691\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118681\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862290\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.209939                \tClf: 3.116764\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118982\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.843165\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.825307                \tClf: 2.733088\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118873\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864912\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.994865                \tClf: 2.901559\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119359\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850816\tlr: 0.001000\ttrain acc:97.0500\tLoss: 2.986563                \tClf: 2.893961\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119784\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837613\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.983004                \tClf: 2.891062\tReg: 0.000061\tFr_p: 0.109719\tFr_r: 0.118967\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844983\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.856702                \tClf: 2.764392\tReg: 0.000061\tFr_p: 0.109813\tFr_r: 0.119188\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860084\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.116469                \tClf: 3.023404\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118846\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.850920\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.939490                \tClf: 2.846883\tReg: 0.000061\tFr_p: 0.109899\tFr_r: 0.119390\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.843343\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.111124                \tClf: 3.018895\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119231\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.854929\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.108901                \tClf: 3.016093\tReg: 0.000061\tFr_p: 0.109802\tFr_r: 0.119511\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835143\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.875871                \tClf: 2.784053\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119161\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839472\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.203971                \tClf: 2.111937\tReg: 0.000061\tFr_p: 0.109749\tFr_r: 0.118880\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.846264\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.984859                \tClf: 2.892485\tReg: 0.000061\tFr_p: 0.383577\tFr_r: 0.414960\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843062\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.983840                \tClf: 2.891626\tReg: 0.000061\tFr_p: 0.109588\tFr_r: 0.118768\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855343\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.881391                \tClf: 2.788563\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118689\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.863461\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.150531                \tClf: 3.057297\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118980\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.842653\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.913823                \tClf: 2.821629\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118882\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865759\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.952206                \tClf: 2.858857\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119343\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.849702\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.954736                \tClf: 2.862189\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119794\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837490\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.992701                \tClf: 2.900765\tReg: 0.000061\tFr_p: 0.109734\tFr_r: 0.118958\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.845205\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.811437                \tClf: 2.719116\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119200\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859387\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.146754                \tClf: 3.053723\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118825\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.850378\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.932945                \tClf: 2.840365\tReg: 0.000061\tFr_p: 0.109890\tFr_r: 0.119372\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.842562\tlr: 0.001000\ttrain acc:97.0750\tLoss: 3.192960                \tClf: 3.100771\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119273\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855497\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.125582                \tClf: 3.032746\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119487\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834855\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.872491                \tClf: 2.780687\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119155\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.840225\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.172229                \tClf: 2.080156\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846497\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.976841                \tClf: 2.884455\tReg: 0.000061\tFr_p: 0.383549\tFr_r: 0.414956\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843136\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.931808                \tClf: 2.839590\tReg: 0.000061\tFr_p: 0.109585\tFr_r: 0.118774\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.854510\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.900593                \tClf: 2.807806\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118703\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.863574\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.145072                \tClf: 3.051832\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.119009\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.842950\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.822295                \tClf: 2.730086\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118889\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865535\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.953398                \tClf: 2.860060\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119373\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.850536\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.960174                \tClf: 2.867587\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119843\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838390\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.050355                \tClf: 2.958374\tReg: 0.000061\tFr_p: 0.109740\tFr_r: 0.118974\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.845348\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.799188                \tClf: 2.706859\tReg: 0.000061\tFr_p: 0.109832\tFr_r: 0.119203\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860820\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.119395                \tClf: 3.026293\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118799\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850018\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.011159                \tClf: 2.918597\tReg: 0.000061\tFr_p: 0.109886\tFr_r: 0.119353\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842295\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.239730                \tClf: 3.147554\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119248\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855962\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.158342                \tClf: 3.065483\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119525\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.835664\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.851234                \tClf: 2.759390\tReg: 0.000061\tFr_p: 0.109772\tFr_r: 0.119167\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839028\tlr: 0.001000\ttrain acc:98.0000\tLoss: 2.186916                \tClf: 2.094904\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118878\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846194\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.988170                \tClf: 2.895800\tReg: 0.000061\tFr_p: 0.383581\tFr_r: 0.414964\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.843314\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.001221                \tClf: 2.908994\tReg: 0.000061\tFr_p: 0.109592\tFr_r: 0.118790\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854735\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.944208                \tClf: 2.851410\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118708\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862407\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.209546                \tClf: 3.116365\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118973\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842928\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.775632                \tClf: 2.683425\tReg: 0.000061\tFr_p: 0.109676\tFr_r: 0.118883\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865906\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.994547                \tClf: 2.901191\tReg: 0.000061\tFr_p: 0.109875\tFr_r: 0.119368\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850397\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.984512                \tClf: 2.891931\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119812\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837908\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.022614                \tClf: 2.930657\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118998\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.844947\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.798388                \tClf: 2.706080\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119219\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.859723\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.083389                \tClf: 2.990341\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118833\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.849865\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.926046                \tClf: 2.833491\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119359\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.843283\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.202614                \tClf: 3.110389\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119291\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.855115\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.154909                \tClf: 3.062092\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119510\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835964\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.888451                \tClf: 2.796592\tReg: 0.000061\tFr_p: 0.109779\tFr_r: 0.119174\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839616\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.203551                \tClf: 2.111510\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118887\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9739/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.846227\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.949851                \tClf: 2.857479\tReg: 0.000061\tFr_p: 0.383511\tFr_r: 0.414925\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843459\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.993378                \tClf: 2.901144\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118782\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.854451\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.912658                \tClf: 2.819875\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118711\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863070\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.126449                \tClf: 3.033234\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118990\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842850\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.756058                \tClf: 2.663855\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118895\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.864877\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.936650                \tClf: 2.843345\tReg: 0.000061\tFr_p: 0.109859\tFr_r: 0.119352\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849985\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.939012                \tClf: 2.846451\tReg: 0.000061\tFr_p: 0.110022\tFr_r: 0.119814\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.837474\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.050935                \tClf: 2.959000\tReg: 0.000061\tFr_p: 0.109748\tFr_r: 0.118966\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.846161\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.845646                \tClf: 2.753277\tReg: 0.000061\tFr_p: 0.109833\tFr_r: 0.119217\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.860033\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.101332                \tClf: 3.008269\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118802\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849498\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.967795                \tClf: 2.875259\tReg: 0.000061\tFr_p: 0.109904\tFr_r: 0.119358\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843279\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.180627                \tClf: 3.088402\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119307\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.855422\tlr: 0.001000\ttrain acc:97.6000\tLoss: 3.097099                \tClf: 3.004267\tReg: 0.000061\tFr_p: 0.109811\tFr_r: 0.119523\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835136\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.849253                \tClf: 2.757435\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.119173\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.840026\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.183669                \tClf: 2.091607\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846831\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.030608                \tClf: 2.938206\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414918\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843335\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.919323                \tClf: 2.827095\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118772\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855973\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.962983                \tClf: 2.870123\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118719\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862965\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.182689                \tClf: 3.089480\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.119010\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842739\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.781012                \tClf: 2.688814\tReg: 0.000061\tFr_p: 0.109690\tFr_r: 0.118878\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864891\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.921089                \tClf: 2.827784\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119388\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.850076\tlr: 0.001000\ttrain acc:97.7500\tLoss: 2.929864                \tClf: 2.837299\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119780\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837647\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.028930                \tClf: 2.936986\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118974\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.845141\tlr: 0.001000\ttrain acc:97.2000\tLoss: 2.862905                \tClf: 2.770587\tReg: 0.000061\tFr_p: 0.109838\tFr_r: 0.119204\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.860563\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.120894                \tClf: 3.027804\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118821\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850470\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.941458                \tClf: 2.848873\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119394\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842611\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.186487                \tClf: 3.094295\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119277\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855626\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.164160                \tClf: 3.071318\tReg: 0.000061\tFr_p: 0.109800\tFr_r: 0.119496\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835781\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.861481                \tClf: 2.769631\tReg: 0.000061\tFr_p: 0.109776\tFr_r: 0.119187\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839311\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.176249                \tClf: 2.084222\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.845914\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.955952                \tClf: 2.863595\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414945\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843156\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.009276                \tClf: 2.917057\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118781\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.854381\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.905554                \tClf: 2.812774\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118708\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862839\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.212124                \tClf: 3.118921\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118993\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.842866\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.830058                \tClf: 2.737853\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118891\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.865018\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.931702                \tClf: 2.838390\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119357\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850897\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.006529                \tClf: 2.913923\tReg: 0.000061\tFr_p: 0.110032\tFr_r: 0.119834\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.836841\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.994324                \tClf: 2.902421\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118957\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.844615\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.824761                \tClf: 2.732470\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119223\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.859989\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.098481                \tClf: 3.005420\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.118796\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850127\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.914204                \tClf: 2.821637\tReg: 0.000061\tFr_p: 0.109872\tFr_r: 0.119340\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.842472\tlr: 0.001000\ttrain acc:96.6750\tLoss: 3.169463                \tClf: 3.077278\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.119274\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855557\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.106093                \tClf: 3.013254\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119523\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.834717\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.873775                \tClf: 2.781978\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119148\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839270\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.221623                \tClf: 2.129599\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.118889\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846491\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.010375                \tClf: 2.917989\tReg: 0.000061\tFr_p: 0.383567\tFr_r: 0.414954\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.842990\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.948229                \tClf: 2.856019\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118772\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854837\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.860613                \tClf: 2.767810\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.118725\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.862751\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.234632                \tClf: 3.141434\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118980\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842500\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.856283                \tClf: 2.764097\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118908\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865086\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.960048                \tClf: 2.866733\tReg: 0.000061\tFr_p: 0.109854\tFr_r: 0.119357\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850085\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.958089                \tClf: 2.865524\tReg: 0.000061\tFr_p: 0.110019\tFr_r: 0.119816\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837912\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.051932                \tClf: 2.959975\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118934\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.845645\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.801044                \tClf: 2.708700\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119195\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.859818\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.111222                \tClf: 3.018170\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.118806\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.850428\tlr: 0.001000\ttrain acc:97.0750\tLoss: 2.941306                \tClf: 2.848724\tReg: 0.000061\tFr_p: 0.109895\tFr_r: 0.119375\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842634\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.104917                \tClf: 3.012724\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.119245\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855596\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.101738                \tClf: 3.008897\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119522\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835327\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.849264                \tClf: 2.757436\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119164\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839682\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.248234                \tClf: 2.156189\tReg: 0.000061\tFr_p: 0.109747\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.846520\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.914007                \tClf: 2.821620\tReg: 0.000061\tFr_p: 0.383552\tFr_r: 0.414952\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.843110\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.957432                \tClf: 2.865215\tReg: 0.000061\tFr_p: 0.109567\tFr_r: 0.118769\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855354\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.922034                \tClf: 2.829205\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118708\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.863127\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.250178                \tClf: 3.156960\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118995\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843097\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.836748                \tClf: 2.744532\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118894\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865746\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.957173                \tClf: 2.863825\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119378\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850874\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.977896                \tClf: 2.885292\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119823\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.837627\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.036109                \tClf: 2.944167\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118963\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844927\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.784956                \tClf: 2.692649\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119188\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860223\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.166367                \tClf: 3.073295\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118819\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.849786\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.864851                \tClf: 2.772301\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119357\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.842132\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.176113                \tClf: 3.083945\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119279\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.855340\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.129949                \tClf: 3.037121\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119512\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.835420\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.828468                \tClf: 2.736636\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.119177\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839042\tlr: 0.001000\ttrain acc:98.2750\tLoss: 2.301212                \tClf: 2.209198\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118899\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.845416\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.969449                \tClf: 2.877117\tReg: 0.000061\tFr_p: 0.383528\tFr_r: 0.414913\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843171\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.009085                \tClf: 2.916866\tReg: 0.000061\tFr_p: 0.109578\tFr_r: 0.118766\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.854792\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.939586                \tClf: 2.846785\tReg: 0.000061\tFr_p: 0.109693\tFr_r: 0.118697\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.862464\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.234827                \tClf: 3.141643\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118972\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842555\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.810932                \tClf: 2.718743\tReg: 0.000061\tFr_p: 0.109699\tFr_r: 0.118893\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.866144\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.941628                \tClf: 2.848259\tReg: 0.000061\tFr_p: 0.109873\tFr_r: 0.119381\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.850501\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.999559                \tClf: 2.906973\tReg: 0.000061\tFr_p: 0.110018\tFr_r: 0.119803\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837697\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.034100                \tClf: 2.942154\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118952\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844940\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.780058                \tClf: 2.687750\tReg: 0.000061\tFr_p: 0.109820\tFr_r: 0.119188\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859834\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.100248                \tClf: 3.007195\tReg: 0.000061\tFr_p: 0.109733\tFr_r: 0.118807\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850051\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.904691                \tClf: 2.812127\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119379\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842655\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.189499                \tClf: 3.097305\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.119295\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855720\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.102848                \tClf: 3.010001\tReg: 0.000061\tFr_p: 0.109808\tFr_r: 0.119538\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834395\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.874739                \tClf: 2.782958\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.119152\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.840021\tlr: 0.001000\ttrain acc:98.4250\tLoss: 2.234804                \tClf: 2.142742\tReg: 0.000061\tFr_p: 0.109786\tFr_r: 0.118930\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846836\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.991046                \tClf: 2.898643\tReg: 0.000061\tFr_p: 0.383528\tFr_r: 0.414940\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.843156\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.988101                \tClf: 2.895882\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118786\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854873\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.915315                \tClf: 2.822510\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118712\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.863111\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.194907                \tClf: 3.101690\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.119004\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.843310\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.861814                \tClf: 2.769587\tReg: 0.000061\tFr_p: 0.109684\tFr_r: 0.118892\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865063\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.014585                \tClf: 2.921271\tReg: 0.000061\tFr_p: 0.109857\tFr_r: 0.119343\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.850759\tlr: 0.001000\ttrain acc:97.1500\tLoss: 2.972148                \tClf: 2.879549\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119819\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838371\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.048414                \tClf: 2.956434\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118956\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.844651\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.834142                \tClf: 2.741848\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119191\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.859734\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.095252                \tClf: 3.002204\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.118800\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849626\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.888393                \tClf: 2.795851\tReg: 0.000061\tFr_p: 0.109882\tFr_r: 0.119358\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842605\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.217588                \tClf: 3.125397\tReg: 0.000061\tFr_p: 0.109687\tFr_r: 0.119259\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855739\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.056018                \tClf: 2.963170\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119507\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.834946\tlr: 0.001000\ttrain acc:97.8250\tLoss: 2.814213                \tClf: 2.722405\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.119137\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839654\tlr: 0.001000\ttrain acc:98.3000\tLoss: 2.196275                \tClf: 2.104231\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118912\n",
      "\n",
      "Test set: Average loss: 0.0950, Accuracy: 9735/10000 (97%)\n",
      "\n",
      "Train Epoch: 0 [3800/60000 (6%)]\tenerg: 1.845943\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.985151                \tClf: 2.892792\tReg: 0.000061\tFr_p: 0.383569\tFr_r: 0.414954\n",
      "Train Epoch: 0 [7800/60000 (13%)]\tenerg: 1.842982\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.936335                \tClf: 2.844125\tReg: 0.000061\tFr_p: 0.109598\tFr_r: 0.118768\n",
      "Train Epoch: 0 [11800/60000 (20%)]\tenerg: 1.855360\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.918207                \tClf: 2.825378\tReg: 0.000061\tFr_p: 0.109721\tFr_r: 0.118711\n",
      "Train Epoch: 0 [15800/60000 (26%)]\tenerg: 1.862556\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.243450                \tClf: 3.150262\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118993\n",
      "Train Epoch: 0 [19800/60000 (33%)]\tenerg: 1.842999\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.852629                \tClf: 2.760418\tReg: 0.000061\tFr_p: 0.109691\tFr_r: 0.118897\n",
      "Train Epoch: 0 [23800/60000 (40%)]\tenerg: 1.865156\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.887796                \tClf: 2.794477\tReg: 0.000061\tFr_p: 0.109864\tFr_r: 0.119353\n",
      "Train Epoch: 0 [27800/60000 (46%)]\tenerg: 1.850546\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.928667                \tClf: 2.836078\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119820\n",
      "Train Epoch: 0 [31800/60000 (53%)]\tenerg: 1.838481\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.934077                \tClf: 2.842092\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118956\n",
      "Train Epoch: 0 [35800/60000 (60%)]\tenerg: 1.845048\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.797204                \tClf: 2.704890\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119213\n",
      "Train Epoch: 0 [39800/60000 (66%)]\tenerg: 1.860648\tlr: 0.001000\ttrain acc:97.0250\tLoss: 3.120102                \tClf: 3.027008\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118823\n",
      "Train Epoch: 0 [43800/60000 (73%)]\tenerg: 1.849899\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.959632                \tClf: 2.867076\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119339\n",
      "Train Epoch: 0 [47800/60000 (80%)]\tenerg: 1.842701\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.232250                \tClf: 3.140054\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119289\n",
      "Train Epoch: 0 [51800/60000 (86%)]\tenerg: 1.855192\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.061823                \tClf: 2.969002\tReg: 0.000061\tFr_p: 0.109796\tFr_r: 0.119524\n",
      "Train Epoch: 0 [55800/60000 (93%)]\tenerg: 1.834538\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.868434                \tClf: 2.776646\tReg: 0.000061\tFr_p: 0.109753\tFr_r: 0.119129\n",
      "Train Epoch: 0 [59800/60000 (100%)]\tenerg: 1.839816\tlr: 0.001000\ttrain acc:97.9750\tLoss: 2.255262                \tClf: 2.163210\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118910\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tenerg: 1.845647\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.024625                \tClf: 2.932282\tReg: 0.000061\tFr_p: 0.383530\tFr_r: 0.414932\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tenerg: 1.842761\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.943137                \tClf: 2.850938\tReg: 0.000061\tFr_p: 0.109594\tFr_r: 0.118786\n",
      "Train Epoch: 1 [11800/60000 (20%)]\tenerg: 1.855171\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.903731                \tClf: 2.810911\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118716\n",
      "Train Epoch: 1 [15800/60000 (26%)]\tenerg: 1.862656\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.211743                \tClf: 3.118549\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118996\n",
      "Train Epoch: 1 [19800/60000 (33%)]\tenerg: 1.843121\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.806187                \tClf: 2.713970\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118880\n",
      "Train Epoch: 1 [23800/60000 (40%)]\tenerg: 1.865558\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.932507                \tClf: 2.839168\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119366\n",
      "Train Epoch: 1 [27800/60000 (46%)]\tenerg: 1.849525\tlr: 0.001000\ttrain acc:97.8000\tLoss: 2.937912                \tClf: 2.845374\tReg: 0.000061\tFr_p: 0.110026\tFr_r: 0.119810\n",
      "Train Epoch: 1 [31800/60000 (53%)]\tenerg: 1.838104\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.067555                \tClf: 2.975589\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118970\n",
      "Train Epoch: 1 [35800/60000 (60%)]\tenerg: 1.844484\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.845055                \tClf: 2.752770\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119178\n",
      "Train Epoch: 1 [39800/60000 (66%)]\tenerg: 1.859665\tlr: 0.001000\ttrain acc:97.0000\tLoss: 3.111562                \tClf: 3.018518\tReg: 0.000061\tFr_p: 0.109732\tFr_r: 0.118840\n",
      "Train Epoch: 1 [43800/60000 (73%)]\tenerg: 1.850142\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.004094                \tClf: 2.911526\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119366\n",
      "Train Epoch: 1 [47800/60000 (80%)]\tenerg: 1.842548\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.208313                \tClf: 3.116124\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.119255\n",
      "Train Epoch: 1 [51800/60000 (86%)]\tenerg: 1.855859\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.120348                \tClf: 3.027494\tReg: 0.000061\tFr_p: 0.109788\tFr_r: 0.119486\n",
      "Train Epoch: 1 [55800/60000 (93%)]\tenerg: 1.834827\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.864249                \tClf: 2.772447\tReg: 0.000061\tFr_p: 0.109761\tFr_r: 0.119157\n",
      "Train Epoch: 1 [59800/60000 (100%)]\tenerg: 1.839142\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.180557                \tClf: 2.088539\tReg: 0.000061\tFr_p: 0.109766\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [3800/60000 (6%)]\tenerg: 1.846593\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.973413                \tClf: 2.881022\tReg: 0.000061\tFr_p: 0.383551\tFr_r: 0.414932\n",
      "Train Epoch: 2 [7800/60000 (13%)]\tenerg: 1.843659\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.025698                \tClf: 2.933454\tReg: 0.000061\tFr_p: 0.109591\tFr_r: 0.118777\n",
      "Train Epoch: 2 [11800/60000 (20%)]\tenerg: 1.854938\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.934839                \tClf: 2.842031\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118737\n",
      "Train Epoch: 2 [15800/60000 (26%)]\tenerg: 1.863038\tlr: 0.001000\ttrain acc:97.1750\tLoss: 3.174291                \tClf: 3.081078\tReg: 0.000061\tFr_p: 0.109707\tFr_r: 0.118953\n",
      "Train Epoch: 2 [19800/60000 (33%)]\tenerg: 1.842612\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.801890                \tClf: 2.709698\tReg: 0.000061\tFr_p: 0.109664\tFr_r: 0.118882\n",
      "Train Epoch: 2 [23800/60000 (40%)]\tenerg: 1.865510\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.891683                \tClf: 2.798346\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119351\n",
      "Train Epoch: 2 [27800/60000 (46%)]\tenerg: 1.851159\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.940430                \tClf: 2.847811\tReg: 0.000061\tFr_p: 0.110044\tFr_r: 0.119831\n",
      "Train Epoch: 2 [31800/60000 (53%)]\tenerg: 1.837277\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.066259                \tClf: 2.974334\tReg: 0.000061\tFr_p: 0.109735\tFr_r: 0.118967\n",
      "Train Epoch: 2 [35800/60000 (60%)]\tenerg: 1.845238\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.835037                \tClf: 2.742714\tReg: 0.000061\tFr_p: 0.109824\tFr_r: 0.119175\n",
      "Train Epoch: 2 [39800/60000 (66%)]\tenerg: 1.859920\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.150290                \tClf: 3.057233\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118814\n",
      "Train Epoch: 2 [43800/60000 (73%)]\tenerg: 1.850041\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.914154                \tClf: 2.821591\tReg: 0.000061\tFr_p: 0.109861\tFr_r: 0.119342\n",
      "Train Epoch: 2 [47800/60000 (80%)]\tenerg: 1.842501\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.177141                \tClf: 3.084955\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.119277\n",
      "Train Epoch: 2 [51800/60000 (86%)]\tenerg: 1.856067\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.114104                \tClf: 3.021239\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119509\n",
      "Train Epoch: 2 [55800/60000 (93%)]\tenerg: 1.834999\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.846241                \tClf: 2.754430\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119210\n",
      "Train Epoch: 2 [59800/60000 (100%)]\tenerg: 1.839976\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.192177                \tClf: 2.100117\tReg: 0.000061\tFr_p: 0.109759\tFr_r: 0.118867\n",
      "\n",
      "Test set: Average loss: 0.0941, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [3800/60000 (6%)]\tenerg: 1.846509\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.956062                \tClf: 2.863675\tReg: 0.000061\tFr_p: 0.383536\tFr_r: 0.414953\n",
      "Train Epoch: 3 [7800/60000 (13%)]\tenerg: 1.843999\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.966276                \tClf: 2.874015\tReg: 0.000061\tFr_p: 0.109579\tFr_r: 0.118777\n",
      "Train Epoch: 3 [11800/60000 (20%)]\tenerg: 1.855105\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.875085                \tClf: 2.782269\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118717\n",
      "Train Epoch: 3 [15800/60000 (26%)]\tenerg: 1.862462\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.158621                \tClf: 3.065437\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118991\n",
      "Train Epoch: 3 [19800/60000 (33%)]\tenerg: 1.842735\tlr: 0.001000\ttrain acc:97.6750\tLoss: 2.823356                \tClf: 2.731158\tReg: 0.000061\tFr_p: 0.109672\tFr_r: 0.118874\n",
      "Train Epoch: 3 [23800/60000 (40%)]\tenerg: 1.865138\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.034936                \tClf: 2.941618\tReg: 0.000061\tFr_p: 0.109858\tFr_r: 0.119360\n",
      "Train Epoch: 3 [27800/60000 (46%)]\tenerg: 1.850382\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.881234                \tClf: 2.788654\tReg: 0.000061\tFr_p: 0.110014\tFr_r: 0.119826\n",
      "Train Epoch: 3 [31800/60000 (53%)]\tenerg: 1.837955\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.039850                \tClf: 2.947892\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.118973\n",
      "Train Epoch: 3 [35800/60000 (60%)]\tenerg: 1.845660\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.883270                \tClf: 2.790926\tReg: 0.000061\tFr_p: 0.109819\tFr_r: 0.119209\n",
      "Train Epoch: 3 [39800/60000 (66%)]\tenerg: 1.859494\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.052516                \tClf: 2.959480\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118799\n",
      "Train Epoch: 3 [43800/60000 (73%)]\tenerg: 1.850398\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.897716                \tClf: 2.805135\tReg: 0.000061\tFr_p: 0.109869\tFr_r: 0.119340\n",
      "Train Epoch: 3 [47800/60000 (80%)]\tenerg: 1.843301\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.175132                \tClf: 3.082906\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.119269\n",
      "Train Epoch: 3 [51800/60000 (86%)]\tenerg: 1.856438\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.162607                \tClf: 3.069724\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119526\n",
      "Train Epoch: 3 [55800/60000 (93%)]\tenerg: 1.834723\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.859374                \tClf: 2.767576\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.119167\n",
      "Train Epoch: 3 [59800/60000 (100%)]\tenerg: 1.839725\tlr: 0.001000\ttrain acc:98.2250\tLoss: 2.201860                \tClf: 2.109813\tReg: 0.000061\tFr_p: 0.109743\tFr_r: 0.118897\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [3800/60000 (6%)]\tenerg: 1.846211\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.944934                \tClf: 2.852562\tReg: 0.000061\tFr_p: 0.383533\tFr_r: 0.414963\n",
      "Train Epoch: 4 [7800/60000 (13%)]\tenerg: 1.842828\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.955781                \tClf: 2.863578\tReg: 0.000061\tFr_p: 0.109582\tFr_r: 0.118786\n",
      "Train Epoch: 4 [11800/60000 (20%)]\tenerg: 1.854465\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.935522                \tClf: 2.842738\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118730\n",
      "Train Epoch: 4 [15800/60000 (26%)]\tenerg: 1.862648\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.208052                \tClf: 3.114858\tReg: 0.000061\tFr_p: 0.109714\tFr_r: 0.118994\n",
      "Train Epoch: 4 [19800/60000 (33%)]\tenerg: 1.842895\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.828242                \tClf: 2.736036\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118878\n",
      "Train Epoch: 4 [23800/60000 (40%)]\tenerg: 1.864602\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.995699                \tClf: 2.902408\tReg: 0.000061\tFr_p: 0.109845\tFr_r: 0.119321\n",
      "Train Epoch: 4 [27800/60000 (46%)]\tenerg: 1.850168\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.013333                \tClf: 2.920763\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119782\n",
      "Train Epoch: 4 [31800/60000 (53%)]\tenerg: 1.837553\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.068696                \tClf: 2.976757\tReg: 0.000061\tFr_p: 0.109745\tFr_r: 0.118946\n",
      "Train Epoch: 4 [35800/60000 (60%)]\tenerg: 1.844689\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.784422                \tClf: 2.692126\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119204\n",
      "Train Epoch: 4 [39800/60000 (66%)]\tenerg: 1.860392\tlr: 0.001000\ttrain acc:97.2250\tLoss: 3.101241                \tClf: 3.008160\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118803\n",
      "Train Epoch: 4 [43800/60000 (73%)]\tenerg: 1.849593\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.997798                \tClf: 2.905257\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119366\n",
      "Train Epoch: 4 [47800/60000 (80%)]\tenerg: 1.842493\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.181067                \tClf: 3.088881\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.119284\n",
      "Train Epoch: 4 [51800/60000 (86%)]\tenerg: 1.855533\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.177191                \tClf: 3.084353\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119486\n",
      "Train Epoch: 4 [55800/60000 (93%)]\tenerg: 1.835499\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.888409                \tClf: 2.796573\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119185\n",
      "Train Epoch: 4 [59800/60000 (100%)]\tenerg: 1.839210\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.216192                \tClf: 2.124171\tReg: 0.000061\tFr_p: 0.109750\tFr_r: 0.118896\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [3800/60000 (6%)]\tenerg: 1.845555\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.954324                \tClf: 2.861985\tReg: 0.000061\tFr_p: 0.383541\tFr_r: 0.414945\n",
      "Train Epoch: 5 [7800/60000 (13%)]\tenerg: 1.843285\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.992677                \tClf: 2.900452\tReg: 0.000061\tFr_p: 0.109599\tFr_r: 0.118798\n",
      "Train Epoch: 5 [11800/60000 (20%)]\tenerg: 1.855744\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.949468                \tClf: 2.856619\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118679\n",
      "Train Epoch: 5 [15800/60000 (26%)]\tenerg: 1.862862\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.156840                \tClf: 3.063636\tReg: 0.000061\tFr_p: 0.109706\tFr_r: 0.118997\n",
      "Train Epoch: 5 [19800/60000 (33%)]\tenerg: 1.843055\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.857437                \tClf: 2.765223\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118887\n",
      "Train Epoch: 5 [23800/60000 (40%)]\tenerg: 1.865060\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.947635                \tClf: 2.854320\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119336\n",
      "Train Epoch: 5 [27800/60000 (46%)]\tenerg: 1.850378\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.993361                \tClf: 2.900781\tReg: 0.000061\tFr_p: 0.110031\tFr_r: 0.119793\n",
      "Train Epoch: 5 [31800/60000 (53%)]\tenerg: 1.837581\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.040426                \tClf: 2.948486\tReg: 0.000061\tFr_p: 0.109711\tFr_r: 0.118947\n",
      "Train Epoch: 5 [35800/60000 (60%)]\tenerg: 1.844915\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.813871                \tClf: 2.721564\tReg: 0.000061\tFr_p: 0.109834\tFr_r: 0.119225\n",
      "Train Epoch: 5 [39800/60000 (66%)]\tenerg: 1.859949\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.109044                \tClf: 3.015986\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118801\n",
      "Train Epoch: 5 [43800/60000 (73%)]\tenerg: 1.849768\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.904732                \tClf: 2.812183\tReg: 0.000061\tFr_p: 0.109891\tFr_r: 0.119345\n",
      "Train Epoch: 5 [47800/60000 (80%)]\tenerg: 1.843214\tlr: 0.001000\ttrain acc:97.1250\tLoss: 3.259880                \tClf: 3.167659\tReg: 0.000061\tFr_p: 0.109715\tFr_r: 0.119291\n",
      "Train Epoch: 5 [51800/60000 (86%)]\tenerg: 1.855832\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.148705                \tClf: 3.055852\tReg: 0.000061\tFr_p: 0.109809\tFr_r: 0.119513\n",
      "Train Epoch: 5 [55800/60000 (93%)]\tenerg: 1.834427\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.815848                \tClf: 2.724066\tReg: 0.000061\tFr_p: 0.109765\tFr_r: 0.119150\n",
      "Train Epoch: 5 [59800/60000 (100%)]\tenerg: 1.840356\tlr: 0.001000\ttrain acc:98.1250\tLoss: 2.222902                \tClf: 2.130823\tReg: 0.000061\tFr_p: 0.109777\tFr_r: 0.118917\n",
      "\n",
      "Test set: Average loss: 0.0951, Accuracy: 9727/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [3800/60000 (6%)]\tenerg: 1.846570\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.963555                \tClf: 2.871166\tReg: 0.000061\tFr_p: 0.383515\tFr_r: 0.414891\n",
      "Train Epoch: 6 [7800/60000 (13%)]\tenerg: 1.843224\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.996963                \tClf: 2.904741\tReg: 0.000061\tFr_p: 0.109557\tFr_r: 0.118737\n",
      "Train Epoch: 6 [11800/60000 (20%)]\tenerg: 1.855049\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.943309                \tClf: 2.850495\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.118695\n",
      "Train Epoch: 6 [15800/60000 (26%)]\tenerg: 1.862392\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.271648                \tClf: 3.178467\tReg: 0.000061\tFr_p: 0.109713\tFr_r: 0.119000\n",
      "Train Epoch: 6 [19800/60000 (33%)]\tenerg: 1.843228\tlr: 0.001000\ttrain acc:97.8500\tLoss: 2.761283                \tClf: 2.669061\tReg: 0.000061\tFr_p: 0.109680\tFr_r: 0.118890\n",
      "Train Epoch: 6 [23800/60000 (40%)]\tenerg: 1.865414\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.935422                \tClf: 2.842090\tReg: 0.000061\tFr_p: 0.109856\tFr_r: 0.119367\n",
      "Train Epoch: 6 [27800/60000 (46%)]\tenerg: 1.849746\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.899485                \tClf: 2.806937\tReg: 0.000061\tFr_p: 0.110027\tFr_r: 0.119799\n",
      "Train Epoch: 6 [31800/60000 (53%)]\tenerg: 1.838348\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.980828                \tClf: 2.888850\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118964\n",
      "Train Epoch: 6 [35800/60000 (60%)]\tenerg: 1.844380\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.787711                \tClf: 2.695431\tReg: 0.000061\tFr_p: 0.109817\tFr_r: 0.119185\n",
      "Train Epoch: 6 [39800/60000 (66%)]\tenerg: 1.860118\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.085284                \tClf: 2.992217\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.118802\n",
      "Train Epoch: 6 [43800/60000 (73%)]\tenerg: 1.850628\tlr: 0.001000\ttrain acc:97.4000\tLoss: 3.001635                \tClf: 2.909042\tReg: 0.000061\tFr_p: 0.109876\tFr_r: 0.119372\n",
      "Train Epoch: 6 [47800/60000 (80%)]\tenerg: 1.842636\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.179240                \tClf: 3.087047\tReg: 0.000061\tFr_p: 0.109720\tFr_r: 0.119267\n",
      "Train Epoch: 6 [51800/60000 (86%)]\tenerg: 1.855070\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.100204                \tClf: 3.007389\tReg: 0.000061\tFr_p: 0.109806\tFr_r: 0.119496\n",
      "Train Epoch: 6 [55800/60000 (93%)]\tenerg: 1.834506\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.947662                \tClf: 2.855875\tReg: 0.000061\tFr_p: 0.109744\tFr_r: 0.119148\n",
      "Train Epoch: 6 [59800/60000 (100%)]\tenerg: 1.839482\tlr: 0.001000\ttrain acc:98.4500\tLoss: 2.213008                \tClf: 2.120972\tReg: 0.000061\tFr_p: 0.109763\tFr_r: 0.118916\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [3800/60000 (6%)]\tenerg: 1.846410\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.991062                \tClf: 2.898680\tReg: 0.000061\tFr_p: 0.383555\tFr_r: 0.414970\n",
      "Train Epoch: 7 [7800/60000 (13%)]\tenerg: 1.842923\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.991155                \tClf: 2.898948\tReg: 0.000061\tFr_p: 0.109564\tFr_r: 0.118758\n",
      "Train Epoch: 7 [11800/60000 (20%)]\tenerg: 1.854997\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.917977                \tClf: 2.825166\tReg: 0.000061\tFr_p: 0.109701\tFr_r: 0.118696\n",
      "Train Epoch: 7 [15800/60000 (26%)]\tenerg: 1.862403\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.165887                \tClf: 3.072706\tReg: 0.000061\tFr_p: 0.109702\tFr_r: 0.118963\n",
      "Train Epoch: 7 [19800/60000 (33%)]\tenerg: 1.842945\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.798517                \tClf: 2.706308\tReg: 0.000061\tFr_p: 0.109682\tFr_r: 0.118868\n",
      "Train Epoch: 7 [23800/60000 (40%)]\tenerg: 1.865169\tlr: 0.001000\ttrain acc:97.1000\tLoss: 2.974369                \tClf: 2.881049\tReg: 0.000061\tFr_p: 0.109884\tFr_r: 0.119353\n",
      "Train Epoch: 7 [27800/60000 (46%)]\tenerg: 1.850921\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.016793                \tClf: 2.924186\tReg: 0.000061\tFr_p: 0.110039\tFr_r: 0.119810\n",
      "Train Epoch: 7 [31800/60000 (53%)]\tenerg: 1.837703\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.018954                \tClf: 2.927008\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118963\n",
      "Train Epoch: 7 [35800/60000 (60%)]\tenerg: 1.845477\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.842002                \tClf: 2.749667\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119202\n",
      "Train Epoch: 7 [39800/60000 (66%)]\tenerg: 1.860258\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.079729                \tClf: 2.986655\tReg: 0.000061\tFr_p: 0.109708\tFr_r: 0.118819\n",
      "Train Epoch: 7 [43800/60000 (73%)]\tenerg: 1.850449\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.891886                \tClf: 2.799302\tReg: 0.000061\tFr_p: 0.109881\tFr_r: 0.119378\n",
      "Train Epoch: 7 [47800/60000 (80%)]\tenerg: 1.842920\tlr: 0.001000\ttrain acc:96.9750\tLoss: 3.207122                \tClf: 3.114915\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119280\n",
      "Train Epoch: 7 [51800/60000 (86%)]\tenerg: 1.854760\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.149385                \tClf: 3.056586\tReg: 0.000061\tFr_p: 0.109803\tFr_r: 0.119494\n",
      "Train Epoch: 7 [55800/60000 (93%)]\tenerg: 1.835129\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.851878                \tClf: 2.760061\tReg: 0.000061\tFr_p: 0.109754\tFr_r: 0.119159\n",
      "Train Epoch: 7 [59800/60000 (100%)]\tenerg: 1.839512\tlr: 0.001000\ttrain acc:98.2500\tLoss: 2.201945                \tClf: 2.109908\tReg: 0.000061\tFr_p: 0.109760\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0967, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [3800/60000 (6%)]\tenerg: 1.845611\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.985538                \tClf: 2.893197\tReg: 0.000061\tFr_p: 0.383567\tFr_r: 0.414941\n",
      "Train Epoch: 8 [7800/60000 (13%)]\tenerg: 1.843542\tlr: 0.001000\ttrain acc:97.6500\tLoss: 3.000962                \tClf: 2.908723\tReg: 0.000061\tFr_p: 0.109602\tFr_r: 0.118809\n",
      "Train Epoch: 8 [11800/60000 (20%)]\tenerg: 1.855619\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.031635                \tClf: 2.938793\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118730\n",
      "Train Epoch: 8 [15800/60000 (26%)]\tenerg: 1.863507\tlr: 0.001000\ttrain acc:97.6250\tLoss: 3.154536                \tClf: 3.061300\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119013\n",
      "Train Epoch: 8 [19800/60000 (33%)]\tenerg: 1.842255\tlr: 0.001000\ttrain acc:97.9000\tLoss: 2.849032                \tClf: 2.756858\tReg: 0.000061\tFr_p: 0.109663\tFr_r: 0.118873\n",
      "Train Epoch: 8 [23800/60000 (40%)]\tenerg: 1.865172\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.946105                \tClf: 2.852785\tReg: 0.000061\tFr_p: 0.109853\tFr_r: 0.119345\n",
      "Train Epoch: 8 [27800/60000 (46%)]\tenerg: 1.849765\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.031184                \tClf: 2.938635\tReg: 0.000061\tFr_p: 0.110023\tFr_r: 0.119820\n",
      "Train Epoch: 8 [31800/60000 (53%)]\tenerg: 1.838353\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.988552                \tClf: 2.896573\tReg: 0.000061\tFr_p: 0.109731\tFr_r: 0.118954\n",
      "Train Epoch: 8 [35800/60000 (60%)]\tenerg: 1.845431\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.865567                \tClf: 2.773235\tReg: 0.000061\tFr_p: 0.109831\tFr_r: 0.119209\n",
      "Train Epoch: 8 [39800/60000 (66%)]\tenerg: 1.859415\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.116414                \tClf: 3.023382\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118826\n",
      "Train Epoch: 8 [43800/60000 (73%)]\tenerg: 1.849789\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.962430                \tClf: 2.869880\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119349\n",
      "Train Epoch: 8 [47800/60000 (80%)]\tenerg: 1.843579\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.133044                \tClf: 3.040804\tReg: 0.000061\tFr_p: 0.109705\tFr_r: 0.119246\n",
      "Train Epoch: 8 [51800/60000 (86%)]\tenerg: 1.856671\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.132858                \tClf: 3.039964\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119555\n",
      "Train Epoch: 8 [55800/60000 (93%)]\tenerg: 1.835569\tlr: 0.001000\ttrain acc:97.8750\tLoss: 2.787715                \tClf: 2.695875\tReg: 0.000061\tFr_p: 0.109758\tFr_r: 0.119163\n",
      "Train Epoch: 8 [59800/60000 (100%)]\tenerg: 1.840237\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.221568                \tClf: 2.129495\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118896\n",
      "\n",
      "Test set: Average loss: 0.0961, Accuracy: 9729/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [3800/60000 (6%)]\tenerg: 1.846348\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.956165                \tClf: 2.863786\tReg: 0.000061\tFr_p: 0.383529\tFr_r: 0.414921\n",
      "Train Epoch: 9 [7800/60000 (13%)]\tenerg: 1.843222\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.912713                \tClf: 2.820491\tReg: 0.000061\tFr_p: 0.109605\tFr_r: 0.118791\n",
      "Train Epoch: 9 [11800/60000 (20%)]\tenerg: 1.855109\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.984264                \tClf: 2.891448\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118709\n",
      "Train Epoch: 9 [15800/60000 (26%)]\tenerg: 1.862621\tlr: 0.001000\ttrain acc:97.4750\tLoss: 3.212757                \tClf: 3.119565\tReg: 0.000061\tFr_p: 0.109696\tFr_r: 0.118985\n",
      "Train Epoch: 9 [19800/60000 (33%)]\tenerg: 1.842633\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.880645                \tClf: 2.788452\tReg: 0.000061\tFr_p: 0.109677\tFr_r: 0.118895\n",
      "Train Epoch: 9 [23800/60000 (40%)]\tenerg: 1.864839\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.973966                \tClf: 2.880663\tReg: 0.000061\tFr_p: 0.109883\tFr_r: 0.119372\n",
      "Train Epoch: 9 [27800/60000 (46%)]\tenerg: 1.849486\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.967858                \tClf: 2.875322\tReg: 0.000061\tFr_p: 0.110025\tFr_r: 0.119793\n",
      "Train Epoch: 9 [31800/60000 (53%)]\tenerg: 1.837553\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.004941                \tClf: 2.913002\tReg: 0.000061\tFr_p: 0.109751\tFr_r: 0.118961\n",
      "Train Epoch: 9 [35800/60000 (60%)]\tenerg: 1.844529\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.759111                \tClf: 2.666824\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119195\n",
      "Train Epoch: 9 [39800/60000 (66%)]\tenerg: 1.859621\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.112114                \tClf: 3.019072\tReg: 0.000061\tFr_p: 0.109712\tFr_r: 0.118834\n",
      "Train Epoch: 9 [43800/60000 (73%)]\tenerg: 1.850269\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.924023                \tClf: 2.831448\tReg: 0.000061\tFr_p: 0.109887\tFr_r: 0.119337\n",
      "Train Epoch: 9 [47800/60000 (80%)]\tenerg: 1.842928\tlr: 0.001000\ttrain acc:97.3750\tLoss: 3.177901                \tClf: 3.085693\tReg: 0.000061\tFr_p: 0.109697\tFr_r: 0.119249\n",
      "Train Epoch: 9 [51800/60000 (86%)]\tenerg: 1.855894\tlr: 0.001000\ttrain acc:97.5500\tLoss: 3.127398                \tClf: 3.034542\tReg: 0.000061\tFr_p: 0.109827\tFr_r: 0.119542\n",
      "Train Epoch: 9 [55800/60000 (93%)]\tenerg: 1.835174\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.828294                \tClf: 2.736475\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.119152\n",
      "Train Epoch: 9 [59800/60000 (100%)]\tenerg: 1.839908\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.192130                \tClf: 2.100074\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.118903\n",
      "\n",
      "Test set: Average loss: 0.0933, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [3800/60000 (6%)]\tenerg: 1.845390\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.996406                \tClf: 2.904075\tReg: 0.000061\tFr_p: 0.383538\tFr_r: 0.414945\n",
      "Train Epoch: 10 [7800/60000 (13%)]\tenerg: 1.843389\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.007122                \tClf: 2.914891\tReg: 0.000061\tFr_p: 0.109573\tFr_r: 0.118759\n",
      "Train Epoch: 10 [11800/60000 (20%)]\tenerg: 1.855151\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.961236                \tClf: 2.868417\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118708\n",
      "Train Epoch: 10 [15800/60000 (26%)]\tenerg: 1.862832\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.205490                \tClf: 3.112288\tReg: 0.000061\tFr_p: 0.109700\tFr_r: 0.118990\n",
      "Train Epoch: 10 [19800/60000 (33%)]\tenerg: 1.843103\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.838195                \tClf: 2.745979\tReg: 0.000061\tFr_p: 0.109673\tFr_r: 0.118873\n",
      "Train Epoch: 10 [23800/60000 (40%)]\tenerg: 1.864921\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.941317                \tClf: 2.848010\tReg: 0.000061\tFr_p: 0.109843\tFr_r: 0.119346\n",
      "Train Epoch: 10 [27800/60000 (46%)]\tenerg: 1.850361\tlr: 0.001000\ttrain acc:97.3500\tLoss: 3.003258                \tClf: 2.910679\tReg: 0.000061\tFr_p: 0.110030\tFr_r: 0.119839\n",
      "Train Epoch: 10 [31800/60000 (53%)]\tenerg: 1.837758\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.000920                \tClf: 2.908971\tReg: 0.000061\tFr_p: 0.109752\tFr_r: 0.118967\n",
      "Train Epoch: 10 [35800/60000 (60%)]\tenerg: 1.845006\tlr: 0.001000\ttrain acc:97.2250\tLoss: 2.876813                \tClf: 2.784501\tReg: 0.000061\tFr_p: 0.109839\tFr_r: 0.119212\n",
      "Train Epoch: 10 [39800/60000 (66%)]\tenerg: 1.858989\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.101140                \tClf: 3.008129\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118801\n",
      "Train Epoch: 10 [43800/60000 (73%)]\tenerg: 1.850022\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.946314                \tClf: 2.853752\tReg: 0.000061\tFr_p: 0.109894\tFr_r: 0.119362\n",
      "Train Epoch: 10 [47800/60000 (80%)]\tenerg: 1.843180\tlr: 0.001000\ttrain acc:96.8250\tLoss: 3.175496                \tClf: 3.083276\tReg: 0.000061\tFr_p: 0.109698\tFr_r: 0.119281\n",
      "Train Epoch: 10 [51800/60000 (86%)]\tenerg: 1.855932\tlr: 0.001000\ttrain acc:97.5750\tLoss: 3.088150                \tClf: 2.995292\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119512\n",
      "Train Epoch: 10 [55800/60000 (93%)]\tenerg: 1.835990\tlr: 0.001000\ttrain acc:97.4500\tLoss: 2.920576                \tClf: 2.828716\tReg: 0.000061\tFr_p: 0.109756\tFr_r: 0.119160\n",
      "Train Epoch: 10 [59800/60000 (100%)]\tenerg: 1.839467\tlr: 0.001000\ttrain acc:98.1500\tLoss: 2.189445                \tClf: 2.097410\tReg: 0.000061\tFr_p: 0.109773\tFr_r: 0.118943\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [3800/60000 (6%)]\tenerg: 1.846142\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.974710                \tClf: 2.882342\tReg: 0.000061\tFr_p: 0.383499\tFr_r: 0.414894\n",
      "Train Epoch: 11 [7800/60000 (13%)]\tenerg: 1.843013\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.965064                \tClf: 2.872852\tReg: 0.000061\tFr_p: 0.109587\tFr_r: 0.118775\n",
      "Train Epoch: 11 [11800/60000 (20%)]\tenerg: 1.854868\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.950267                \tClf: 2.857463\tReg: 0.000061\tFr_p: 0.109694\tFr_r: 0.118693\n",
      "Train Epoch: 11 [15800/60000 (26%)]\tenerg: 1.863624\tlr: 0.001000\ttrain acc:96.9500\tLoss: 3.181819                \tClf: 3.088577\tReg: 0.000061\tFr_p: 0.109724\tFr_r: 0.119011\n",
      "Train Epoch: 11 [19800/60000 (33%)]\tenerg: 1.842584\tlr: 0.001000\ttrain acc:97.7250\tLoss: 2.872605                \tClf: 2.780414\tReg: 0.000061\tFr_p: 0.109668\tFr_r: 0.118888\n",
      "Train Epoch: 11 [23800/60000 (40%)]\tenerg: 1.865241\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.977964                \tClf: 2.884641\tReg: 0.000061\tFr_p: 0.109862\tFr_r: 0.119364\n",
      "Train Epoch: 11 [27800/60000 (46%)]\tenerg: 1.850551\tlr: 0.001000\ttrain acc:97.5000\tLoss: 2.923311                \tClf: 2.830722\tReg: 0.000061\tFr_p: 0.110041\tFr_r: 0.119788\n",
      "Train Epoch: 11 [31800/60000 (53%)]\tenerg: 1.837998\tlr: 0.001000\ttrain acc:97.3000\tLoss: 3.000644                \tClf: 2.908683\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118952\n",
      "Train Epoch: 11 [35800/60000 (60%)]\tenerg: 1.844843\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.798495                \tClf: 2.706192\tReg: 0.000061\tFr_p: 0.109835\tFr_r: 0.119194\n",
      "Train Epoch: 11 [39800/60000 (66%)]\tenerg: 1.860104\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.083982                \tClf: 2.990916\tReg: 0.000061\tFr_p: 0.109718\tFr_r: 0.118806\n",
      "Train Epoch: 11 [43800/60000 (73%)]\tenerg: 1.849077\tlr: 0.001000\ttrain acc:97.2750\tLoss: 2.926978                \tClf: 2.834463\tReg: 0.000061\tFr_p: 0.109888\tFr_r: 0.119368\n",
      "Train Epoch: 11 [47800/60000 (80%)]\tenerg: 1.842966\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.178913                \tClf: 3.086704\tReg: 0.000061\tFr_p: 0.109695\tFr_r: 0.119271\n",
      "Train Epoch: 11 [51800/60000 (86%)]\tenerg: 1.855365\tlr: 0.001000\ttrain acc:97.4500\tLoss: 3.085230                \tClf: 2.992401\tReg: 0.000061\tFr_p: 0.109792\tFr_r: 0.119513\n",
      "Train Epoch: 11 [55800/60000 (93%)]\tenerg: 1.835287\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.834153                \tClf: 2.742328\tReg: 0.000061\tFr_p: 0.109779\tFr_r: 0.119171\n",
      "Train Epoch: 11 [59800/60000 (100%)]\tenerg: 1.839505\tlr: 0.001000\ttrain acc:98.2000\tLoss: 2.163625                \tClf: 2.071589\tReg: 0.000061\tFr_p: 0.109755\tFr_r: 0.118902\n",
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [3800/60000 (6%)]\tenerg: 1.845496\tlr: 0.001000\ttrain acc:97.6750\tLoss: 3.044971                \tClf: 2.952635\tReg: 0.000061\tFr_p: 0.383515\tFr_r: 0.414918\n",
      "Train Epoch: 12 [7800/60000 (13%)]\tenerg: 1.842936\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.938106                \tClf: 2.845898\tReg: 0.000061\tFr_p: 0.109601\tFr_r: 0.118797\n",
      "Train Epoch: 12 [11800/60000 (20%)]\tenerg: 1.855137\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.959453                \tClf: 2.866635\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118714\n",
      "Train Epoch: 12 [15800/60000 (26%)]\tenerg: 1.862962\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.216655                \tClf: 3.123446\tReg: 0.000061\tFr_p: 0.109728\tFr_r: 0.118990\n",
      "Train Epoch: 12 [19800/60000 (33%)]\tenerg: 1.843506\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.857301                \tClf: 2.765064\tReg: 0.000061\tFr_p: 0.109689\tFr_r: 0.118899\n",
      "Train Epoch: 12 [23800/60000 (40%)]\tenerg: 1.865194\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.960044                \tClf: 2.866723\tReg: 0.000061\tFr_p: 0.109866\tFr_r: 0.119356\n",
      "Train Epoch: 12 [27800/60000 (46%)]\tenerg: 1.850817\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.977764                \tClf: 2.885162\tReg: 0.000061\tFr_p: 0.110036\tFr_r: 0.119823\n",
      "Train Epoch: 12 [31800/60000 (53%)]\tenerg: 1.838417\tlr: 0.001000\ttrain acc:97.3250\tLoss: 3.061120                \tClf: 2.969138\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118989\n",
      "Train Epoch: 12 [35800/60000 (60%)]\tenerg: 1.844935\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.787860                \tClf: 2.695552\tReg: 0.000061\tFr_p: 0.109822\tFr_r: 0.119204\n",
      "Train Epoch: 12 [39800/60000 (66%)]\tenerg: 1.860073\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.119360                \tClf: 3.026295\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118813\n",
      "Train Epoch: 12 [43800/60000 (73%)]\tenerg: 1.850019\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.873991                \tClf: 2.781429\tReg: 0.000061\tFr_p: 0.109878\tFr_r: 0.119356\n",
      "Train Epoch: 12 [47800/60000 (80%)]\tenerg: 1.843265\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.188486                \tClf: 3.096261\tReg: 0.000061\tFr_p: 0.109710\tFr_r: 0.119287\n",
      "Train Epoch: 12 [51800/60000 (86%)]\tenerg: 1.856120\tlr: 0.001000\ttrain acc:97.7000\tLoss: 3.116313                \tClf: 3.023446\tReg: 0.000061\tFr_p: 0.109821\tFr_r: 0.119522\n",
      "Train Epoch: 12 [55800/60000 (93%)]\tenerg: 1.834855\tlr: 0.001000\ttrain acc:97.4250\tLoss: 2.843639                \tClf: 2.751835\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119152\n",
      "Train Epoch: 12 [59800/60000 (100%)]\tenerg: 1.839507\tlr: 0.001000\ttrain acc:98.3750\tLoss: 2.149839                \tClf: 2.057803\tReg: 0.000061\tFr_p: 0.109764\tFr_r: 0.118914\n",
      "\n",
      "Test set: Average loss: 0.0953, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [3800/60000 (6%)]\tenerg: 1.846596\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.979766                \tClf: 2.887375\tReg: 0.000061\tFr_p: 0.383537\tFr_r: 0.414928\n",
      "Train Epoch: 13 [7800/60000 (13%)]\tenerg: 1.843766\tlr: 0.001000\ttrain acc:97.7750\tLoss: 2.929375                \tClf: 2.837125\tReg: 0.000061\tFr_p: 0.109593\tFr_r: 0.118781\n",
      "Train Epoch: 13 [11800/60000 (20%)]\tenerg: 1.855217\tlr: 0.001000\ttrain acc:97.4000\tLoss: 2.864449                \tClf: 2.771627\tReg: 0.000061\tFr_p: 0.109717\tFr_r: 0.118716\n",
      "Train Epoch: 13 [15800/60000 (26%)]\tenerg: 1.863282\tlr: 0.001000\ttrain acc:97.2000\tLoss: 3.169983                \tClf: 3.076757\tReg: 0.000061\tFr_p: 0.109722\tFr_r: 0.119004\n",
      "Train Epoch: 13 [19800/60000 (33%)]\tenerg: 1.842977\tlr: 0.001000\ttrain acc:97.6500\tLoss: 2.846182                \tClf: 2.753972\tReg: 0.000061\tFr_p: 0.109685\tFr_r: 0.118888\n",
      "Train Epoch: 13 [23800/60000 (40%)]\tenerg: 1.865175\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.981189                \tClf: 2.887869\tReg: 0.000061\tFr_p: 0.109871\tFr_r: 0.119367\n",
      "Train Epoch: 13 [27800/60000 (46%)]\tenerg: 1.849309\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.984945                \tClf: 2.892419\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119790\n",
      "Train Epoch: 13 [31800/60000 (53%)]\tenerg: 1.837223\tlr: 0.001000\ttrain acc:97.2500\tLoss: 3.051716                \tClf: 2.959794\tReg: 0.000061\tFr_p: 0.109737\tFr_r: 0.118976\n",
      "Train Epoch: 13 [35800/60000 (60%)]\tenerg: 1.844480\tlr: 0.001000\ttrain acc:97.5750\tLoss: 2.818290                \tClf: 2.726005\tReg: 0.000061\tFr_p: 0.109816\tFr_r: 0.119185\n",
      "Train Epoch: 13 [39800/60000 (66%)]\tenerg: 1.859883\tlr: 0.001000\ttrain acc:97.0500\tLoss: 3.127998                \tClf: 3.034943\tReg: 0.000061\tFr_p: 0.109723\tFr_r: 0.118823\n",
      "Train Epoch: 13 [43800/60000 (73%)]\tenerg: 1.850345\tlr: 0.001000\ttrain acc:97.3500\tLoss: 2.923759                \tClf: 2.831180\tReg: 0.000061\tFr_p: 0.109901\tFr_r: 0.119385\n",
      "Train Epoch: 13 [47800/60000 (80%)]\tenerg: 1.842712\tlr: 0.001000\ttrain acc:97.1000\tLoss: 3.208673                \tClf: 3.116476\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.119265\n",
      "Train Epoch: 13 [51800/60000 (86%)]\tenerg: 1.855770\tlr: 0.001000\ttrain acc:97.4250\tLoss: 3.145599                \tClf: 3.052749\tReg: 0.000061\tFr_p: 0.109807\tFr_r: 0.119537\n",
      "Train Epoch: 13 [55800/60000 (93%)]\tenerg: 1.834696\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.866895                \tClf: 2.775099\tReg: 0.000061\tFr_p: 0.109767\tFr_r: 0.119177\n",
      "Train Epoch: 13 [59800/60000 (100%)]\tenerg: 1.839458\tlr: 0.001000\ttrain acc:98.1750\tLoss: 2.188312                \tClf: 2.096278\tReg: 0.000061\tFr_p: 0.109746\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0958, Accuracy: 9733/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [3800/60000 (6%)]\tenerg: 1.846128\tlr: 0.001000\ttrain acc:97.6000\tLoss: 2.933500                \tClf: 2.841133\tReg: 0.000061\tFr_p: 0.383534\tFr_r: 0.414937\n",
      "Train Epoch: 14 [7800/60000 (13%)]\tenerg: 1.842874\tlr: 0.001000\ttrain acc:97.5500\tLoss: 2.950753                \tClf: 2.858549\tReg: 0.000061\tFr_p: 0.109576\tFr_r: 0.118769\n",
      "Train Epoch: 14 [11800/60000 (20%)]\tenerg: 1.854748\tlr: 0.001000\ttrain acc:97.3750\tLoss: 2.894436                \tClf: 2.801638\tReg: 0.000061\tFr_p: 0.109716\tFr_r: 0.118727\n",
      "Train Epoch: 14 [15800/60000 (26%)]\tenerg: 1.862500\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.237567                \tClf: 3.144381\tReg: 0.000061\tFr_p: 0.109709\tFr_r: 0.118991\n",
      "Train Epoch: 14 [19800/60000 (33%)]\tenerg: 1.842938\tlr: 0.001000\ttrain acc:97.7000\tLoss: 2.852072                \tClf: 2.759864\tReg: 0.000061\tFr_p: 0.109683\tFr_r: 0.118889\n",
      "Train Epoch: 14 [23800/60000 (40%)]\tenerg: 1.865492\tlr: 0.001000\ttrain acc:97.6250\tLoss: 2.949147                \tClf: 2.855812\tReg: 0.000061\tFr_p: 0.109860\tFr_r: 0.119354\n",
      "Train Epoch: 14 [27800/60000 (46%)]\tenerg: 1.849833\tlr: 0.001000\ttrain acc:97.3250\tLoss: 2.944009                \tClf: 2.851457\tReg: 0.000061\tFr_p: 0.110020\tFr_r: 0.119827\n",
      "Train Epoch: 14 [31800/60000 (53%)]\tenerg: 1.838424\tlr: 0.001000\ttrain acc:97.5250\tLoss: 3.003119                \tClf: 2.911137\tReg: 0.000061\tFr_p: 0.109736\tFr_r: 0.118957\n",
      "Train Epoch: 14 [35800/60000 (60%)]\tenerg: 1.845233\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.833932                \tClf: 2.741609\tReg: 0.000061\tFr_p: 0.109826\tFr_r: 0.119189\n",
      "Train Epoch: 14 [39800/60000 (66%)]\tenerg: 1.861053\tlr: 0.001000\ttrain acc:97.2750\tLoss: 3.088628                \tClf: 2.995514\tReg: 0.000061\tFr_p: 0.109741\tFr_r: 0.118846\n",
      "Train Epoch: 14 [43800/60000 (73%)]\tenerg: 1.849921\tlr: 0.001000\ttrain acc:97.5250\tLoss: 2.892694                \tClf: 2.800137\tReg: 0.000061\tFr_p: 0.109879\tFr_r: 0.119354\n",
      "Train Epoch: 14 [47800/60000 (80%)]\tenerg: 1.842206\tlr: 0.001000\ttrain acc:97.1500\tLoss: 3.177974                \tClf: 3.085802\tReg: 0.000061\tFr_p: 0.109703\tFr_r: 0.119259\n",
      "Train Epoch: 14 [51800/60000 (86%)]\tenerg: 1.855820\tlr: 0.001000\ttrain acc:97.5000\tLoss: 3.078748                \tClf: 2.985896\tReg: 0.000061\tFr_p: 0.109810\tFr_r: 0.119516\n",
      "Train Epoch: 14 [55800/60000 (93%)]\tenerg: 1.835157\tlr: 0.001000\ttrain acc:97.4750\tLoss: 2.873681                \tClf: 2.781862\tReg: 0.000061\tFr_p: 0.109775\tFr_r: 0.119192\n",
      "Train Epoch: 14 [59800/60000 (100%)]\tenerg: 1.839779\tlr: 0.001000\ttrain acc:98.1000\tLoss: 2.185634                \tClf: 2.093583\tReg: 0.000061\tFr_p: 0.109768\tFr_r: 0.118908\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 9714/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "named_params = get_stats_named_params(model)\n",
    "all_test_losses = []\n",
    "all_test_acc = []\n",
    "all_test_energy = []\n",
    "seeds = [3,7,11,55,79,101,123,304,709,999]\n",
    "energy_all_test_acc, control_all_test_acc = [], []\n",
    "add_noise = True\n",
    "noise_mean, noise_std = 0, 0.1\n",
    "\n",
    "for seed in seeds:\n",
    "    for model_type in [\"energy\",\"control\"]:\n",
    "        for epoch in range(epochs):\n",
    "            model_name = \"/home/p318679/Documents/SNN_PC_Multicomp/results/{}_seed{}_\".format(alg,seed) + model_type + \"/{}_model.pth\".format(epoch+1)\n",
    "            model = torch.load(model_name, weights_only=False)\n",
    "\n",
    "            if add_noise:\n",
    "                # add Gaussian noise to the input data\n",
    "                test_loss, test_acc, test_energy = test_with_added_noise(model, test_loader, T, noise_mean, noise_std)\n",
    "            else:\n",
    "                test_loss, test_acc, test_energy = test(model, test_loader, T)\n",
    "                \n",
    "            if model_type == \"energy\":\n",
    "                energy_all_test_acc.append(test_acc)\n",
    "            elif model_type == \"control\":\n",
    "                control_all_test_acc.append(test_acc)\n",
    "\n",
    "energy_all_test_acc = np.array(energy_all_test_acc).reshape(len(seeds),epochs)\n",
    "control_all_test_acc = np.array(control_all_test_acc).reshape(len(seeds),epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "errors_control = 100*(10000-100*np.array(control_all_test_acc))/10000\n",
    "errors_energy = 100*(10000-100*np.array(energy_all_test_acc))/10000\n",
    "\n",
    "means_control = errors_control.mean(axis=0)\n",
    "std_control = errors_control.std(axis=0)\n",
    "\n",
    "means_energy = errors_energy.mean(axis=0)\n",
    "std_energy = errors_energy.std(axis=0)\n",
    "\n",
    "plt.plot(means_control,label=\"Control\")\n",
    "plt.fill_between(range(15), means_control - std_control, means_control + std_control, color='b', alpha=0.2)\n",
    "\n",
    "plt.plot(means_energy,label=\"Energy\")\n",
    "plt.fill_between(range(15), means_energy - std_energy, means_energy + std_energy, color='orange', alpha=0.2)\n",
    "\n",
    "plt.legend(fontsize=17,frameon=False)\n",
    "\n",
    "plt.xticks([4,9,14],[5,10,15],fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.title('Test error rate during training',fontsize=20)\n",
    "plt.xlabel('Epoch',fontsize=20)\n",
    "plt.ylabel('Error (%)',fontsize=20)\n",
    "plt.savefig('10_models_graphs\\\\test_error_rate.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2d with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each seed, extract the model at the epoch that first reaches 97% accuracy\n",
    "\n",
    "def get_epochs_97_acc(model_type):\n",
    "    models = []\n",
    "    all_test_accs = []\n",
    "    seeds = [3,7,11,55,79,101,123,304,709,999]\n",
    "    epochs = []\n",
    "    for seed in seeds:\n",
    "        path = \"{}_seed{}_{}\\\\\".format(alg,seed,model_type)\n",
    "        test_accs = np.load(path+\"test_acc.npy\")\n",
    "        all_test_accs.append(test_accs)\n",
    "        if model_type=='control' and seed==7:\n",
    "            idx = 9\n",
    "        elif model_type=='control' and seed==999:\n",
    "            idx = 9\n",
    "        else:\n",
    "            for idx,acc in enumerate(test_accs):\n",
    "                if round(acc) == 97:\n",
    "                    break\n",
    "\n",
    "        epochs.append(idx+1)\n",
    "        model = torch.load(path+\"{}_model.pth\".format(idx+1))\n",
    "        models.append(model)\n",
    "\n",
    "    return epochs, np.array(all_test_accs), models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_control, control_all_test_acc, control_models = get_epochs_97_acc('control')\n",
    "epochs_energy, energy_all_test_acc, energy_models = get_epochs_97_acc('energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get energy & spikes of the 2 models\n",
    "def get_energy(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "\n",
    "    spikes1, spikes2, spikes3 = [],[],[]\n",
    "    errors1,errors2,errors3=[],[],[]\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            errors_batch1=[]\n",
    "            errors_batch2=[]\n",
    "            errors_batch3=[]\n",
    "            spikes_batch1=[]\n",
    "            spikes_batch2=[]\n",
    "            spikes_batch3=[]\n",
    "            \n",
    "            #log_softmax_outputs, hidden, errors = model.inference(data, hidden, time_steps)\n",
    "            for t in range(time_steps):\n",
    "                log_softmax, hidden = model.forward(data, hidden)\n",
    "\n",
    "                errors_batch1.append(torch.abs(hidden[0]-hidden[2]).sum())\n",
    "                errors_batch2.append(torch.abs(hidden[4]-hidden[6]).sum())\n",
    "                errors_batch3.append(torch.abs(hidden[8]-hidden[10]).sum())\n",
    "                \n",
    "                spikes_batch1.append(torch.abs(hidden[1]).sum())\n",
    "                spikes_batch2.append(torch.abs(hidden[5]).sum())\n",
    "                spikes_batch3.append(torch.abs(hidden[9]).sum())\n",
    "                \n",
    "        errors1.append(torch.stack(errors_batch1))\n",
    "        errors2.append(torch.stack(errors_batch2))\n",
    "        errors3.append(torch.stack(errors_batch3))\n",
    "\n",
    "        spikes1.append(torch.stack(spikes_batch1))\n",
    "        spikes2.append(torch.stack(spikes_batch1))\n",
    "        spikes3.append(torch.stack(spikes_batch1))\n",
    "        \n",
    "    errors1 = torch.stack(errors1)\n",
    "    errors2 = torch.stack(errors2)\n",
    "    errors3 = torch.stack(errors3)\n",
    "    spikes1 = torch.stack(spikes1)\n",
    "    spikes2 = torch.stack(spikes2)\n",
    "    spikes3 = torch.stack(spikes3)\n",
    "\n",
    "    #return  spikes1/len(test_loader.dataset)/hidden_dim[0], spikes2/len(test_loader.dataset)/hidden_dim[1], spikes3/len(test_loader.dataset)/hidden_dim[2], torch.sum(energy1/len(test_loader.dataset))/energy1.shape[1], torch.sum(energy2/len(test_loader.dataset))/energy2.shape[1], torch.sum(energy3/len(test_loader.dataset))/energy2.shape[1]\n",
    "    #return  errors, spikes1/len(test_loader.dataset)/hidden_dim[0], spikes2/len(test_loader.dataset)/hidden_dim[1], spikes3/len(test_loader.dataset)/hidden_dim[2], torch.sum(energy1/len(test_loader.dataset))/energy1.shape[1], torch.sum(energy2/len(test_loader.dataset))/energy2.shape[1], torch.sum(energy3/len(test_loader.dataset))/energy2.shape[1]\n",
    "    return (torch.sum(errors1/len(test_loader.dataset)/hidden_dim[0],axis=0).cpu().detach().numpy(),\n",
    "           torch.sum(errors2/len(test_loader.dataset)/hidden_dim[1],axis=0).cpu().detach().numpy(),\n",
    "            torch.sum(errors3/len(test_loader.dataset)/hidden_dim[2],axis=0).cpu().detach().numpy(),\n",
    "            torch.sum(spikes1).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[0]/time_steps,\n",
    "            torch.sum(spikes2).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[1]/time_steps,\n",
    "            torch.sum(spikes3).cpu().detach().numpy()/len(test_loader.dataset)/hidden_dim[2]/time_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_energy1_all,control_energy2_all,control_energy3_all = [],[],[]\n",
    "control_spikes1_all,control_spikes2_all,control_spikes3_all = [],[],[]\n",
    "for control_model in control_models:\n",
    "    control_energy1, control_energy2, control_energy3, control_spikes1, control_spikes2, control_spikes3 = get_energy(control_model, test_loader, T)\n",
    "    control_energy1_all.append(control_energy1)\n",
    "    control_energy2_all.append(control_energy2)\n",
    "    control_energy3_all.append(control_energy3)\n",
    "    control_spikes1_all.append(control_spikes1)\n",
    "    control_spikes2_all.append(control_spikes2)\n",
    "    control_spikes3_all.append(control_spikes3)\n",
    "energy_energy1_all,energy_energy2_all,energy_energy3_all = [],[],[]\n",
    "energy_spikes1_all,energy_spikes2_all,energy_spikes3_all = [],[],[]\n",
    "for energy_model in energy_models:\n",
    "    energy_energy1, energy_energy2, energy_energy3, energy_spikes1, energy_spikes2, energy_spikes3 = get_energy(energy_model, test_loader, T)\n",
    "    energy_energy1_all.append(energy_energy1)\n",
    "    energy_energy2_all.append(energy_energy2)\n",
    "    energy_energy3_all.append(energy_energy3)\n",
    "    energy_spikes1_all.append(energy_spikes1)\n",
    "    energy_spikes2_all.append(energy_spikes2)\n",
    "    energy_spikes3_all.append(energy_spikes3)\n",
    "\n",
    "control_energy1_all = np.array(control_energy1_all)\n",
    "control_energy2_all = np.array(control_energy2_all)\n",
    "control_energy3_all = np.array(control_energy3_all)\n",
    "control_spikes1_all = np.array(control_spikes1_all)\n",
    "control_spikes2_all = np.array(control_spikes2_all)\n",
    "control_spikes3_all = np.array(control_spikes3_all)\n",
    "\n",
    "energy_energy1_all = np.array(energy_energy1_all)\n",
    "energy_energy2_all = np.array(energy_energy2_all)\n",
    "energy_energy3_all = np.array(energy_energy3_all)\n",
    "energy_spikes1_all = np.array(energy_spikes1_all)\n",
    "energy_spikes2_all = np.array(energy_spikes2_all)\n",
    "energy_spikes3_all = np.array(energy_spikes3_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplots of mean spike rates \n",
    "\n",
    "ax = plt.gca()\n",
    "# Remove the top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "pastel_blue = palette[0]\n",
    "pastel_orange = palette[1]\n",
    "\n",
    "X_axis = np.arange(3)\n",
    "plt.bar(X_axis-0.2,[control_spikes1_all.mean(), control_spikes2_all.mean(), control_spikes3_all.mean()],0.4,label=\"Control\",color=pastel_blue)\n",
    "plt.bar(X_axis+0.2,[energy_spikes1_all.mean(), energy_spikes2_all.mean(), energy_spikes3_all.mean()],0.4,label=\"Energy\",color=pastel_orange)\n",
    "\n",
    "labels=[\"1\",\"2\",\"3\"]\n",
    "plt.xticks(X_axis, labels, fontsize=20) \n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"L\",fontsize=20) \n",
    "plt.ylabel(\"Mean R\",fontsize=20) \n",
    "#plt.title(\"Average spike rate over all samples\",fontsize=15) \n",
    "plt.legend(frameon=False,fontsize=17,loc='upper left',bbox_to_anchor=(0, 0., 0, 1.05)) \n",
    "plt.savefig('10_models_graphs\\\\average_spike_rate_retry.png')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SNN_PC_Multicomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
