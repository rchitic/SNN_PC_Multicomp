{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMA-943Aev9e"
   },
   "source": [
    "# Spiking multicompartment PC network\n",
    "\n",
    "## Abstract\n",
    "Predictive coding is a promising theoretical framework for understanding the hierarchical sensory processing in the brain, yet how it is implemented with cortical spiking neurons is still unclear. While most existing works have taken a hand-wiring approach to creating microcircuits which match experimental results, recent work in applying the optimisation approach revealed that cortical connectivity might result from self-organisation given some fundamental computational principle, ie. energy efficiency. We thus investigated whether predictive coding properties in a multicompartment spiking neural network can result from energy optimisation. We found that only the model trained with an energy objective in addition to a task-relevant objective was able to reconstruct internal representations given top-down expectation signals alone. Neurons in the energy-optimised model also showed differential responses to expected vs unexpected stimuli, qualitatively similar to experimental evidence for predictive coding. These findings indicated that predictive-coding-like behaviour might be an emergent property of energy optimisation, providing a new perspective on how predictive coding could be achieved in the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e5fUxmZcxfc",
    "outputId": "87a4b8e1-ab15-4a24-dc7e-20a6796837c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms \n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set seed\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RlhsaGdOj55t"
   },
   "outputs": [],
   "source": [
    "## Utils\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, prefix, filename='_rec2_bias_checkpoint.pth.tar'):\n",
    "    print('saving at ', prefix + filename)\n",
    "    torch.save(state, prefix + filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(prefix + filename, prefix + '_rec2_bias_model_best.pth.tar')\n",
    "\n",
    "\n",
    "def model_result_dict_load(fn):\n",
    "    \"\"\"load tar file with saved model\n",
    "\n",
    "    Args:\n",
    "        fn (str): tar file name\n",
    "\n",
    "    Returns:\n",
    "        dict: dictornary containing saved results\n",
    "    \"\"\"\n",
    "    with open(fn, 'rb') as f:\n",
    "        dict = torch.load(f)\n",
    "    return dict\n",
    "\n",
    "def save_model(model_name,model):\n",
    "    torch.save(model,\"{}_model.pth\".format(model_name))\n",
    "    torch.save(model.state_dict(),\"{}_state_dict.pth\".format(model_name))\n",
    "\n",
    "def load_model(model_name):\n",
    "    model=torch.load(\".\\\\{}_model.pth\".format(model_name))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p7KkVxPfe8q"
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNZglAYUfXQF",
    "outputId": "1e81f699-656e-425f-b289-71eeff15ab2a"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "traindata = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testdata = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# data loading\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfP3cI8BfnoK"
   },
   "source": [
    "## Surrogate gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1KJqRTDNgaqj"
   },
   "outputs": [],
   "source": [
    "\n",
    "b_j0 = 0.1  # neural threshold baseline\n",
    "\n",
    "R_m = 3  # membrane resistance\n",
    "gamma = .5  # gradient scale\n",
    "lens = 0.5\n",
    "\n",
    "\n",
    "def gaussian(x, mu=0., sigma=.5):\n",
    "    return torch.exp(-((x - mu) ** 2) / (2 * sigma ** 2)) / torch.sqrt(2 * torch.tensor(math.pi)) / sigma\n",
    "\n",
    "\n",
    "class ActFun_adp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # input = membrane potential- threshold\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(0).float()  # is firing ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # approximate the gradients\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        # temp = abs(input) < lens\n",
    "        scale = 6.0\n",
    "        hight = .15\n",
    "        # temp = torch.exp(-(input**2)/(2*lens**2))/torch.sqrt(2*torch.tensor(math.pi))/lens\n",
    "        temp = gaussian(input, mu=0., sigma=lens) * (1. + hight) \\\n",
    "               - gaussian(input, mu=lens, sigma=scale * lens) * hight \\\n",
    "               - gaussian(input, mu=-lens, sigma=scale * lens) * hight\n",
    "        # temp =  gaussian(input, mu=0., sigma=lens)\n",
    "        return grad_input * temp.float() * gamma\n",
    "        # return grad_input\n",
    "\n",
    "\n",
    "act_fun_adp = ActFun_adp.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gAwZBV_5gKUS"
   },
   "outputs": [],
   "source": [
    "# layers\n",
    "def shifted_sigmoid(currents):\n",
    "    return (1 / (1 + torch.exp(-currents)) - 0.5)/2\n",
    "\n",
    "\n",
    "class SnnLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "\n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = ff \n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, b_t1\n",
    "\n",
    "\n",
    "class SnnLayerRiseTime(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            is_rec: bool,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            tau_m_init=15.,\n",
    "            tau_curr_decay_init=10.,\n",
    "            tau_adap_init=20,\n",
    "            tau_a_init=15.,\n",
    "            dt = 0.5,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnLayerRiseTime, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.is_rec = is_rec\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_rec:\n",
    "            self.rec_w = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "            # init weights\n",
    "            if bias:\n",
    "                nn.init.constant_(self.rec_w.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.rec_w.weight)\n",
    "\n",
    "            p = torch.full(self.rec_w.weight.size(), fill_value=0.5).to(device)\n",
    "            self.weight_mask = torch.bernoulli(p)\n",
    "\n",
    "        else:\n",
    "            self.fc_weights = nn.Linear(in_dim, hidden_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc_weights.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc_weights.weight)\n",
    "\n",
    "        # define param for time constants\n",
    "        self.tau_adp = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_m = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_curr_decay = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        self.tau_a = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "\n",
    "        nn.init.normal_(self.tau_adp, tau_adap_init, .1)\n",
    "        nn.init.normal_(self.tau_m, tau_m_init, .1)\n",
    "        nn.init.normal_(self.tau_curr_decay, tau_curr_decay_init, .1)\n",
    "        nn.init.normal_(self.tau_a, tau_a_init, .1)\n",
    "\n",
    "        # self.tau_adp = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_m = nn.Parameter(torch.Tensor(1))\n",
    "        # self.tau_a = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        # nn.init.constant_(self.tau_adp, tau_adap_init)\n",
    "        # nn.init.constant_(self.tau_m, tau_m_init)\n",
    "        # nn.init.constant_(self.tau_a, tau_a_init)\n",
    "\n",
    "        # nn.init.normal_(self.tau_adp, 200., 20.)\n",
    "        # nn.init.normal_(self.tau_m, 20., .5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def mem_update(self, ff, fb, soma, spike, a_curr, current_curr, b, is_adapt, baseline_thre=b_j0, r_m=3):\n",
    "        \"\"\"\n",
    "        mem update for each layer of neurons\n",
    "        :param ff: feedforward signal\n",
    "        :param fb: feedback signal to apical tuft\n",
    "        :param soma: mem voltage potential at soma\n",
    "        :param spike: spiking at last time step\n",
    "        :param a_curr: apical tuft current at last t\n",
    "        :param current: input current at last t\n",
    "        :param b: adaptive threshold\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "        # rho = self.sigmoid(self.tau_adp)\n",
    "        # eta = self.sigmoid(self.tau_a)\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        current_decay = torch.exp(-self.dt/self.tau_curr_decay)\n",
    "        rho = torch.exp(-self.dt/self.tau_adp)\n",
    "        eta = torch.exp(-self.dt/self.tau_a)\n",
    "\n",
    "        if is_adapt:\n",
    "            beta = 1.8\n",
    "        else:\n",
    "            beta = 0.\n",
    "                \n",
    "        b = rho * b + (1 - rho) * spike  # adaptive contribution\n",
    "        new_thre = baseline_thre + beta * b  # udpated threshold\n",
    "        \n",
    "        current_new = current_decay * current_curr + ff\n",
    "\n",
    "        a_new = eta * a_curr + fb  # fb into apical tuft\n",
    "\n",
    "        #print(\"mem update\",current_decay , current_curr , ff, eta , a_curr , fb)\n",
    "        \n",
    "        soma_new = alpha * soma + shifted_sigmoid(a_new) + current_new - new_thre * spike\n",
    "        # soma_new = alpha * soma + shifted_sigmoid(a_new) + rise * ff - new_thre * spike\n",
    "        # soma_new = alpha * soma + 1/2 * (a_new) + ffs - new_thre * spike\n",
    "\n",
    "        inputs_ = soma_new - new_thre\n",
    "\n",
    "        spike = act_fun_adp(inputs_)  # act_fun : approximation firing function\n",
    "        # mem = (1 - spike) * mem\n",
    "\n",
    "        return soma_new, spike, a_new, current_new, new_thre, b\n",
    "\n",
    "    def forward(self, ff, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t):\n",
    "        \"\"\"\n",
    "        forward function of a single layer. given previous neuron states and current input, update neuron states\n",
    "\n",
    "        :param ff: ff signal (not counting rec)\n",
    "        :param fb: fb top down signal\n",
    "        :param soma_t: soma voltage\n",
    "        :param a_curr_t: apical tuft voltage\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_rec:\n",
    "            self.rec_w.weight.data = self.rec_w.weight.data * self.weight_mask\n",
    "            # self.rec_w.weight.data = (self.rec_w.weight.data < 0).float() * self.rec_w.weight.data\n",
    "            r_in = ff + self.rec_w(spk_t)\n",
    "        else:\n",
    "            if self.one_to_one:\n",
    "                r_in = ff\n",
    "            else:\n",
    "                r_in = self.fc_weights(ff)\n",
    "\n",
    "        soma_t1, spk_t1, a_curr_t1, current_curr_t1, _, b_t1 = self.mem_update(r_in, fb, soma_t, spk_t, a_curr_t, current_curr_t, b_t, self.is_adapt)\n",
    "\n",
    "        return soma_t1, spk_t1, a_curr_t1, current_curr_t1, b_t1\n",
    "        \n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            out_dim: int,\n",
    "            is_fc: bool,\n",
    "            tau_fixed=None,\n",
    "            bias = True,\n",
    "            dt=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        output layer class\n",
    "        :param is_fc: whether integrator is fc to r_out in rec or not\n",
    "        \"\"\"\n",
    "        super(OutputLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.is_fc = is_fc\n",
    "        self.dt = dt\n",
    "\n",
    "        if is_fc:\n",
    "            self.fc = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "            if bias:\n",
    "                nn.init.constant_(self.fc.bias, 0)\n",
    "            nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "        # tau_m\n",
    "        if tau_fixed is None:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim))\n",
    "            nn.init.constant_(self.tau_m, 5)\n",
    "        else:\n",
    "            self.tau_m = nn.Parameter(torch.Tensor(out_dim), requires_grad=False)\n",
    "            nn.init.constant_(self.tau_m, tau_fixed)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_t, mem_t):\n",
    "        \"\"\"\n",
    "        integrator neuron without spikes\n",
    "        \"\"\"\n",
    "        alpha = torch.exp(-self.dt/self.tau_m)\n",
    "        # alpha = self.sigmoid(self.tau_m)\n",
    "\n",
    "        if self.is_fc:\n",
    "            x_t = self.fc(x_t)\n",
    "        else:\n",
    "            x_t = x_t.view(-1, 10, int(self.in_dim / 10)).mean(dim=2)  # sum up population spike\n",
    "\n",
    "        # d_mem = -soma_t + x_t\n",
    "        mem = (mem_t + x_t) * alpha\n",
    "        # mem = alpha * soma_t + (1 - alpha) * x_t\n",
    "        return mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hpf_RNkHfknR"
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers\n",
    "class Decorrelation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decorrelation, self).__init__()\n",
    "        self.decorr_matrix_next = None\n",
    "    \n",
    "    def forward(self, input, decorr_matrix_prev_batch):\n",
    "        n=1e-3\n",
    "        diag = torch.diag_embed(torch.square(input)) # (batch_size,hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1],1) # (batch_size,hidden_dim,1)\n",
    "        input = torch.matmul(decorr_matrix_prev_batch, input) # (batch_size,hidden_dim,1)\n",
    "\n",
    "        mult = torch.matmul(input, torch.transpose(input,1,2)) # (batch_size,hidden_dim,hidden_dim)\n",
    "        update = torch.mean(mult - diag, dim=0) # (hidden_dim,hidden_dim)\n",
    "        self.decorr_matrix_next = decorr_matrix_prev_batch - n * torch.matmul(update, decorr_matrix_prev_batch) # (hidden_dim,hidden_dim)\n",
    "\n",
    "        input = input.reshape(input.shape[0],input.shape[1]) # (batch_size,hidden_dim)\n",
    "        return input\n",
    "        \n",
    "class SnnNetwork(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super(SnnNetwork, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.out_dim = out_dim\n",
    "        self.is_adapt = is_adapt\n",
    "        self.one_to_one = one_to_one\n",
    "        self.is_rec = is_rec\n",
    "        self.rise_time = rise_time\n",
    "        self.dp = nn.Dropout(dp_rate)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer1 = SnnLayerRiseTime(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer1 = SnnLayer(hidden_dims[0], hidden_dims[0], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        # r in to r out\n",
    "        self.layer1to2 = nn.Linear(hidden_dims[0], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer1to2.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer2to1 = nn.Linear(hidden_dims[1], hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to1.weight)\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer2 = SnnLayerRiseTime(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer2 = SnnLayer(hidden_dims[1], hidden_dims[1], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[1], out_dim, is_fc=True, bias=bias)\n",
    "\n",
    "        self.out2layer2 = nn.Linear(out_dim, hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer2.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer1to2.bias, 0)\n",
    "            nn.init.constant_(self.layer2to1.bias, 0)\n",
    "            nn.init.constant_(self.out2layer2.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "        self.fr_layer2 = 0\n",
    "        self.fr_layer1 = 0\n",
    "\n",
    "        self.error1 = 0\n",
    "        self.error2 = 0\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t*0.5)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.out2layer2(F.normalize(h[-1], dim=1)), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_2, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def inference(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def inference_rise_time(self, x_t, h, T, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h = self.forward_rise_time(x_t, h)\n",
    "            else:\n",
    "                log_softmax, h = self.forward_rise_time(x_t[t], h)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def clamped_generate_rise_time(self, test_class, zeros, h_clamped, T, clamp_value=0.5, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward_rise_time(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # r\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # p\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "\n",
    "# 3 hidden layers\n",
    "\n",
    "class SnnNetwork3Layer(SnnNetwork):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dims: list,\n",
    "            out_dim: int,\n",
    "            is_adapt: bool,\n",
    "            one_to_one: bool,\n",
    "            dp_rate: float,\n",
    "            is_rec: bool,\n",
    "            rise_time: bool,\n",
    "            bias = True\n",
    "    ):\n",
    "        super().__init__(in_dim, hidden_dims, out_dim, is_adapt, one_to_one, dp_rate, is_rec, rise_time)\n",
    "\n",
    "        # decorrelation\n",
    "        self.decorr_layer_0 = Decorrelation()\n",
    "        self.decorr_layer_1 = Decorrelation()\n",
    "        self.decorr_layer_2 = Decorrelation()\n",
    "        self.decorr_layer_3 = Decorrelation()\n",
    "        self.decorr_layer_4 = Decorrelation()\n",
    "\n",
    "        if self.rise_time:\n",
    "            self.layer3 = SnnLayerRiseTime(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "        else:\n",
    "            self.layer3 = SnnLayer(hidden_dims[2], hidden_dims[2], is_rec=is_rec, is_adapt=is_adapt,\n",
    "                               one_to_one=one_to_one, bias=bias)\n",
    "            \n",
    "        self.layer2to3 = nn.Linear(hidden_dims[1], hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer2to3.weight)\n",
    "\n",
    "        # r out to r in\n",
    "        self.layer3to2 = nn.Linear(hidden_dims[2], hidden_dims[1], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.layer3to2.weight)\n",
    "\n",
    "        self.output_layer = OutputLayer(hidden_dims[2], out_dim, is_fc=True)\n",
    "\n",
    "        self.out2layer3 = nn.Linear(out_dim, hidden_dims[2], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.out2layer3.weight)\n",
    "\n",
    "        self.fr_layer3 = 0\n",
    "\n",
    "        self.error3 = 0\n",
    "\n",
    "        self.input_fc = nn.Linear(in_dim, hidden_dims[0], bias=bias)\n",
    "        nn.init.xavier_uniform_(self.input_fc.weight)\n",
    "\n",
    "        if bias:\n",
    "            nn.init.constant_(self.layer2to3.bias, 0)\n",
    "            nn.init.constant_(self.layer3to2.bias, 0)\n",
    "            nn.init.constant_(self.out2layer3.bias, 0)\n",
    "            nn.init.constant_(self.input_fc.bias, 0)\n",
    "            print('bias set to 0')\n",
    "\n",
    "    def forward_rise_time(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, current_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[6]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], current_curr_t=h[3], b_t=h[4])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, current_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[11]), soma_t=h[5],\n",
    "                                                   spk_t=h[6], a_curr_t=h[7], current_curr_t=h[8], b_t=h[9])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, current_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[10],\n",
    "                                                   spk_t=h[11], a_curr_t=h[12], current_curr_t=h[13], b_t=h[14])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, current_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, current_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, current_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "\n",
    "    def forward(self, x_t, h):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        # soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=0, soma_t=h[8],\n",
    "        #                                            spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "\n",
    "        return log_softmax, h\n",
    "        \n",
    "    def forward_decorrelation(self, x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "        batch_dim, input_size = x_t.shape\n",
    "\n",
    "        x_t = x_t.reshape(batch_dim, input_size).float()\n",
    "        x_t = self.dp(x_t)\n",
    "\n",
    "        # poisson\n",
    "        # x_t = x_t.gt(0.7).float()\n",
    "\n",
    "        # decorrelate input\n",
    "        x_t = self.decorr_layer_0(x_t, decorr_matrix_0)\n",
    "        decorr_matrix_0 = self.decorr_layer_0.decorr_matrix_next.data.clone()\n",
    "        x_t = self.input_fc(x_t)\n",
    "\n",
    "        # decorrelate input to L1\n",
    "        x_t = self.decorr_layer_1(x_t, decorr_matrix_1)\n",
    "        decorr_matrix_1 = self.decorr_layer_1.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        soma_1, spk_1, a_curr_1, b_1 = self.layer1(ff=x_t, fb=self.layer2to1(h[5]), soma_t=h[0], spk_t=h[1],\n",
    "                                                   a_curr_t=h[2], b_t=h[3])\n",
    "        self.error1 = a_curr_1 - soma_1\n",
    "\n",
    "        # decorrelate input to L2\n",
    "        spk_1 = self.decorr_layer_2(spk_1, decorr_matrix_2)\n",
    "        decorr_matrix_2 = self.decorr_layer_2.decorr_matrix_next.data.clone()\n",
    "\n",
    "        # use out mem signal as feedback\n",
    "        soma_2, spk_2, a_curr_2, b_2 = self.layer2(ff=self.layer1to2(spk_1), fb=self.layer3to2(h[9]), soma_t=h[4],\n",
    "                                                   spk_t=h[5], a_curr_t=h[6], b_t=h[7])\n",
    "        self.error2 = a_curr_2 - soma_2\n",
    "\n",
    "        # decorrelate input to L3\n",
    "        spk_2 = self.decorr_layer_3(spk_2, decorr_matrix_3)\n",
    "        decorr_matrix_3 = self.decorr_layer_3.decorr_matrix_next.data.clone()\n",
    "\n",
    "        soma_3, spk_3, a_curr_3, b_3 = self.layer3(ff=self.layer2to3(spk_2), fb=self.out2layer3(F.normalize(h[-1], dim=1)), soma_t=h[8],\n",
    "                                                   spk_t=h[9], a_curr_t=h[10], b_t=h[11])\n",
    "        self.error3 = a_curr_3 - soma_3\n",
    "\n",
    "        # decorrelate input to output layer\n",
    "        spk_3 = self.decorr_layer_4(spk_3, decorr_matrix_4)\n",
    "        decorr_matrix_4 = self.decorr_layer_4.decorr_matrix_next.data.clone()\n",
    "        \n",
    "        self.fr_layer3 = self.fr_layer3 + spk_3.detach().cpu().numpy().mean()\n",
    "        self.fr_layer2 = self.fr_layer2 + spk_2.detach().cpu().numpy().mean()\n",
    "        self.fr_layer1 = self.fr_layer1 + spk_1.detach().cpu().numpy().mean()\n",
    "\n",
    "        # read out from r_out neurons\n",
    "        mem_out = self.output_layer(spk_3, h[-1])\n",
    "\n",
    "        h = (soma_1, spk_1, a_curr_1, b_1,\n",
    "             soma_2, spk_2, a_curr_2, b_2,\n",
    "             soma_3, spk_3, a_curr_3, b_3,\n",
    "             mem_out)\n",
    "\n",
    "        log_softmax = F.log_softmax(mem_out, dim=1)\n",
    "        return log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "    def inference_decorrelation(self, x_t, h, T, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4, bystep=None):\n",
    "        \"\"\"\n",
    "        only called during inference\n",
    "        :param x_t: input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param bystep: if true, then x_t is a sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            if bystep is None:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            else:\n",
    "                log_softmax, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = self.forward_decorrelation(x_t[t], h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h)\n",
    "            \n",
    "        return log_softmax_hist, h_hist\n",
    "\n",
    "    def init_hidden_rise_time(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).uniform_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "        \n",
    "    def init_hidden_allzero(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (\n",
    "            # l1\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[0]).fill_(b_j0),\n",
    "            # l2\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[1]).fill_(b_j0),\n",
    "            # l3\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).zero_(),\n",
    "            weight.new(bsz, self.hidden_dims[2]).fill_(b_j0),\n",
    "            # layer out\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "            # sum spike\n",
    "            weight.new(bsz, self.out_dim).zero_(),\n",
    "        )\n",
    "\n",
    "    def clamp_withnoise(self, test_class, zeros, h_clamped, T, noise, index, batch=False, clamp_value=0.5):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param zeros: input containing zeros, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :param noise: noise values\n",
    "        :param index: index in h where noise is added to\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if not batch:\n",
    "                h_clamped[-1][0] = -clamp_value\n",
    "                h_clamped[-1][0, test_class] = clamp_value\n",
    "            else:\n",
    "                h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                h_clamped[index][:, :] += noise * h_clamped[index][:, :]\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = self.forward(zeros, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8p3rNn-kPx7"
   },
   "source": [
    "## FTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VOFtn6gOkPfk"
   },
   "outputs": [],
   "source": [
    "alpha = .2\n",
    "beta = .5\n",
    "rho = 0.\n",
    "\n",
    "\n",
    "# %%\n",
    "def get_stats_named_params(model):\n",
    "    named_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        sm, lm, dm = param.detach().clone(), 0.0 * param.detach().clone(), 0.0 * param.detach().clone()\n",
    "        named_params[name] = (param, sm, lm, dm)\n",
    "    return named_params\n",
    "\n",
    "\n",
    "def post_optimizer_updates(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        lm.data.add_(-alpha * (param - sm))\n",
    "        sm.data.mul_((1.0 - beta))\n",
    "        sm.data.add_(beta * param - (beta / alpha) * lm)\n",
    "\n",
    "\n",
    "def get_regularizer_named_params(named_params, _lambda=1.0):\n",
    "    regularization = torch.zeros([], device=device)\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        regularization += (rho - 1.) * torch.sum(param * lm)\n",
    "        r_p = _lambda * 0.5 * alpha * torch.sum(torch.square(param - sm))\n",
    "        regularization += r_p\n",
    "        # print(name,r_p)\n",
    "    return regularization\n",
    "\n",
    "\n",
    "def reset_named_params(named_params):\n",
    "    for name in named_params:\n",
    "        param, sm, lm, dm = named_params[name]\n",
    "        param.data.copy_(sm.data)\n",
    "\n",
    "\n",
    "def train_fptt(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, rise_time):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    spk3,memout=[],[]\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "            \n",
    "            if p == 0:\n",
    "                if rise_time:\n",
    "                    h = model.init_hidden_rise_time(data.size(0))\n",
    "                else:\n",
    "                    h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            if rise_time:\n",
    "                o, h = model.forward_rise_time(data, h)\n",
    "            else:\n",
    "                o, h = model.forward(data, h)\n",
    "            #print(\"\\n\\nbatch\",batch_idx)\n",
    "            #print(\"\\np\",p)\n",
    "            #print(\"\\nh1\",h[1],\"\\nh6\",h[6],\"\\nh11\",h[11])\n",
    "            #print(\"\\nmemout\",h[-1])\n",
    "            #spk3.append(h[11])\n",
    "            #memout.append(h[-1])\n",
    "\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                #print(\"k updates\",k_updates,F.nll_loss(o, target),\"o\",o,o.shape,\"target\",target,target.shape)\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                #print(\"Loss\", clf_loss, regularizer, energy, spike_loss)\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "\n",
    "def train_fptt_decorr(epoch, batch_size, log_interval,\n",
    "               train_loader, model, named_params,\n",
    "               time_steps, k_updates, omega, optimizer,\n",
    "               clf_alpha, energy_alpha, spike_alpha, clip, lr, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    train_loss = 0\n",
    "    total_clf_loss = 0\n",
    "    total_regularizaton_loss = 0\n",
    "    total_energy_loss = 0\n",
    "    total_spike_loss = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "\n",
    "    # for each batch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # to device and reshape\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        B = target.size()[0]\n",
    "\n",
    "        for p in range(time_steps):\n",
    "\n",
    "            if p == 0:\n",
    "                h = model.init_hidden(data.size(0))\n",
    "            elif p % omega == 0:\n",
    "                h = tuple(v.detach() for v in h)\n",
    "\n",
    "            o, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4 = model.forward_decorrelation(data, h, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "            # wandb.log({\n",
    "            #         'rec layer adap threshold': h[5].detach().cpu().numpy(),\n",
    "            #         'rec layer mem potential': h[3].detach().cpu().numpy()\n",
    "            #     })\n",
    "\n",
    "            # get prediction\n",
    "            if p == (time_steps - 1):\n",
    "                pred = o.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if p % omega == 0 and p > 0:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # classification loss\n",
    "                clf_loss = (p + 1) / k_updates * F.nll_loss(o, target)\n",
    "                # clf_loss = snr*F.cross_entropy(output, target,reduction='none')\n",
    "                # clf_loss = torch.mean(clf_loss)\n",
    "\n",
    "                # regularizer loss\n",
    "                regularizer = get_regularizer_named_params(named_params, _lambda=1.0)\n",
    "\n",
    "                # mem potential loss take l1 norm / num of neurons /batch size\n",
    "                if len(model.hidden_dims) == 2:\n",
    "                    energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5])) / B / sum(model.hidden_dims)\n",
    "                elif len(model.hidden_dims) == 3:\n",
    "                    # energy = (torch.sum(model.error1 ** 2) + torch.sum(model.error2 ** 2) + torch.sum(model.error3 ** 2)) / B / sum(model.hidden_dims)\n",
    "                    energy = (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / B / sum(model.hidden_dims)\n",
    "                    spike_loss = (torch.sum(h[1]) + torch.sum(h[5]) + torch.sum(h[9])) / B / sum(model.hidden_dims)\n",
    "\n",
    "\n",
    "                # overall loss\n",
    "                loss = clf_alpha * clf_loss + regularizer + energy_alpha * energy + spike_alpha * spike_loss\n",
    "\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                if clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "                optimizer.step()\n",
    "                post_optimizer_updates(named_params)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                total_clf_loss += clf_loss.item()\n",
    "                total_regularizaton_loss += regularizer  # .item()\n",
    "                total_energy_loss += energy.item()\n",
    "                total_spike_loss += spike_loss.item()\n",
    "\n",
    "\n",
    "                model.error1 = 0\n",
    "                model.error2 = 0\n",
    "                if len(model.hidden_dims) == 3:\n",
    "                    model.error3 = 0\n",
    "\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % log_interval == (log_interval - 1):\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tenerg: {:.6f}\\tlr: {:.6f}\\ttrain acc:{:.4f}\\tLoss: {:.6f}\\\n",
    "                \\tClf: {:.6f}\\tReg: {:.6f}\\tFr_p: {:.6f}\\tFr_r: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), total_energy_loss / log_interval,\n",
    "                      lr, 100 * correct / (log_interval * B),\n",
    "                       train_loss / log_interval,\n",
    "                       total_clf_loss / log_interval, total_regularizaton_loss / log_interval,\n",
    "                       model.fr_layer2 / time_steps / log_interval,\n",
    "                       model.fr_layer1 / time_steps / log_interval))\n",
    "\n",
    "\n",
    "            train_loss = 0\n",
    "            total_clf_loss = 0\n",
    "            total_regularizaton_loss = 0\n",
    "            total_energy_loss = 0\n",
    "            total_spike_loss = 0\n",
    "            correct = 0\n",
    "            # model.network.fr = 0\n",
    "            model.fr_layer2 = 0\n",
    "            model.fr_layer1 = 0\n",
    "            if len(model.hidden_dims) == 3:\n",
    "                model.fr_layer3 = 0\n",
    "\n",
    "    return decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCAgkJ3hkxPH"
   },
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WL8XFlKykz7K"
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_with_added_noise(model, test_loader, time_steps, noise_mean, noise_std):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data + torch.randn(data.size()) * noise_std + noise_mean\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "def test_rise_time(model, test_loader, time_steps):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_energy = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden_rise_time(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_rise_time(data, hidden, time_steps)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "            test_energy += (torch.sum(torch.abs(model.error1)) + torch.sum(torch.abs(model.error2)) + torch.sum(torch.abs(model.error3))) / target.size()[0] / sum(model.hidden_dims)\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    test_energy /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset), test_energy\n",
    "\n",
    "# test function\n",
    "def test_decorrelation(model, test_loader, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # for data, target in test_loader:\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, model.in_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            hidden = model.init_hidden(data.size(0))\n",
    "            \n",
    "            log_softmax_outputs, hidden = model.inference_decorrelation(data, hidden, time_steps, decorr_matrix_0, decorr_matrix_1, decorr_matrix_2, decorr_matrix_3, decorr_matrix_4)\n",
    "\n",
    "            test_loss += F.nll_loss(log_softmax_outputs[-1], target, reduction='sum').data.item()\n",
    "\n",
    "            pred = log_softmax_outputs[-1].data.max(1, keepdim=True)[1]\n",
    "\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # wandb.log({'spike sequence': plot_spiking_sequence(hidden, target)})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        test_acc))\n",
    "\n",
    "    return test_loss, 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_DORsdsg-TS"
   },
   "source": [
    "## Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oaqFPBCFg0vF"
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "adap_neuron = True  # whether use adaptive neuron or not\n",
    "clf_alpha = 1\n",
    "\n",
    "model_type = \"energy\"\n",
    "if model_type == \"control\":\n",
    "    energy_alpha = 0\n",
    "else:\n",
    "    energy_alpha = 0.05\n",
    "    \n",
    "spike_alpha = 0.  # energy loss on spikes\n",
    "num_readout = 10\n",
    "onetoone = True\n",
    "lr = 1e-3\n",
    "alg = 'fptt'\n",
    "dp = 0.4\n",
    "is_rec = False\n",
    "\n",
    "# training parameters\n",
    "T = 50\n",
    "K = 10  # k_updates is num updates per sequence\n",
    "omega = int(T / K)  # update frequency\n",
    "clip = 1.\n",
    "log_interval = 20\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias set to 0\n",
      "SnnNetwork3Layer(\n",
      "  (dp): Dropout(p=0.4, inplace=False)\n",
      "  (layer1): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer1to2): Linear(in_features=600, out_features=500, bias=True)\n",
      "  (layer2to1): Linear(in_features=500, out_features=600, bias=True)\n",
      "  (layer2): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (output_layer): OutputLayer(\n",
      "    (fc): Linear(in_features=500, out_features=10, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (out2layer2): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (decorr_layer_0): Decorrelation()\n",
      "  (decorr_layer_1): Decorrelation()\n",
      "  (decorr_layer_2): Decorrelation()\n",
      "  (decorr_layer_3): Decorrelation()\n",
      "  (decorr_layer_4): Decorrelation()\n",
      "  (layer3): SnnLayer(\n",
      "    (fc_weights): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer2to3): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (layer3to2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (out2layer3): Linear(in_features=10, out_features=500, bias=True)\n",
      "  (input_fc): Linear(in_features=784, out_features=600, bias=True)\n",
      ")\n",
      "total param count 2455520\n"
     ]
    }
   ],
   "source": [
    "# set input and t param\n",
    "IN_dim = 784\n",
    "hidden_dim = [600, 500, 500]\n",
    "n_classes = 10\n",
    "rise_time=False\n",
    "\n",
    "# define network\n",
    "model = SnnNetwork3Layer(IN_dim, hidden_dim, n_classes, is_adapt=adap_neuron,\n",
    "                         one_to_one=onetoone, dp_rate=dp, is_rec=is_rec, rise_time=rise_time)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# define new loss and optimiser\n",
    "total_params = count_parameters(model)\n",
    "print('total param count %i' % total_params)\n",
    "\n",
    "# define optimiser\n",
    "optimizer = optim.Adamax(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "# reduce the learning after 20 epochs by a factor of 10\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-seG48koNP"
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint to load for 97% accuracy\n",
    "load_epoch_control = 7\n",
    "load_epoch_energy = 9 \n",
    "\n",
    "control_model = torch.load('C:\\\\Users\\\\Raluca Chitic\\\\Desktop\\\\SNN_PC_Multicomp\\\\results/base_control/{}_model.pth'.format(load_epoch_control),map_location=torch.device('cpu'))\n",
    "energy_model = torch.load('C:\\\\Users\\\\Raluca Chitic\\\\Desktop\\\\SNN_PC_Multicomp\\\\results/base_energy/{}_model.pth'.format(load_epoch_energy),map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures 5 c-f with clamping with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamped_generate_surprise(model, test_class, input, h_clamped, T, clamp_value=0.5, clamp_bool=False, batch=False, noise=None):\n",
    "        \"\"\"\n",
    "        generate representations with mem of read out clamped\n",
    "        :param test_class: which class is clamped\n",
    "        :param input: input containing input, absence of input\n",
    "        :param h: hidden states\n",
    "        :param T: sequence length\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        log_softmax_hist = []\n",
    "        h_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if clamp_bool:\n",
    "                if not batch:\n",
    "                    h_clamped[-1][0] = -clamp_value\n",
    "                    h_clamped[-1][0, test_class] = clamp_value\n",
    "                else:\n",
    "                    h_clamped[-1][:, :] = torch.full(h_clamped[-1].size(), -clamp_value).to(device)\n",
    "                    h_clamped[-1][:, test_class] = clamp_value\n",
    "\n",
    "            if noise is not None:\n",
    "                    h_clamped[-1][:] += noise\n",
    "\n",
    "            # if t==0:\n",
    "            #     print(h_clamped[-1])\n",
    "\n",
    "            log_softmax, h_clamped = model.forward(input, h_clamped)\n",
    "\n",
    "            log_softmax_hist.append(log_softmax)\n",
    "            h_hist.append(h_clamped)\n",
    "\n",
    "        return log_softmax_hist, h_hist, h_clamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(hiddens_all_: list, idx: int, hidden_dim_: int, batch_size, T=20, num_samples=10000):\n",
    "    \"\"\"\n",
    "    get a particular internal state depending on index passed to hidden\n",
    "    :param hidden_dim_: the size of a state, eg. num of r or p neurons\n",
    "    :param T: total time steps\n",
    "    :param hiddens_all_: list containing hidden states of all batch and time steps during inference\n",
    "    :param idx: which index in h is taken out\n",
    "    :return: np array containing desired states\n",
    "    \"\"\"\n",
    "    all_states = []\n",
    "    for batch_idx in range(len(hiddens_all_)):  # iterate over batch\n",
    "        batch_ = []\n",
    "        for t in range(T):\n",
    "            seq_ = []\n",
    "            for b in range(batch_size):\n",
    "                seq_.append(hiddens_all_[batch_idx][t][idx][b].detach().cpu().numpy())\n",
    "            seq_ = np.stack(seq_)\n",
    "            batch_.append(seq_)\n",
    "        batch_ = np.stack(batch_)\n",
    "        all_states.append(batch_)\n",
    "\n",
    "    all_states = np.stack(all_states)\n",
    "\n",
    "    return all_states.transpose(0, 2, 1, 3).reshape(num_samples, T, hidden_dim_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model,element,keep_time,seed,clamp_value):\n",
    "    set_seeds(seed)\n",
    "\n",
    "    # clamped generation of internal representations \n",
    "    no_input = torch.zeros((1, IN_dim)).to(device)\n",
    "\n",
    "    clamp_T = T \n",
    "\n",
    "    # Expected clamp class \n",
    "    result_id = 0\n",
    "    if keep_time:\n",
    "        l1_clamp_expected = np.zeros((10000, int(T/2), hidden_dim[0]))\n",
    "        l2_clamp_expected = np.zeros((10000, int(T/2), hidden_dim[1]))\n",
    "        l3_clamp_expected = np.zeros((10000, int(T/2), hidden_dim[2]))\n",
    "    else:\n",
    "        l1_clamp_expected = np.zeros((10000, hidden_dim[0]))\n",
    "        l2_clamp_expected = np.zeros((10000, hidden_dim[1]))\n",
    "        l3_clamp_expected = np.zeros((10000, hidden_dim[2]))\n",
    "\n",
    "    test_loader2 = torch.utils.data.DataLoader(testdata, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "    for batch_id, (x,y) in enumerate(test_loader2):\n",
    "\n",
    "        for input_class in range(10):\n",
    "            n_images = (y==input_class).sum()\n",
    "            for image_idx in range(0,int(n_images)/2,2):\n",
    "                image1 = x[y==input_class][image_idx][0].reshape(1,784).to(device)\n",
    "                image2 = x[y==input_class][image_idx+1][0].reshape(1,784).to(device)\n",
    "                clamp_class = input_class\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "\n",
    "                    hidden_i = model.init_hidden(1)\n",
    "                    # no input for T/4 timesteps before stimulus onset\n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # clamped stimulus for T/2 timesteps\n",
    "                    _, h_hist1, hidden_i = clamped_generate_surprise(model, clamp_class, image1, hidden_i, int(clamp_T / 2), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # no input for T/4 timesteps after stimulus \n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    \n",
    "                    hidden_i = model.init_hidden(1)\n",
    "                    # no input for T/4 timesteps before stimulus onset\n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # clamped stimulus for T/2 timesteps\n",
    "                    _, h_hist2, hidden_i = clamped_generate_surprise(model, clamp_class, image2, hidden_i, int(clamp_T / 2), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # no input for T/4 timesteps after stimulus \n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "\n",
    "                    if element == 'apical':\n",
    "                        #l1_E = get_states([h_hist2], 2, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 6, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        #l3_E = get_states([h_hist2], 10, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "                    elif element == 'soma':\n",
    "                        #l1_E = get_states([h_hist2], 0, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 4, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        #l3_E = get_states([h_hist2], 8, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "                    elif element == 'spikes':\n",
    "                        l1_E = get_states([h_hist2], 1, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 5, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l3_E = get_states([h_hist2], 9, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "\n",
    "                    if keep_time:\n",
    "                        l2_clamp_expected[result_id] += np.squeeze(l2_E)\n",
    "                        if element == 'spikes':\n",
    "                            l1_clamp_expected[result_id] += np.squeeze(l1_E)\n",
    "                            l3_clamp_expected[result_id] += np.squeeze(l3_E)\n",
    "                    else:\n",
    "                        l2_clamp_expected[result_id] += np.squeeze(l2_E.mean(axis=1))\n",
    "                        if element == 'spikes':\n",
    "                            l1_clamp_expected[result_id] += np.squeeze(l1_E.mean(axis=1))\n",
    "                            l3_clamp_expected[result_id] += np.squeeze(l3_E.mean(axis=1))\n",
    "\n",
    "                    result_id += 1\n",
    "                    if result_id > 9999:\n",
    "                        break\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Surprise clamp class \n",
    "    result_id = 0\n",
    "    set_seeds(seed)\n",
    "    if keep_time:\n",
    "        l1_clamp_surprise = np.zeros((10000, int(T/2), hidden_dim[0]))\n",
    "        l2_clamp_surprise = np.zeros((10000, int(T/2), hidden_dim[1]))\n",
    "        l3_clamp_surprise = np.zeros((10000, int(T/2), hidden_dim[2]))\n",
    "    else:\n",
    "        l1_clamp_surprise = np.zeros((10000, hidden_dim[0]))\n",
    "        l2_clamp_surprise = np.zeros((10000, hidden_dim[1]))\n",
    "        l3_clamp_surprise = np.zeros((10000, hidden_dim[2]))\n",
    "\n",
    "    for batch_id, (x,y) in enumerate(test_loader2):\n",
    "        for input_class in range(10):\n",
    "            n_images = (y==input_class).sum()\n",
    "            for image_idx in range(0,int(n_images)/2,2):\n",
    "                image1 = x[y==input_class][image_idx][0].reshape(1,784).to(device)\n",
    "                surprise_class = np.random.choice(np.array(list(set(range(10))-set([input_class]))))\n",
    "                image2 = x[y==surprise_class][image_idx][0].reshape(1,784).to(device)\n",
    "                clamp_class = surprise_class\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "\n",
    "                    hidden_i = model.init_hidden(1)\n",
    "                    # no input for T/4 timesteps before stimulus onset\n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # clamped stimulus for T/2 timesteps\n",
    "                    _, h_hist1, hidden_i = clamped_generate_surprise(model, clamp_class, image1, hidden_i, int(clamp_T / 2), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # no input for T/4 timesteps after stimulus \n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    \n",
    "                    hidden_i = model.init_hidden(1)\n",
    "                    # no input for T/4 timesteps before stimulus onset\n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # clamped stimulus for T/2 timesteps\n",
    "                    _, h_hist2, hidden_i = clamped_generate_surprise(model, clamp_class, image2, hidden_i, int(clamp_T / 2), clamp_value=clamp_value, clamp_bool=False)\n",
    "                    # no input for T/4 timesteps after stimulus \n",
    "                    _, _, hidden_i = clamped_generate_surprise(model, clamp_class, no_input, hidden_i, int(clamp_T / 4), clamp_value=clamp_value, clamp_bool=False)\n",
    "\n",
    "                    #\n",
    "                    if element == 'apical':\n",
    "                        #l1_E = get_states([h_hist2], 2, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 6, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        #l3_E = get_states([h_hist2], 10, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "                    elif element == 'soma':\n",
    "                        #l1_E = get_states([h_hist2], 0, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 4, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        #l3_E = get_states([h_hist2], 8, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "                    elif element == 'spikes':\n",
    "                        l1_E = get_states([h_hist2], 1, hidden_dim[0], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l2_E = get_states([h_hist2], 5, hidden_dim[1], 1, int(clamp_T/2), num_samples=1)\n",
    "                        l3_E = get_states([h_hist2], 9, hidden_dim[2], 1, int(clamp_T/2), num_samples=1)\n",
    "\n",
    "                    if keep_time:\n",
    "                        l2_clamp_surprise[result_id] += np.squeeze(l2_E)\n",
    "                        if element == 'spikes':\n",
    "                            l1_clamp_surprise[result_id] += np.squeeze(l1_E)\n",
    "                            l3_clamp_surprise[result_id] += np.squeeze(l3_E)\n",
    "                    else:\n",
    "                        l2_clamp_surprise[result_id] += np.squeeze(l2_E.mean(axis=1))\n",
    "                        if element == 'spikes':\n",
    "                            l1_clamp_surprise[result_id] += np.squeeze(l1_E.mean(axis=1))\n",
    "                            l3_clamp_surprise[result_id] += np.squeeze(l3_E.mean(axis=1))\n",
    "\n",
    "                    result_id += 1\n",
    "                    if result_id > 9999:\n",
    "                        break\n",
    "                        \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    if element == 'spikes':\n",
    "        return l1_clamp_surprise,l1_clamp_expected, l2_clamp_surprise,l2_clamp_expected, l3_clamp_surprise,l3_clamp_expected\n",
    "    else:\n",
    "        return l2_clamp_surprise, l2_clamp_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voltage_diff_only_L2(l2_diff_control,l2_diff_energy,element):\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    #_,bins2,_ = ax.hist(l2_diff_control.flatten(),bins=10,weights=[100/len(l2_diff_control.flatten())]*len(l2_diff_control.flatten()),label='Control')\n",
    "    #ax.hist(l2_diff_energy.flatten(),bins=10,weights=[100/len(l2_diff_energy.flatten())]*len(l2_diff_energy.flatten()),alpha=0.5,label='Energy')\n",
    "    # use static bins\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    pastel_blue = palette[0]\n",
    "    pastel_orange = palette[1]\n",
    "    \n",
    "    _,bins2,_ = ax.hist(l2_diff_control.flatten(),bins=[-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05],weights=[100/len(l2_diff_control.flatten())]*len(l2_diff_control.flatten()),alpha=0.5,color=pastel_blue,label='Control')\n",
    "    ax.hist(l2_diff_energy.flatten(),bins=[-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05],weights=[100/len(l2_diff_energy.flatten())]*len(l2_diff_energy.flatten()),alpha=0.5,color=pastel_orange,label='Energy')\n",
    "    \n",
    "    ax.spines[['right', 'top']].set_visible(False)\n",
    "    \n",
    "    plt.legend(fontsize=20,loc='upper right')   \n",
    "    plt.ylabel('Percentage',fontsize=20)\n",
    "    plt.xlabel('MSD',fontsize=15)\n",
    "    plt.title('L2 {}'.format(element),fontsize=20)\n",
    "    plt.xticks(fontsize=17)\n",
    "    plt.yticks(fontsize=17)\n",
    "    plt.savefig('base_graphs\\\\surprise_voltage_diff_{}_with_feedback.png'.format(element),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set clamp value. If 1 (maximum value), one class will be clamped to 1 and the rest to -1. If 0.1, one class will be clamped to 0.1 and the rest to -0.1, leading to more uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clamp_value = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keep_time=True # value of each neuron is averaged over all timepoints\n",
    "element='apical'\n",
    "l2_diff_control_surprise,l2_diff_control_expected  = get_results(control_model,element,keep_time,seed,clamp_value)\n",
    "l2_diff_energy_surprise,l2_diff_energy_expected = get_results(energy_model,element,keep_time,seed,clamp_value)\n",
    "\n",
    "l2_diff_control = np.mean(l2_diff_control_expected-l2_diff_control_surprise,axis=1)\n",
    "l2_diff_energy = np.mean(l2_diff_energy_expected-l2_diff_energy_surprise,axis=1)\n",
    "\n",
    "#np.save('base_graphs\\\\surprise_voltage_diff_control_apical_L2_retry.npy',l2_diff_control)\n",
    "#np.save('base_graphs\\\\surprise_voltage_diff_energy_apical_L2_retry.npy',l2_diff_energy)\n",
    "\n",
    "plot_voltage_diff_only_L2(l2_diff_control,l2_diff_energy,'apical tuft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_time=True\n",
    "element='soma'\n",
    "l2_diff_control_surprise,l2_diff_control_expected  = get_results(control_model,element,keep_time,seed,clamp_value)\n",
    "l2_diff_energy_surprise,l2_diff_energy_expected = get_results(energy_model,element,keep_time,seed,clamp_value)\n",
    "\n",
    "l2_diff_control = np.mean(l2_diff_control_expected-l2_diff_control_surprise,axis=1)\n",
    "l2_diff_energy = np.mean(l2_diff_energy_expected-l2_diff_energy_surprise,axis=1)\n",
    "\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_control_soma_L2_with_feedback.npy',l2_diff_control)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_energy_soma_L2_with_feedback.npy',l2_diff_energy)\n",
    "\n",
    "plot_voltage_diff_only_L2(l2_diff_control,l2_diff_energy,'soma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_time=True\n",
    "element='spikes'\n",
    "l1_diff_control_surprise,l1_diff_control_expected, l2_diff_control_surprise,l2_diff_control_expected, l3_diff_control_surprise,l3_diff_control_expected = get_results(control_model,element,keep_time,seed,clamp_value)\n",
    "l1_diff_energy_surprise,l1_diff_energy_expected, l2_diff_energy_surprise,l2_diff_energy_expected, l3_diff_energy_surprise,l3_diff_energy_expected = get_results(energy_model,element,keep_time,seed,clamp_value)\n",
    "\n",
    "l1_diff_control = np.mean(l1_diff_control_expected-l1_diff_control_surprise,axis=1)\n",
    "l1_diff_energy = np.mean(l1_diff_energy_expected-l1_diff_energy_surprise,axis=1)\n",
    "l2_diff_control = np.mean(l2_diff_control_expected-l2_diff_control_surprise,axis=1)\n",
    "l2_diff_energy = np.mean(l2_diff_energy_expected-l2_diff_energy_surprise,axis=1)\n",
    "l3_diff_control = np.mean(l3_diff_control_expected-l3_diff_control_surprise,axis=1)\n",
    "l3_diff_energy = np.mean(l3_diff_energy_expected-l3_diff_energy_surprise,axis=1)\n",
    "\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_control_spikes_L1_with_feedback.npy',l1_diff_control)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_energy_spikes_L1_with_feedback.npy',l1_diff_energy)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_control_spikes_L2_with_feedback.npy',l2_diff_control)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_energy_spikes_L2_with_feedback.npy',l2_diff_energy)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_control_spikes_L3_with_feedback.npy',l3_diff_control)\n",
    "np.save('base_graphs\\\\surprise_voltage_diff_energy_spikes_L3_with_feedback.npy',l3_diff_energy)\n",
    "\n",
    "plot_voltage_diff_only_L2(l2_diff_control,l2_diff_energy,\"$\\delta$$R$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_time=True\n",
    "element='spikes'\n",
    "\n",
    "fig,ax=plt.subplots(1,1)\n",
    "X_axis = np.arange(3)\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "pastel_blue = palette[0]\n",
    "pastel_orange = palette[1]\n",
    "\n",
    "ax.bar(X_axis-0.2,[np.abs(l1_diff_control).mean(), np.abs(l2_diff_control).mean(), np.abs(l3_diff_control).mean()],0.4,color=pastel_blue,label=\"Control\")\n",
    "ax.bar(X_axis+0.2,[np.abs(l1_diff_energy).mean(), np.abs(l2_diff_energy).mean(), np.abs(l3_diff_energy).mean()],0.4,color=pastel_orange,label=\"Energy\")\n",
    "\n",
    "ax.spines[['right', 'top']].set_visible(False)\n",
    "\n",
    "labels=[\"1\",\"2\",\"3\"]\n",
    "plt.xticks(X_axis, labels, fontsize=20) \n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"Layer\",fontsize=20) \n",
    "plt.ylabel(r\"$\\delta$$R$\",fontsize=20) \n",
    "plt.title(r\"$\\delta$$R$ match vs. mismatch\",fontsize=20) \n",
    "plt.legend(fontsize=20) \n",
    "plt.savefig('base_graphs\\\\surprise_voltage_diff_{}_average_with_feedback.png'.format('spikes'),bbox_inches='tight')\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SNN_pred_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
